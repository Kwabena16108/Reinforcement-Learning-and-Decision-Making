{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6b811bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-17T21:06:45.959042Z",
     "start_time": "2022-07-17T21:06:45.956615Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings ; warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a68b022",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-17T21:06:48.180618Z",
     "start_time": "2022-07-17T21:06:45.959980Z"
    }
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "import os\n",
    "import numpy as np\n",
    "from ray.tune.registry import register_env\n",
    "from rldm.utils import football_tools as ft\n",
    "from rldm.utils import system_tools as st\n",
    "from ray import tune\n",
    "from ray.rllib.policy.policy import PolicySpec\n",
    "from ray.rllib.utils import merge_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed235fcd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-17T21:06:48.204717Z",
     "start_time": "2022-07-17T21:06:48.181505Z"
    }
   },
   "outputs": [],
   "source": [
    "n_cpus, n_gpus = st.get_cpu_gpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5a5a45",
   "metadata": {},
   "source": [
    "# Ray Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "788b3b18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-17T21:06:49.614335Z",
     "start_time": "2022-07-17T21:06:48.206328Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '172.17.0.2',\n",
       " 'raylet_ip_address': '172.17.0.2',\n",
       " 'redis_address': '172.17.0.2:10934',\n",
       " 'object_store_address': '/tmp/ray/session_2022-07-22_13-00-13_060519_202/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2022-07-22_13-00-13_060519_202/sockets/raylet',\n",
       " 'webui_url': None,\n",
       " 'session_dir': '/tmp/ray/session_2022-07-22_13-00-13_060519_202',\n",
       " 'metrics_export_port': 64389,\n",
       " 'node_id': '01fce9cfb8de8fe79ec38be9be85e7eb2b3cf3dc261a5566d89938e4'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init(num_cpus=n_cpus, num_gpus=n_gpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6406a63c",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fcb91d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-17T21:06:49.639649Z",
     "start_time": "2022-07-17T21:06:49.621261Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3_vs_3_auto_GK'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_name = ft.n_players_to_env_name(n_players=3, auto_GK=True)\n",
    "env_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b490ecb0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-17T21:06:49.652694Z",
     "start_time": "2022-07-17T21:06:49.640559Z"
    }
   },
   "outputs": [],
   "source": [
    "register_env(env_name, lambda _: ft.RllibGFootball(env_name=env_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a83836ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-17T21:06:49.968866Z",
     "start_time": "2022-07-17T21:06:49.653425Z"
    }
   },
   "outputs": [],
   "source": [
    "obs_space, act_space = ft.get_obs_act_space(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7b38f9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 4.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=321)\u001b[0m 2022-07-22 13:01:48,103\tINFO ppo.py:159 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m 2022-07-22 13:01:50,606\tINFO torch_policy.py:148 -- TorchPolicy (worker=1) running on CPU.\n",
      "\u001b[2m\u001b[36m(pid=321)\u001b[0m 2022-07-22 13:01:50,815\tINFO torch_policy.py:170 -- TorchPolicy (worker=local) running on 1 GPU(s).\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m 2022-07-22 13:01:50,746\tINFO torch_policy.py:148 -- TorchPolicy (worker=1) running on CPU.\n",
      "\u001b[2m\u001b[36m(pid=321)\u001b[0m 2022-07-22 13:01:58,066\tINFO torch_policy.py:170 -- TorchPolicy (worker=local) running on 1 GPU(s).\n",
      "\u001b[2m\u001b[36m(pid=321)\u001b[0m 2022-07-22 13:01:58,105\tINFO rollout_worker.py:1379 -- Built policy map: {}\n",
      "\u001b[2m\u001b[36m(pid=321)\u001b[0m 2022-07-22 13:01:58,105\tINFO rollout_worker.py:1380 -- Built preprocessor map: {'agent_0': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7f7aa58cca50>, 'agent_1': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7f7aa2a642d0>}\n",
      "\u001b[2m\u001b[36m(pid=321)\u001b[0m 2022-07-22 13:01:58,105\tINFO rollout_worker.py:611 -- Built filter map: {'agent_0': <ray.rllib.utils.filter.NoFilter object at 0x7f7aa42a1b50>, 'agent_1': <ray.rllib.utils.filter.NoFilter object at 0x7f7aa42a1f90>}\n",
      "\u001b[2m\u001b[36m(pid=321)\u001b[0m 2022-07-22 13:01:58,110\tINFO trainable.py:109 -- Trainable.setup took 10.017 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m 2022-07-22 13:01:58,134\tINFO rollout_worker.py:742 -- Generating sample batch of size 100\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m 2022-07-22 13:01:58,152\tINFO sampler.py:593 -- Raw obs from env: { 0: { 'player_0': np.ndarray((43,), dtype=float32, min=-1.011, max=1.011, mean=0.082),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m        'player_1': np.ndarray((43,), dtype=float32, min=-1.011, max=1.011, mean=0.082)}}\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m 2022-07-22 13:01:58,152\tINFO sampler.py:594 -- Info return from env: {0: {}}\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m 2022-07-22 13:01:58,153\tINFO sampler.py:823 -- Preprocessed obs: np.ndarray((43,), dtype=float32, min=-1.011, max=1.011, mean=0.082)\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m 2022-07-22 13:01:58,153\tINFO sampler.py:827 -- Filtered obs: np.ndarray((43,), dtype=float32, min=-1.011, max=1.011, mean=0.082)\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m 2022-07-22 13:01:58,153\tINFO sampler.py:1014 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m { 'agent_0': [ { 'data': { 'agent_id': 'player_0',\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                            'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                            'info': {},\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                            'obs': np.ndarray((43,), dtype=float32, min=-1.011, max=1.011, mean=0.082),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                            'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                            'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                            'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                  'type': 'PolicyEvalData'}],\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m   'agent_1': [ { 'data': { 'agent_id': 'player_1',\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                            'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                            'info': {},\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                            'obs': np.ndarray((43,), dtype=float32, min=-1.011, max=1.011, mean=0.082),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                            'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                            'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                            'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                  'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m 2022-07-22 13:01:58,159\tINFO sampler.py:1035 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m { 'agent_0': ( np.ndarray((1,), dtype=int64, min=10.0, max=10.0, mean=10.0),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                [ np.ndarray((1, 256), dtype=float32, min=-0.142, max=0.135, mean=-0.002),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                  np.ndarray((1, 256), dtype=float32, min=-0.287, max=0.286, mean=-0.004)],\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                { 'action_dist_inputs': np.ndarray((1, 19), dtype=float32, min=-0.127, max=0.187, mean=0.005),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                  'action_logp': np.ndarray((1,), dtype=float32, min=-2.833, max=-2.833, mean=-2.833),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                  'action_prob': np.ndarray((1,), dtype=float32, min=0.059, max=0.059, mean=0.059),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                  'vf_preds': np.ndarray((1,), dtype=float32, min=-0.022, max=-0.022, mean=-0.022)}),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m   'agent_1': ( np.ndarray((1,), dtype=int64, min=17.0, max=17.0, mean=17.0),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                [ np.ndarray((1, 256), dtype=float32, min=-0.144, max=0.109, mean=0.002),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                  np.ndarray((1, 256), dtype=float32, min=-0.238, max=0.218, mean=0.003)],\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                { 'action_dist_inputs': np.ndarray((1, 19), dtype=float32, min=-0.108, max=0.092, mean=0.016),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                  'action_logp': np.ndarray((1,), dtype=float32, min=-2.93, max=-2.93, mean=-2.93),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                  'action_prob': np.ndarray((1,), dtype=float32, min=0.053, max=0.053, mean=0.053),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                  'vf_preds': np.ndarray((1,), dtype=float32, min=-0.136, max=-0.136, mean=-0.136)})}\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m 2022-07-22 13:01:58,624\tINFO simple_list_collector.py:659 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m { 'player_0': { 'action_dist_inputs': np.ndarray((100, 19), dtype=float32, min=-0.336, max=0.399, mean=0.012),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                 'action_logp': np.ndarray((100,), dtype=float32, min=-3.272, max=-2.576, mean=-2.904),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                 'actions': np.ndarray((100,), dtype=int64, min=0.0, max=18.0, mean=8.93),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                 'advantages': np.ndarray((100,), dtype=float32, min=-0.065, max=0.068, mean=-0.005),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                 'agent_index': np.ndarray((100,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                 'dones': np.ndarray((100,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                 'eps_id': np.ndarray((100,), dtype=int64, min=1755074935.0, max=1755074935.0, mean=1755074935.0),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                 'infos': np.ndarray((100,), dtype=object, head={'score_reward': 0, 'game_scenario': '3_vs_3_auto_GK', 'game_info': {'ball': np.ndarray((3,), dtype=float64, min=0.0, max=0.111, mean=0.037), 'ball_direction': np.ndarray((3,), dtype=float64, min=-0.002, max=-0.0, mean=-0.001), 'ball_rotation': np.ndarray((3,), dtype=float64, min=0.0, max=0.0, mean=0.0), 'left_team': np.ndarray((3, 2), dtype=float64, min=-1.011, max=0.02, mean=-0.253), 'left_team_direction': np.ndarray((3, 2), dtype=float64, min=-0.0, max=0.002, mean=0.0), 'left_team_tired_factor': np.ndarray((3,), dtype=float64, min=0.0, max=0.0, mean=0.0), 'left_team_active': np.ndarray((3,), dtype=bool, min=1.0, max=1.0, mean=1.0), 'left_team_yellow_card': np.ndarray((3,), dtype=bool, min=0.0, max=0.0, mean=0.0), 'left_team_roles': np.ndarray((3,), dtype=int64, min=0.0, max=9.0, mean=3.333), 'left_team_designated_player': 2, 'right_team': np.ndarray((3, 2), dtype=float64, min=-0.019, max=1.01, mean=0.319), 'right_team_direction': np.ndarray((3, 2), dtype=float64, min=-0.004, max=0.002, mean=-0.001), 'right_team_tired_factor': np.ndarray((3,), dtype=float64, min=0.0, max=0.0, mean=0.0), 'right_team_active': np.ndarray((3,), dtype=bool, min=1.0, max=1.0, mean=1.0), 'right_team_yellow_card': np.ndarray((3,), dtype=bool, min=0.0, max=0.0, mean=0.0), 'right_team_roles': np.ndarray((3,), dtype=int64, min=0.0, max=9.0, mean=3.333), 'right_team_designated_player': 2, 'left_agent_sticky_actions': [np.ndarray((10,), dtype=uint8, min=0.0, max=0.0, mean=0.0), np.ndarray((10,), dtype=uint8, min=0.0, max=1.0, mean=0.1)], 'left_agent_controlled_player': [1, 2], 'right_agent_sticky_actions': [], 'right_agent_controlled_player': [], 'game_mode': 0, 'score': [0, 0], 'ball_owned_team': -1, 'ball_owned_player': -1, 'steps_left': 500}, 'action': 17}),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                 'new_obs': np.ndarray((100, 43), dtype=float32, min=-1.011, max=2.236, mean=0.089),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                 'obs': np.ndarray((100, 43), dtype=float32, min=-1.011, max=2.236, mean=0.089),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                 'prev_actions': np.ndarray((100,), dtype=int64, min=0.0, max=18.0, mean=8.83),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                 'prev_rewards': np.ndarray((100,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                 'rewards': np.ndarray((100,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                 'state_in_0': np.ndarray((8, 256), dtype=float32, min=-0.317, max=0.275, mean=-0.006),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                 'state_in_1': np.ndarray((8, 256), dtype=float32, min=-0.681, max=0.645, mean=-0.012),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                 'state_out_0': np.ndarray((100, 256), dtype=float32, min=-0.33, max=0.28, mean=-0.007),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                 'state_out_1': np.ndarray((100, 256), dtype=float32, min=-0.716, max=0.668, mean=-0.014),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                 'unroll_id': np.ndarray((100,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                 'value_targets': np.ndarray((100,), dtype=float32, min=0.047, max=0.071, mean=0.058),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                 'vf_preds': np.ndarray((100,), dtype=float32, min=-0.022, max=0.122, mean=0.063)},\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m   'player_1': { 'action_dist_inputs': np.ndarray((100, 19), dtype=float32, min=-0.377, max=0.355, mean=0.045),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                 'action_logp': np.ndarray((100,), dtype=float32, min=-3.277, max=-2.742, mean=-2.944),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                 'actions': np.ndarray((100,), dtype=int64, min=0.0, max=18.0, mean=8.2),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                 'advantages': np.ndarray((100,), dtype=float32, min=-0.06, max=0.07, mean=0.019),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                 'agent_index': np.ndarray((100,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                 'dones': np.ndarray((100,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                 'eps_id': np.ndarray((100,), dtype=int64, min=1755074935.0, max=1755074935.0, mean=1755074935.0),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                 'infos': np.ndarray((100,), dtype=object, head={'score_reward': 0, 'game_scenario': '3_vs_3_auto_GK', 'game_info': {'ball': np.ndarray((3,), dtype=float64, min=0.0, max=0.111, mean=0.037), 'ball_direction': np.ndarray((3,), dtype=float64, min=-0.002, max=-0.0, mean=-0.001), 'ball_rotation': np.ndarray((3,), dtype=float64, min=0.0, max=0.0, mean=0.0), 'left_team': np.ndarray((3, 2), dtype=float64, min=-1.011, max=0.02, mean=-0.253), 'left_team_direction': np.ndarray((3, 2), dtype=float64, min=-0.0, max=0.002, mean=0.0), 'left_team_tired_factor': np.ndarray((3,), dtype=float64, min=0.0, max=0.0, mean=0.0), 'left_team_active': np.ndarray((3,), dtype=bool, min=1.0, max=1.0, mean=1.0), 'left_team_yellow_card': np.ndarray((3,), dtype=bool, min=0.0, max=0.0, mean=0.0), 'left_team_roles': np.ndarray((3,), dtype=int64, min=0.0, max=9.0, mean=3.333), 'left_team_designated_player': 2, 'right_team': np.ndarray((3, 2), dtype=float64, min=-0.019, max=1.01, mean=0.319), 'right_team_direction': np.ndarray((3, 2), dtype=float64, min=-0.004, max=0.002, mean=-0.001), 'right_team_tired_factor': np.ndarray((3,), dtype=float64, min=0.0, max=0.0, mean=0.0), 'right_team_active': np.ndarray((3,), dtype=bool, min=1.0, max=1.0, mean=1.0), 'right_team_yellow_card': np.ndarray((3,), dtype=bool, min=0.0, max=0.0, mean=0.0), 'right_team_roles': np.ndarray((3,), dtype=int64, min=0.0, max=9.0, mean=3.333), 'right_team_designated_player': 2, 'left_agent_sticky_actions': [np.ndarray((10,), dtype=uint8, min=0.0, max=0.0, mean=0.0), np.ndarray((10,), dtype=uint8, min=0.0, max=1.0, mean=0.1)], 'left_agent_controlled_player': [1, 2], 'right_agent_sticky_actions': [], 'right_agent_controlled_player': [], 'game_mode': 0, 'score': [0, 0], 'ball_owned_team': -1, 'ball_owned_player': -1, 'steps_left': 500}, 'action': 17}),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                 'new_obs': np.ndarray((100, 43), dtype=float32, min=-1.011, max=2.236, mean=0.089),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                 'obs': np.ndarray((100, 43), dtype=float32, min=-1.011, max=2.236, mean=0.089),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                 'prev_actions': np.ndarray((100,), dtype=int64, min=0.0, max=18.0, mean=8.05),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                 'prev_rewards': np.ndarray((100,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                 'rewards': np.ndarray((100,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                 'state_in_0': np.ndarray((8, 256), dtype=float32, min=-0.339, max=0.291, mean=0.004),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                 'state_in_1': np.ndarray((8, 256), dtype=float32, min=-0.639, max=0.59, mean=0.008),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                 'state_out_0': np.ndarray((100, 256), dtype=float32, min=-0.381, max=0.35, mean=0.005),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                 'state_out_1': np.ndarray((100, 256), dtype=float32, min=-0.69, max=0.703, mean=0.009),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                 'unroll_id': np.ndarray((100,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                 'value_targets': np.ndarray((100,), dtype=float32, min=-0.201, max=-0.149, mean=-0.181),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                 'vf_preds': np.ndarray((100,), dtype=float32, min=-0.26, max=-0.105, mean=-0.2)}}\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m 2022-07-22 13:01:58,629\tINFO rollout_worker.py:780 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m { 'count': 100,\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m   'policy_batches': { 'agent_0': { 'action_dist_inputs': np.ndarray((100, 19), dtype=float32, min=-0.336, max=0.399, mean=0.012),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                                    'action_logp': np.ndarray((100,), dtype=float32, min=-3.272, max=-2.576, mean=-2.904),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                                    'actions': np.ndarray((100,), dtype=int64, min=0.0, max=18.0, mean=8.93),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                                    'advantages': np.ndarray((100,), dtype=float32, min=-0.065, max=0.068, mean=-0.005),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                                    'agent_index': np.ndarray((100,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                                    'dones': np.ndarray((100,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                                    'eps_id': np.ndarray((100,), dtype=int64, min=1755074935.0, max=1755074935.0, mean=1755074935.0),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                                    'infos': np.ndarray((100,), dtype=object, head={'score_reward': 0, 'game_scenario': '3_vs_3_auto_GK', 'game_info': {'ball': np.ndarray((3,), dtype=float64, min=0.0, max=0.111, mean=0.037), 'ball_direction': np.ndarray((3,), dtype=float64, min=-0.002, max=-0.0, mean=-0.001), 'ball_rotation': np.ndarray((3,), dtype=float64, min=0.0, max=0.0, mean=0.0), 'left_team': np.ndarray((3, 2), dtype=float64, min=-1.011, max=0.02, mean=-0.253), 'left_team_direction': np.ndarray((3, 2), dtype=float64, min=-0.0, max=0.002, mean=0.0), 'left_team_tired_factor': np.ndarray((3,), dtype=float64, min=0.0, max=0.0, mean=0.0), 'left_team_active': np.ndarray((3,), dtype=bool, min=1.0, max=1.0, mean=1.0), 'left_team_yellow_card': np.ndarray((3,), dtype=bool, min=0.0, max=0.0, mean=0.0), 'left_team_roles': np.ndarray((3,), dtype=int64, min=0.0, max=9.0, mean=3.333), 'left_team_designated_player': 2, 'right_team': np.ndarray((3, 2), dtype=float64, min=-0.019, max=1.01, mean=0.319), 'right_team_direction': np.ndarray((3, 2), dtype=float64, min=-0.004, max=0.002, mean=-0.001), 'right_team_tired_factor': np.ndarray((3,), dtype=float64, min=0.0, max=0.0, mean=0.0), 'right_team_active': np.ndarray((3,), dtype=bool, min=1.0, max=1.0, mean=1.0), 'right_team_yellow_card': np.ndarray((3,), dtype=bool, min=0.0, max=0.0, mean=0.0), 'right_team_roles': np.ndarray((3,), dtype=int64, min=0.0, max=9.0, mean=3.333), 'right_team_designated_player': 2, 'left_agent_sticky_actions': [np.ndarray((10,), dtype=uint8, min=0.0, max=0.0, mean=0.0), np.ndarray((10,), dtype=uint8, min=0.0, max=1.0, mean=0.1)], 'left_agent_controlled_player': [1, 2], 'right_agent_sticky_actions': [], 'right_agent_controlled_player': [], 'game_mode': 0, 'score': [0, 0], 'ball_owned_team': -1, 'ball_owned_player': -1, 'steps_left': 500}, 'action': 17}),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                                    'new_obs': np.ndarray((100, 43), dtype=float32, min=-1.011, max=2.236, mean=0.089),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                                    'obs': np.ndarray((100, 43), dtype=float32, min=-1.011, max=2.236, mean=0.089),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                                    'prev_actions': np.ndarray((100,), dtype=int64, min=0.0, max=18.0, mean=8.83),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                                    'prev_rewards': np.ndarray((100,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                                    'rewards': np.ndarray((100,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                                    'seq_lens': np.ndarray((8,), dtype=int32, min=9.0, max=13.0, mean=12.5),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                                    'state_in_0': np.ndarray((8, 256), dtype=float32, min=-0.317, max=0.275, mean=-0.006),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                                    'state_in_1': np.ndarray((8, 256), dtype=float32, min=-0.681, max=0.645, mean=-0.012),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                                    'state_out_0': np.ndarray((100, 256), dtype=float32, min=-0.33, max=0.28, mean=-0.007),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                                    'state_out_1': np.ndarray((100, 256), dtype=float32, min=-0.716, max=0.668, mean=-0.014),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                                    'unroll_id': np.ndarray((100,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                                    'value_targets': np.ndarray((100,), dtype=float32, min=0.047, max=0.071, mean=0.058),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                                    'vf_preds': np.ndarray((100,), dtype=float32, min=-0.022, max=0.122, mean=0.063)},\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                       'agent_1': { 'action_dist_inputs': np.ndarray((100, 19), dtype=float32, min=-0.377, max=0.355, mean=0.045),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                                    'action_logp': np.ndarray((100,), dtype=float32, min=-3.277, max=-2.742, mean=-2.944),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                                    'actions': np.ndarray((100,), dtype=int64, min=0.0, max=18.0, mean=8.2),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                                    'advantages': np.ndarray((100,), dtype=float32, min=-0.06, max=0.07, mean=0.019),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                                    'agent_index': np.ndarray((100,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                                    'dones': np.ndarray((100,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                                    'eps_id': np.ndarray((100,), dtype=int64, min=1755074935.0, max=1755074935.0, mean=1755074935.0),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                                    'infos': np.ndarray((100,), dtype=object, head={'score_reward': 0, 'game_scenario': '3_vs_3_auto_GK', 'game_info': {'ball': np.ndarray((3,), dtype=float64, min=0.0, max=0.111, mean=0.037), 'ball_direction': np.ndarray((3,), dtype=float64, min=-0.002, max=-0.0, mean=-0.001), 'ball_rotation': np.ndarray((3,), dtype=float64, min=0.0, max=0.0, mean=0.0), 'left_team': np.ndarray((3, 2), dtype=float64, min=-1.011, max=0.02, mean=-0.253), 'left_team_direction': np.ndarray((3, 2), dtype=float64, min=-0.0, max=0.002, mean=0.0), 'left_team_tired_factor': np.ndarray((3,), dtype=float64, min=0.0, max=0.0, mean=0.0), 'left_team_active': np.ndarray((3,), dtype=bool, min=1.0, max=1.0, mean=1.0), 'left_team_yellow_card': np.ndarray((3,), dtype=bool, min=0.0, max=0.0, mean=0.0), 'left_team_roles': np.ndarray((3,), dtype=int64, min=0.0, max=9.0, mean=3.333), 'left_team_designated_player': 2, 'right_team': np.ndarray((3, 2), dtype=float64, min=-0.019, max=1.01, mean=0.319), 'right_team_direction': np.ndarray((3, 2), dtype=float64, min=-0.004, max=0.002, mean=-0.001), 'right_team_tired_factor': np.ndarray((3,), dtype=float64, min=0.0, max=0.0, mean=0.0), 'right_team_active': np.ndarray((3,), dtype=bool, min=1.0, max=1.0, mean=1.0), 'right_team_yellow_card': np.ndarray((3,), dtype=bool, min=0.0, max=0.0, mean=0.0), 'right_team_roles': np.ndarray((3,), dtype=int64, min=0.0, max=9.0, mean=3.333), 'right_team_designated_player': 2, 'left_agent_sticky_actions': [np.ndarray((10,), dtype=uint8, min=0.0, max=0.0, mean=0.0), np.ndarray((10,), dtype=uint8, min=0.0, max=1.0, mean=0.1)], 'left_agent_controlled_player': [1, 2], 'right_agent_sticky_actions': [], 'right_agent_controlled_player': [], 'game_mode': 0, 'score': [0, 0], 'ball_owned_team': -1, 'ball_owned_player': -1, 'steps_left': 500}, 'action': 17}),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                                    'new_obs': np.ndarray((100, 43), dtype=float32, min=-1.011, max=2.236, mean=0.089),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                                    'obs': np.ndarray((100, 43), dtype=float32, min=-1.011, max=2.236, mean=0.089),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                                    'prev_actions': np.ndarray((100,), dtype=int64, min=0.0, max=18.0, mean=8.05),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                                    'prev_rewards': np.ndarray((100,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                                    'rewards': np.ndarray((100,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                                    'seq_lens': np.ndarray((8,), dtype=int32, min=9.0, max=13.0, mean=12.5),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                                    'state_in_0': np.ndarray((8, 256), dtype=float32, min=-0.339, max=0.291, mean=0.004),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                                    'state_in_1': np.ndarray((8, 256), dtype=float32, min=-0.639, max=0.59, mean=0.008),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                                    'state_out_0': np.ndarray((100, 256), dtype=float32, min=-0.381, max=0.35, mean=0.005),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                                    'state_out_1': np.ndarray((100, 256), dtype=float32, min=-0.69, max=0.703, mean=0.009),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                                    'unroll_id': np.ndarray((100,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                                    'value_targets': np.ndarray((100,), dtype=float32, min=-0.201, max=-0.149, mean=-0.181),\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m                                    'vf_preds': np.ndarray((100,), dtype=float32, min=-0.26, max=-0.105, mean=-0.2)}},\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m   'type': 'MultiAgentBatch'}\n",
      "\u001b[2m\u001b[36m(pid=323)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=321)\u001b[0m 2022-07-22 13:02:11,861\tINFO rnn_sequencing.py:140 -- Padded input for RNN/Attn.Nets/MA:\n",
      "\u001b[2m\u001b[36m(pid=321)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=321)\u001b[0m { 'features': [ np.ndarray((2977, 43), dtype=float32, min=-1.314, max=10.796, mean=0.056),\n",
      "\u001b[2m\u001b[36m(pid=321)\u001b[0m                 np.ndarray((2977, 43), dtype=float32, min=-1.314, max=10.796, mean=0.056),\n",
      "\u001b[2m\u001b[36m(pid=321)\u001b[0m                 np.ndarray((2977,), dtype=int64, min=0.0, max=18.0, mean=8.594),\n",
      "\u001b[2m\u001b[36m(pid=321)\u001b[0m                 np.ndarray((2977,), dtype=int64, min=0.0, max=18.0, mean=8.549),\n",
      "\u001b[2m\u001b[36m(pid=321)\u001b[0m                 np.ndarray((2977,), dtype=float32, min=-1.0, max=0.1, mean=-0.001),\n",
      "\u001b[2m\u001b[36m(pid=321)\u001b[0m                 np.ndarray((2977,), dtype=float32, min=0.0, max=0.1, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=321)\u001b[0m                 np.ndarray((2977,), dtype=float32, min=0.0, max=1.0, mean=0.003),\n",
      "\u001b[2m\u001b[36m(pid=321)\u001b[0m                 np.ndarray((2977,), dtype=int64, min=0.0, max=1.0, mean=0.941),\n",
      "\u001b[2m\u001b[36m(pid=321)\u001b[0m                 np.ndarray((2977,), dtype=float32, min=-0.293, max=0.098, mean=-0.179),\n",
      "\u001b[2m\u001b[36m(pid=321)\u001b[0m                 np.ndarray((2977, 19), dtype=float32, min=-0.406, max=0.503, mean=0.041),\n",
      "\u001b[2m\u001b[36m(pid=321)\u001b[0m                 np.ndarray((2977,), dtype=float32, min=-3.362, max=0.0, mean=-2.761),\n",
      "\u001b[2m\u001b[36m(pid=321)\u001b[0m                 np.ndarray((2977,), dtype=int64, min=0.0, max=1755074935.0, mean=1044204527.103),\n",
      "\u001b[2m\u001b[36m(pid=321)\u001b[0m                 np.ndarray((2977,), dtype=int64, min=0.0, max=73.0, mean=34.958),\n",
      "\u001b[2m\u001b[36m(pid=321)\u001b[0m                 np.ndarray((2977,), dtype=float32, min=-9.466, max=2.172, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=321)\u001b[0m                 np.ndarray((2977,), dtype=float32, min=-1.0, max=0.03, mean=-0.168)],\n",
      "\u001b[2m\u001b[36m(pid=321)\u001b[0m   'initial_states': [ np.ndarray((229, 256), dtype=float32, min=-0.521, max=0.406, mean=0.004),\n",
      "\u001b[2m\u001b[36m(pid=321)\u001b[0m                       np.ndarray((229, 256), dtype=float32, min=-1.016, max=0.892, mean=0.009)],\n",
      "\u001b[2m\u001b[36m(pid=321)\u001b[0m   'max_seq_len': 13,\n",
      "\u001b[2m\u001b[36m(pid=321)\u001b[0m   'seq_lens': np.ndarray((229,), dtype=int32, min=3.0, max=13.0, mean=12.227)}\n",
      "\u001b[2m\u001b[36m(pid=321)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 5600\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-02-16\n",
      "  done: false\n",
      "  episode_len_mean: 236.2\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.5899999998509884\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 10\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.929771138798623\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.004435248793507975\n",
      "          policy_loss: -0.021723774261772633\n",
      "          total_loss: -0.015210162426921584\n",
      "          vf_explained_var: -0.5804483294487\n",
      "          vf_loss: 0.009411394315369156\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.930734703228587\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.005523871329509484\n",
      "          policy_loss: -0.0288310630366184\n",
      "          total_loss: -0.02177828932750951\n",
      "          vf_explained_var: 0.28068727254867554\n",
      "          vf_loss: 0.007844010812429284\n",
      "    num_agent_steps_sampled: 5600\n",
      "    num_agent_steps_trained: 5600\n",
      "    num_steps_sampled: 2800\n",
      "    num_steps_trained: 2800\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.8374999999999995\n",
      "    gpu_util_percent0: 0.05333333333333334\n",
      "    ram_util_percent: 52.80000000000001\n",
      "    vram_util_percent0: 0.22310791015625\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.0\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.3\n",
      "    agent_1: -0.2899999998509884\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.050806394860983664\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8096002921254586\n",
      "    mean_inference_ms: 1.7197126152259201\n",
      "    mean_raw_obs_processing_ms: 0.14599958771852375\n",
      "  time_since_restore: 17.910996198654175\n",
      "  time_this_iter_s: 17.910996198654175\n",
      "  time_total_s: 17.910996198654175\n",
      "  timers:\n",
      "    learn_throughput: 673.268\n",
      "    learn_time_ms: 4158.818\n",
      "    load_throughput: 93213.414\n",
      "    load_time_ms: 30.039\n",
      "    sample_throughput: 204.183\n",
      "    sample_time_ms: 13713.161\n",
      "    update_time_ms: 4.616\n",
      "  timestamp: 1658494936\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2800\n",
      "  training_iteration: 1\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">          17.911</td><td style=\"text-align: right;\">2800</td><td style=\"text-align: right;\">   -0.59</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             236.2</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 11200\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-02-35\n",
      "  done: false\n",
      "  episode_len_mean: 250.77272727272728\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.6318181817504492\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 22\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.9273503848484586\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.009113372598797742\n",
      "          policy_loss: -0.014785041693574471\n",
      "          total_loss: -0.00705299639542188\n",
      "          vf_explained_var: -0.08877194672822952\n",
      "          vf_loss: 0.012540086189195184\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.9241206780785607\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.005719548576355408\n",
      "          policy_loss: -0.010538100603829056\n",
      "          total_loss: -0.0023144753809208937\n",
      "          vf_explained_var: 0.45951372385025024\n",
      "          vf_loss: 0.010619965431942546\n",
      "    num_agent_steps_sampled: 11200\n",
      "    num_agent_steps_trained: 11200\n",
      "    num_steps_sampled: 5600\n",
      "    num_steps_trained: 5600\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.028\n",
      "    gpu_util_percent0: 0.0536\n",
      "    ram_util_percent: 53.372\n",
      "    vram_util_percent0: 0.23192578125000002\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.0\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.3181818181818182\n",
      "    agent_1: -0.31363636356863106\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0512775457637096\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8618173698280716\n",
      "    mean_inference_ms: 1.7431647979538303\n",
      "    mean_raw_obs_processing_ms: 0.15024773538605515\n",
      "  time_since_restore: 36.848185300827026\n",
      "  time_this_iter_s: 18.93718910217285\n",
      "  time_total_s: 36.848185300827026\n",
      "  timers:\n",
      "    learn_throughput: 666.145\n",
      "    learn_time_ms: 4203.288\n",
      "    load_throughput: 71588.897\n",
      "    load_time_ms: 39.112\n",
      "    sample_throughput: 197.711\n",
      "    sample_time_ms: 14162.066\n",
      "    update_time_ms: 4.708\n",
      "  timestamp: 1658494955\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 5600\n",
      "  training_iteration: 2\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         36.8482</td><td style=\"text-align: right;\">5600</td><td style=\"text-align: right;\">-0.631818</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">           250.773</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 16800\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-02-53\n",
      "  done: false\n",
      "  episode_len_mean: 230.38888888888889\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.49444444436166024\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 36\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.9181897044181824\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.008115647946657001\n",
      "          policy_loss: -0.004942833503654194\n",
      "          total_loss: 6.882163801319188e-05\n",
      "          vf_explained_var: -0.2972274720668793\n",
      "          vf_loss: 0.006187483558928112\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.9190131965137662\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.005626473437289445\n",
      "          policy_loss: -0.006641339688074021\n",
      "          total_loss: -0.0002896375621535948\n",
      "          vf_explained_var: 0.31506162881851196\n",
      "          vf_loss: 0.005535875689149959\n",
      "    num_agent_steps_sampled: 16800\n",
      "    num_agent_steps_trained: 16800\n",
      "    num_steps_sampled: 8400\n",
      "    num_steps_trained: 8400\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.172\n",
      "    gpu_util_percent0: 0.06720000000000001\n",
      "    ram_util_percent: 53.61200000000001\n",
      "    vram_util_percent0: 0.23044140624999998\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.0\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.25\n",
      "    agent_1: -0.2444444443616602\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051410951102356335\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8778595681047494\n",
      "    mean_inference_ms: 1.753224563345386\n",
      "    mean_raw_obs_processing_ms: 0.15402822345160247\n",
      "  time_since_restore: 55.304033041000366\n",
      "  time_this_iter_s: 18.45584774017334\n",
      "  time_total_s: 55.304033041000366\n",
      "  timers:\n",
      "    learn_throughput: 669.282\n",
      "    learn_time_ms: 4183.59\n",
      "    load_throughput: 78230.221\n",
      "    load_time_ms: 35.792\n",
      "    sample_throughput: 197.372\n",
      "    sample_time_ms: 14186.378\n",
      "    update_time_ms: 4.237\n",
      "  timestamp: 1658494973\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8400\n",
      "  training_iteration: 3\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">          55.304</td><td style=\"text-align: right;\">8400</td><td style=\"text-align: right;\">-0.494444</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">           230.389</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 22400\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-03-11\n",
      "  done: false\n",
      "  episode_len_mean: 233.125\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.20000000298023224\n",
      "  episode_reward_mean: -0.44791666651144624\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 48\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.908589502175649\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.008129848223021943\n",
      "          policy_loss: -0.03079861557732026\n",
      "          total_loss: -0.026365828814007165\n",
      "          vf_explained_var: -0.1211400255560875\n",
      "          vf_loss: 0.004503341241182906\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.908071063813709\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.006009808204502473\n",
      "          policy_loss: -0.023138708235949577\n",
      "          total_loss: -0.01673736024167738\n",
      "          vf_explained_var: 0.20085828006267548\n",
      "          vf_loss: 0.004570309164461962\n",
      "    num_agent_steps_sampled: 22400\n",
      "    num_agent_steps_trained: 22400\n",
      "    num_steps_sampled: 11200\n",
      "    num_steps_trained: 11200\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.9875\n",
      "    gpu_util_percent0: 0.06041666666666667\n",
      "    ram_util_percent: 53.55833333333334\n",
      "    vram_util_percent0: 0.22919514973958335\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.20000000298023224\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.22916666666666666\n",
      "    agent_1: -0.21874999984477958\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051436456414073996\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8835934217209584\n",
      "    mean_inference_ms: 1.756986676421164\n",
      "    mean_raw_obs_processing_ms: 0.1554695936513533\n",
      "  time_since_restore: 73.69399785995483\n",
      "  time_this_iter_s: 18.389964818954468\n",
      "  time_total_s: 73.69399785995483\n",
      "  timers:\n",
      "    learn_throughput: 668.919\n",
      "    learn_time_ms: 4185.86\n",
      "    load_throughput: 82329.176\n",
      "    load_time_ms: 34.01\n",
      "    sample_throughput: 197.538\n",
      "    sample_time_ms: 14174.489\n",
      "    update_time_ms: 3.927\n",
      "  timestamp: 1658494991\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 11200\n",
      "  training_iteration: 4\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">          73.694</td><td style=\"text-align: right;\">11200</td><td style=\"text-align: right;\">-0.447917</td><td style=\"text-align: right;\">                 0.2</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">           233.125</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 28000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-03-30\n",
      "  done: false\n",
      "  episode_len_mean: 228.63934426229508\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.20000000298023224\n",
      "  episode_reward_mean: -0.45081967200900686\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 61\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.8885347296794257\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.009662150763911035\n",
      "          policy_loss: -0.03598702148156008\n",
      "          total_loss: -0.029854975994523903\n",
      "          vf_explained_var: -0.05582302063703537\n",
      "          vf_loss: 0.007143245311843722\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.895065025914283\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.006784455486803424\n",
      "          policy_loss: -0.017322371331309632\n",
      "          total_loss: -0.009274790599842422\n",
      "          vf_explained_var: 0.0019305774476379156\n",
      "          vf_loss: 0.0070429499883175595\n",
      "    num_agent_steps_sampled: 28000\n",
      "    num_agent_steps_trained: 28000\n",
      "    num_steps_sampled: 14000\n",
      "    num_steps_trained: 14000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.188\n",
      "    gpu_util_percent0: 0.0644\n",
      "    ram_util_percent: 53.663999999999994\n",
      "    vram_util_percent0: 0.2304453125\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.20000000298023224\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.22950819672131148\n",
      "    agent_1: -0.2213114752876954\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051477081498118214\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8884170436494387\n",
      "    mean_inference_ms: 1.7601382938496113\n",
      "    mean_raw_obs_processing_ms: 0.15672541498290335\n",
      "  time_since_restore: 92.470454454422\n",
      "  time_this_iter_s: 18.776456594467163\n",
      "  time_total_s: 92.470454454422\n",
      "  timers:\n",
      "    learn_throughput: 662.219\n",
      "    learn_time_ms: 4228.208\n",
      "    load_throughput: 84979.278\n",
      "    load_time_ms: 32.949\n",
      "    sample_throughput: 197.14\n",
      "    sample_time_ms: 14203.104\n",
      "    update_time_ms: 3.866\n",
      "  timestamp: 1658495010\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 14000\n",
      "  training_iteration: 5\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         92.4705</td><td style=\"text-align: right;\">14000</td><td style=\"text-align: right;\">-0.45082</td><td style=\"text-align: right;\">                 0.2</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">           228.639</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 33600\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-03-49\n",
      "  done: false\n",
      "  episode_len_mean: 241.95652173913044\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.20000000298023224\n",
      "  episode_reward_mean: -0.45507246363854065\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 69\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.8594742254132317\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01117707111622733\n",
      "          policy_loss: -0.027636764231525984\n",
      "          total_loss: -0.020810780436607144\n",
      "          vf_explained_var: 0.12507519125938416\n",
      "          vf_loss: 0.006927407993706376\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.857532860977309\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.0072679224532870065\n",
      "          policy_loss: -0.02338154937388026\n",
      "          total_loss: -0.015036048684811206\n",
      "          vf_explained_var: 0.070526123046875\n",
      "          vf_loss: 0.006468724396351415\n",
      "    num_agent_steps_sampled: 33600\n",
      "    num_agent_steps_trained: 33600\n",
      "    num_steps_sampled: 16800\n",
      "    num_steps_trained: 16800\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.320833333333333\n",
      "    gpu_util_percent0: 0.062083333333333324\n",
      "    ram_util_percent: 54.23333333333333\n",
      "    vram_util_percent0: 0.23193766276041664\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.20000000298023224\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.2318840579710145\n",
      "    agent_1: -0.22318840566752612\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051489270239240735\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8904121037335853\n",
      "    mean_inference_ms: 1.761141658769009\n",
      "    mean_raw_obs_processing_ms: 0.1567713219913558\n",
      "  time_since_restore: 111.0691978931427\n",
      "  time_this_iter_s: 18.598743438720703\n",
      "  time_total_s: 111.0691978931427\n",
      "  timers:\n",
      "    learn_throughput: 657.888\n",
      "    learn_time_ms: 4256.041\n",
      "    load_throughput: 86597.829\n",
      "    load_time_ms: 32.333\n",
      "    sample_throughput: 197.279\n",
      "    sample_time_ms: 14193.077\n",
      "    update_time_ms: 3.763\n",
      "  timestamp: 1658495029\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16800\n",
      "  training_iteration: 6\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         111.069</td><td style=\"text-align: right;\">16800</td><td style=\"text-align: right;\">-0.455072</td><td style=\"text-align: right;\">                 0.2</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">           241.957</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 39200\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-04-07\n",
      "  done: false\n",
      "  episode_len_mean: 233.4578313253012\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.20000000298023224\n",
      "  episode_reward_mean: -0.473493975777942\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 83\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.8674230511699403\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010476256660864006\n",
      "          policy_loss: -0.031965212271426846\n",
      "          total_loss: -0.025152675555888425\n",
      "          vf_explained_var: -0.18024694919586182\n",
      "          vf_loss: 0.007898743379363233\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.8873510225897743\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.008002832293041107\n",
      "          policy_loss: -0.03853416115522296\n",
      "          total_loss: -0.02949589734219314\n",
      "          vf_explained_var: 0.24613216519355774\n",
      "          vf_loss: 0.006383809245042877\n",
      "    num_agent_steps_sampled: 39200\n",
      "    num_agent_steps_trained: 39200\n",
      "    num_steps_sampled: 19600\n",
      "    num_steps_trained: 19600\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.224999999999999\n",
      "    gpu_util_percent0: 0.05875\n",
      "    ram_util_percent: 54.4125\n",
      "    vram_util_percent0: 0.23087565104166666\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.20000000298023224\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.24096385542168675\n",
      "    agent_1: -0.23253012035625525\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051493695031056955\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.892322616394893\n",
      "    mean_inference_ms: 1.762406477759838\n",
      "    mean_raw_obs_processing_ms: 0.15719929726468915\n",
      "  time_since_restore: 129.46339631080627\n",
      "  time_this_iter_s: 18.394198417663574\n",
      "  time_total_s: 129.46339631080627\n",
      "  timers:\n",
      "    learn_throughput: 657.007\n",
      "    learn_time_ms: 4261.752\n",
      "    load_throughput: 87907.113\n",
      "    load_time_ms: 31.852\n",
      "    sample_throughput: 197.591\n",
      "    sample_time_ms: 14170.681\n",
      "    update_time_ms: 3.729\n",
      "  timestamp: 1658495047\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 19600\n",
      "  training_iteration: 7\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         129.463</td><td style=\"text-align: right;\">19600</td><td style=\"text-align: right;\">-0.473494</td><td style=\"text-align: right;\">                 0.2</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">           233.458</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 44800\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-04-26\n",
      "  done: false\n",
      "  episode_len_mean: 226.24742268041237\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.20000000298023224\n",
      "  episode_reward_mean: -0.5072164947224647\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 97\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.847076659401258\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011779278781368816\n",
      "          policy_loss: -0.08831112781522929\n",
      "          total_loss: -0.07945612417201378\n",
      "          vf_explained_var: -0.0588018037378788\n",
      "          vf_loss: 0.01184530889059672\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.8770229816436768\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.007557945921085648\n",
      "          policy_loss: -0.08046243950680253\n",
      "          total_loss: -0.07021982719778039\n",
      "          vf_explained_var: 0.052201174199581146\n",
      "          vf_loss: 0.011079563710760945\n",
      "    num_agent_steps_sampled: 44800\n",
      "    num_agent_steps_trained: 44800\n",
      "    num_steps_sampled: 22400\n",
      "    num_steps_trained: 22400\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.008000000000001\n",
      "    gpu_util_percent0: 0.0716\n",
      "    ram_util_percent: 54.088\n",
      "    vram_util_percent0: 0.22735546874999998\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.20000000298023224\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.25773195876288657\n",
      "    agent_1: -0.24948453595957806\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05149471393682514\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.894274470105999\n",
      "    mean_inference_ms: 1.763040843567121\n",
      "    mean_raw_obs_processing_ms: 0.1577229297159287\n",
      "  time_since_restore: 148.18070530891418\n",
      "  time_this_iter_s: 18.71730899810791\n",
      "  time_total_s: 148.18070530891418\n",
      "  timers:\n",
      "    learn_throughput: 654.876\n",
      "    learn_time_ms: 4275.62\n",
      "    load_throughput: 88832.798\n",
      "    load_time_ms: 31.52\n",
      "    sample_throughput: 197.399\n",
      "    sample_time_ms: 14184.504\n",
      "    update_time_ms: 3.709\n",
      "  timestamp: 1658495066\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 22400\n",
      "  training_iteration: 8\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         148.181</td><td style=\"text-align: right;\">22400</td><td style=\"text-align: right;\">-0.507216</td><td style=\"text-align: right;\">                 0.2</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">           226.247</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 50400\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-04-45\n",
      "  done: false\n",
      "  episode_len_mean: 233.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.20000000298023224\n",
      "  episode_reward_mean: -0.5109999998658895\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 107\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.8426998477606547\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011118542326161253\n",
      "          policy_loss: -0.05235042505730581\n",
      "          total_loss: -0.04561122776820931\n",
      "          vf_explained_var: 0.13808174431324005\n",
      "          vf_loss: 0.00674328914040719\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.855142437985965\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.007705693307098421\n",
      "          policy_loss: -0.01954498982688944\n",
      "          total_loss: -0.010715337575501985\n",
      "          vf_explained_var: 0.1799292266368866\n",
      "          vf_loss: 0.006598292364720865\n",
      "    num_agent_steps_sampled: 50400\n",
      "    num_agent_steps_trained: 50400\n",
      "    num_steps_sampled: 25200\n",
      "    num_steps_trained: 25200\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.824\n",
      "    gpu_util_percent0: 0.0564\n",
      "    ram_util_percent: 54.0\n",
      "    vram_util_percent0: 0.22340625\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.20000000298023224\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.26\n",
      "    agent_1: -0.25099999986588956\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05154911383292214\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.9012457539734635\n",
      "    mean_inference_ms: 1.7667846409353252\n",
      "    mean_raw_obs_processing_ms: 0.15871988044765417\n",
      "  time_since_restore: 166.78337454795837\n",
      "  time_this_iter_s: 18.60266923904419\n",
      "  time_total_s: 166.78337454795837\n",
      "  timers:\n",
      "    learn_throughput: 654.747\n",
      "    learn_time_ms: 4276.461\n",
      "    load_throughput: 89822.644\n",
      "    load_time_ms: 31.173\n",
      "    sample_throughput: 197.296\n",
      "    sample_time_ms: 14191.883\n",
      "    update_time_ms: 3.859\n",
      "  timestamp: 1658495085\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 25200\n",
      "  training_iteration: 9\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         166.783</td><td style=\"text-align: right;\">25200</td><td style=\"text-align: right;\">  -0.511</td><td style=\"text-align: right;\">                 0.2</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            233.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 56000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-05-03\n",
      "  done: false\n",
      "  episode_len_mean: 236.98\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.30000001192092896\n",
      "  episode_reward_mean: -0.5059999997168779\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 117\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.787471198609897\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.015948089949155815\n",
      "          policy_loss: -0.06242829187024091\n",
      "          total_loss: -0.05321923785266422\n",
      "          vf_explained_var: 0.1716715395450592\n",
      "          vf_loss: 0.006834959218308324\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.852558609275591\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.009386418398399784\n",
      "          policy_loss: -0.050451981577290486\n",
      "          total_loss: -0.039591675252831054\n",
      "          vf_explained_var: 0.34115299582481384\n",
      "          vf_loss: 0.007594149779054403\n",
      "    num_agent_steps_sampled: 56000\n",
      "    num_agent_steps_trained: 56000\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.845833333333334\n",
      "    gpu_util_percent0: 0.048749999999999995\n",
      "    ram_util_percent: 54.00416666666666\n",
      "    vram_util_percent0: 0.2219807942708333\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.30000001192092896\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.26\n",
      "    agent_1: -0.24599999971687794\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0515708514497967\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.9039306549112918\n",
      "    mean_inference_ms: 1.7690670932602917\n",
      "    mean_raw_obs_processing_ms: 0.15941670834923108\n",
      "  time_since_restore: 185.43612837791443\n",
      "  time_this_iter_s: 18.652753829956055\n",
      "  time_total_s: 185.43612837791443\n",
      "  timers:\n",
      "    learn_throughput: 653.675\n",
      "    learn_time_ms: 4283.476\n",
      "    load_throughput: 90598.734\n",
      "    load_time_ms: 30.906\n",
      "    sample_throughput: 197.225\n",
      "    sample_time_ms: 14196.963\n",
      "    update_time_ms: 3.773\n",
      "  timestamp: 1658495103\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 10\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         185.436</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">  -0.506</td><td style=\"text-align: right;\">                 0.3</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            236.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 61600\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-05-22\n",
      "  done: false\n",
      "  episode_len_mean: 234.77\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.30000001192092896\n",
      "  episode_reward_mean: -0.5239999996870757\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 127\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.795101460246813\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.012286398743353879\n",
      "          policy_loss: -0.05202851009865602\n",
      "          total_loss: -0.044658723331633066\n",
      "          vf_explained_var: -0.07446881383657455\n",
      "          vf_loss: 0.006819964764222927\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.8495974292357764\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.006830730318116429\n",
      "          policy_loss: -0.05134240536773134\n",
      "          total_loss: -0.04320314957848972\n",
      "          vf_explained_var: 0.12932786345481873\n",
      "          vf_loss: 0.007118585019703633\n",
      "    num_agent_steps_sampled: 61600\n",
      "    num_agent_steps_trained: 61600\n",
      "    num_steps_sampled: 30800\n",
      "    num_steps_trained: 30800\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.8759999999999994\n",
      "    gpu_util_percent0: 0.057999999999999996\n",
      "    ram_util_percent: 54.0\n",
      "    vram_util_percent0: 0.22026171875000003\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.30000001192092896\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.27\n",
      "    agent_1: -0.2539999996870756\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051575611580853546\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.903795867346147\n",
      "    mean_inference_ms: 1.7699165193289534\n",
      "    mean_raw_obs_processing_ms: 0.15949290172006875\n",
      "  time_since_restore: 204.00651621818542\n",
      "  time_this_iter_s: 18.570387840270996\n",
      "  time_total_s: 204.00651621818542\n",
      "  timers:\n",
      "    learn_throughput: 653.072\n",
      "    learn_time_ms: 4287.432\n",
      "    load_throughput: 90790.222\n",
      "    load_time_ms: 30.84\n",
      "    sample_throughput: 196.449\n",
      "    sample_time_ms: 14253.096\n",
      "    update_time_ms: 5.841\n",
      "  timestamp: 1658495122\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 30800\n",
      "  training_iteration: 11\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         204.007</td><td style=\"text-align: right;\">30800</td><td style=\"text-align: right;\">  -0.524</td><td style=\"text-align: right;\">                 0.3</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            234.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 67200\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-05-40\n",
      "  done: false\n",
      "  episode_len_mean: 249.58\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.30000001192092896\n",
      "  episode_reward_mean: -0.46499999970197675\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 137\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.761435012022654\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.016378314658005887\n",
      "          policy_loss: -0.06436470445069495\n",
      "          total_loss: -0.055634431652877195\n",
      "          vf_explained_var: 0.2635875940322876\n",
      "          vf_loss: 0.004823209644763708\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.8367904793648493\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010311366342955215\n",
      "          policy_loss: -0.06026776839003321\n",
      "          total_loss: -0.04999782742621998\n",
      "          vf_explained_var: 0.2554069757461548\n",
      "          vf_loss: 0.003249706489614688\n",
      "    num_agent_steps_sampled: 67200\n",
      "    num_agent_steps_trained: 67200\n",
      "    num_steps_sampled: 33600\n",
      "    num_steps_trained: 33600\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.841666666666666\n",
      "    gpu_util_percent0: 0.050833333333333335\n",
      "    ram_util_percent: 54.0\n",
      "    vram_util_percent0: 0.22066650390625\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.30000001192092896\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.24\n",
      "    agent_1: -0.2249999997019768\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05158907771125497\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.904007063531434\n",
      "    mean_inference_ms: 1.7705942727697306\n",
      "    mean_raw_obs_processing_ms: 0.15918814531088415\n",
      "  time_since_restore: 222.5127592086792\n",
      "  time_this_iter_s: 18.506242990493774\n",
      "  time_total_s: 222.5127592086792\n",
      "  timers:\n",
      "    learn_throughput: 654.333\n",
      "    learn_time_ms: 4279.164\n",
      "    load_throughput: 96792.615\n",
      "    load_time_ms: 28.928\n",
      "    sample_throughput: 196.91\n",
      "    sample_time_ms: 14219.661\n",
      "    update_time_ms: 5.678\n",
      "  timestamp: 1658495140\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 33600\n",
      "  training_iteration: 12\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         222.513</td><td style=\"text-align: right;\">33600</td><td style=\"text-align: right;\">  -0.465</td><td style=\"text-align: right;\">                 0.3</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            249.58</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 72800\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-05-59\n",
      "  done: false\n",
      "  episode_len_mean: 251.55\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.30000001192092896\n",
      "  episode_reward_mean: -0.4459999997168779\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 148\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.7729182768435705\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.017048518346345326\n",
      "          policy_loss: -0.06714072203405001\n",
      "          total_loss: -0.05873596184201666\n",
      "          vf_explained_var: 0.09606347978115082\n",
      "          vf_loss: 0.002951004745184383\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.8389965139684223\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011099218633665264\n",
      "          policy_loss: -0.0644130458995711\n",
      "          total_loss: -0.05362256004549896\n",
      "          vf_explained_var: 0.27835074067115784\n",
      "          vf_loss: 0.002489260301315101\n",
      "    num_agent_steps_sampled: 72800\n",
      "    num_agent_steps_trained: 72800\n",
      "    num_steps_sampled: 36400\n",
      "    num_steps_trained: 36400\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.879166666666666\n",
      "    gpu_util_percent0: 0.049999999999999996\n",
      "    ram_util_percent: 54.0\n",
      "    vram_util_percent0: 0.22007649739583335\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.30000001192092896\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.23\n",
      "    agent_1: -0.21599999971687794\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05161650704872274\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.904545108340658\n",
      "    mean_inference_ms: 1.7714590956842335\n",
      "    mean_raw_obs_processing_ms: 0.15885161749817234\n",
      "  time_since_restore: 240.9954125881195\n",
      "  time_this_iter_s: 18.482653379440308\n",
      "  time_total_s: 240.9954125881195\n",
      "  timers:\n",
      "    learn_throughput: 654.848\n",
      "    learn_time_ms: 4275.802\n",
      "    load_throughput: 97054.344\n",
      "    load_time_ms: 28.85\n",
      "    sample_throughput: 196.809\n",
      "    sample_time_ms: 14226.957\n",
      "    update_time_ms: 5.682\n",
      "  timestamp: 1658495159\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 36400\n",
      "  training_iteration: 13\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         240.995</td><td style=\"text-align: right;\">36400</td><td style=\"text-align: right;\">  -0.446</td><td style=\"text-align: right;\">                 0.3</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            251.55</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 78400\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-06-18\n",
      "  done: false\n",
      "  episode_len_mean: 251.23\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.30000001192092896\n",
      "  episode_reward_mean: -0.42399999968707563\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 161\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.740273731805029\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.014580337117602466\n",
      "          policy_loss: -0.058714831907612584\n",
      "          total_loss: -0.05029856329201721\n",
      "          vf_explained_var: -0.05157607048749924\n",
      "          vf_loss: 0.006468029076564736\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.83763833911646\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.008306900526142844\n",
      "          policy_loss: -0.048363928867946104\n",
      "          total_loss: -0.03952767542934799\n",
      "          vf_explained_var: 0.07562252134084702\n",
      "          vf_loss: 0.004880101318641599\n",
      "    num_agent_steps_sampled: 78400\n",
      "    num_agent_steps_trained: 78400\n",
      "    num_steps_sampled: 39200\n",
      "    num_steps_trained: 39200\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.904\n",
      "    gpu_util_percent0: 0.0556\n",
      "    ram_util_percent: 54.004\n",
      "    vram_util_percent0: 0.22117968750000003\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.30000001192092896\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.22\n",
      "    agent_1: -0.2039999996870756\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051635426736390426\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.9045963354903392\n",
      "    mean_inference_ms: 1.7722997087670906\n",
      "    mean_raw_obs_processing_ms: 0.1583500344230151\n",
      "  time_since_restore: 259.63531827926636\n",
      "  time_this_iter_s: 18.63990569114685\n",
      "  time_total_s: 259.63531827926636\n",
      "  timers:\n",
      "    learn_throughput: 654.652\n",
      "    learn_time_ms: 4277.083\n",
      "    load_throughput: 96840.903\n",
      "    load_time_ms: 28.913\n",
      "    sample_throughput: 196.492\n",
      "    sample_time_ms: 14249.911\n",
      "    update_time_ms: 5.703\n",
      "  timestamp: 1658495178\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 39200\n",
      "  training_iteration: 14\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         259.635</td><td style=\"text-align: right;\">39200</td><td style=\"text-align: right;\">  -0.424</td><td style=\"text-align: right;\">                 0.3</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            251.23</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 84000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-06-36\n",
      "  done: false\n",
      "  episode_len_mean: 249.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.30000001192092896\n",
      "  episode_reward_mean: -0.4039999996870756\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 171\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.701237680656569\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.016596325211390768\n",
      "          policy_loss: -0.06607803708123226\n",
      "          total_loss: -0.056464329158188775\n",
      "          vf_explained_var: -0.1311293989419937\n",
      "          vf_loss: 0.006962476947852078\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.8059035440286\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010727413051756781\n",
      "          policy_loss: -0.06825051297866074\n",
      "          total_loss: -0.05616526355756183\n",
      "          vf_explained_var: -0.012413473799824715\n",
      "          vf_loss: 0.0072074783741603215\n",
      "    num_agent_steps_sampled: 84000\n",
      "    num_agent_steps_trained: 84000\n",
      "    num_steps_sampled: 42000\n",
      "    num_steps_trained: 42000\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.820833333333333\n",
      "    gpu_util_percent0: 0.04958333333333333\n",
      "    ram_util_percent: 54.0\n",
      "    vram_util_percent0: 0.2206705729166667\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.30000001192092896\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.21\n",
      "    agent_1: -0.19399999968707562\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05165967065795627\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.9050140849823047\n",
      "    mean_inference_ms: 1.7734713371996744\n",
      "    mean_raw_obs_processing_ms: 0.1582878444032556\n",
      "  time_since_restore: 278.2890815734863\n",
      "  time_this_iter_s: 18.65376329421997\n",
      "  time_total_s: 278.2890815734863\n",
      "  timers:\n",
      "    learn_throughput: 657.805\n",
      "    learn_time_ms: 4256.577\n",
      "    load_throughput: 96924.103\n",
      "    load_time_ms: 28.889\n",
      "    sample_throughput: 196.386\n",
      "    sample_time_ms: 14257.623\n",
      "    update_time_ms: 5.675\n",
      "  timestamp: 1658495196\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 42000\n",
      "  training_iteration: 15\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         278.289</td><td style=\"text-align: right;\">42000</td><td style=\"text-align: right;\">  -0.404</td><td style=\"text-align: right;\">                 0.3</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            249.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 89600\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-06-55\n",
      "  done: false\n",
      "  episode_len_mean: 244.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.30000001192092896\n",
      "  episode_reward_mean: -0.4019999996572733\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 186\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.726239196601368\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.015575609898263147\n",
      "          policy_loss: -0.06662421201478234\n",
      "          total_loss: -0.0565053795948881\n",
      "          vf_explained_var: -0.13603349030017853\n",
      "          vf_loss: 0.009891025450737547\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.8149308498416628\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.009621057535967457\n",
      "          policy_loss: -0.04961899811695773\n",
      "          total_loss: -0.037800933578095976\n",
      "          vf_explained_var: -0.06862107664346695\n",
      "          vf_loss: 0.009613742925596722\n",
      "    num_agent_steps_sampled: 89600\n",
      "    num_agent_steps_trained: 89600\n",
      "    num_steps_sampled: 44800\n",
      "    num_steps_trained: 44800\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.843999999999999\n",
      "    gpu_util_percent0: 0.0588\n",
      "    ram_util_percent: 54.0\n",
      "    vram_util_percent0: 0.2200234375\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.30000001192092896\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.21\n",
      "    agent_1: -0.1919999996572733\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05170624500669062\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.906128094051593\n",
      "    mean_inference_ms: 1.7753013896222547\n",
      "    mean_raw_obs_processing_ms: 0.15810016837684734\n",
      "  time_since_restore: 296.918381690979\n",
      "  time_this_iter_s: 18.629300117492676\n",
      "  time_total_s: 296.918381690979\n",
      "  timers:\n",
      "    learn_throughput: 660.516\n",
      "    learn_time_ms: 4239.11\n",
      "    load_throughput: 97126.504\n",
      "    load_time_ms: 28.828\n",
      "    sample_throughput: 196.113\n",
      "    sample_time_ms: 14277.516\n",
      "    update_time_ms: 5.699\n",
      "  timestamp: 1658495215\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 44800\n",
      "  training_iteration: 16\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         296.918</td><td style=\"text-align: right;\">44800</td><td style=\"text-align: right;\">  -0.402</td><td style=\"text-align: right;\">                 0.3</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            244.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 95200\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-07-14\n",
      "  done: false\n",
      "  episode_len_mean: 242.77\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.30000001192092896\n",
      "  episode_reward_mean: -0.3609999996423721\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 200\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.6974660477467944\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.019359186665941365\n",
      "          policy_loss: -0.06767363507450293\n",
      "          total_loss: -0.056449015678178206\n",
      "          vf_explained_var: 0.2126660794019699\n",
      "          vf_loss: 0.007613089712928438\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.798106844226519\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011608938269092347\n",
      "          policy_loss: -0.06978285057786186\n",
      "          total_loss: -0.05682552856238284\n",
      "          vf_explained_var: 0.10550440102815628\n",
      "          vf_loss: 0.007171234523931551\n",
      "    num_agent_steps_sampled: 95200\n",
      "    num_agent_steps_trained: 95200\n",
      "    num_steps_sampled: 47600\n",
      "    num_steps_trained: 47600\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.856\n",
      "    gpu_util_percent0: 0.0572\n",
      "    ram_util_percent: 53.992\n",
      "    vram_util_percent0: 0.22098046875000002\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.30000001192092896\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.19\n",
      "    agent_1: -0.17099999964237214\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051750898396321174\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.9066410851069056\n",
      "    mean_inference_ms: 1.7773108201013847\n",
      "    mean_raw_obs_processing_ms: 0.157922305956948\n",
      "  time_since_restore: 315.63711380958557\n",
      "  time_this_iter_s: 18.718732118606567\n",
      "  time_total_s: 315.63711380958557\n",
      "  timers:\n",
      "    learn_throughput: 659.945\n",
      "    learn_time_ms: 4242.775\n",
      "    load_throughput: 97050.414\n",
      "    load_time_ms: 28.851\n",
      "    sample_throughput: 195.725\n",
      "    sample_time_ms: 14305.765\n",
      "    update_time_ms: 5.651\n",
      "  timestamp: 1658495234\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 47600\n",
      "  training_iteration: 17\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         315.637</td><td style=\"text-align: right;\">47600</td><td style=\"text-align: right;\">  -0.361</td><td style=\"text-align: right;\">                 0.3</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            242.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 100800\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-07-32\n",
      "  done: false\n",
      "  episode_len_mean: 245.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.10000000149011612\n",
      "  episode_reward_mean: -0.3619999997317791\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 210\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.7049558340083983\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01624389121021034\n",
      "          policy_loss: -0.06524182841531001\n",
      "          total_loss: -0.05510209367050612\n",
      "          vf_explained_var: -0.04185109958052635\n",
      "          vf_loss: 0.008971564865402928\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.799417409868467\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010301475054193035\n",
      "          policy_loss: -0.06124350342551701\n",
      "          total_loss: -0.04985934356504697\n",
      "          vf_explained_var: 0.06330536305904388\n",
      "          vf_loss: 0.006414303292331169\n",
      "    num_agent_steps_sampled: 100800\n",
      "    num_agent_steps_trained: 100800\n",
      "    num_steps_sampled: 50400\n",
      "    num_steps_trained: 50400\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.900000000000001\n",
      "    gpu_util_percent0: 0.048749999999999995\n",
      "    ram_util_percent: 54.0\n",
      "    vram_util_percent0: 0.21988932291666666\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.10000000149011612\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.19\n",
      "    agent_1: -0.1719999997317791\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05177705021126673\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.907140260508044\n",
      "    mean_inference_ms: 1.778516990716014\n",
      "    mean_raw_obs_processing_ms: 0.157872649992684\n",
      "  time_since_restore: 334.3017566204071\n",
      "  time_this_iter_s: 18.664642810821533\n",
      "  time_total_s: 334.3017566204071\n",
      "  timers:\n",
      "    learn_throughput: 660.904\n",
      "    learn_time_ms: 4236.619\n",
      "    load_throughput: 97202.874\n",
      "    load_time_ms: 28.806\n",
      "    sample_throughput: 195.721\n",
      "    sample_time_ms: 14306.073\n",
      "    update_time_ms: 5.625\n",
      "  timestamp: 1658495252\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 50400\n",
      "  training_iteration: 18\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         334.302</td><td style=\"text-align: right;\">50400</td><td style=\"text-align: right;\">  -0.362</td><td style=\"text-align: right;\">                 0.1</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            245.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 106400\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-07-51\n",
      "  done: false\n",
      "  episode_len_mean: 230.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.10000000149011612\n",
      "  episode_reward_mean: -0.4049999997764826\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 226\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.694143139180683\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.014987886487131882\n",
      "          policy_loss: -0.06509348391236494\n",
      "          total_loss: -0.05492830322847502\n",
      "          vf_explained_var: 0.08173184096813202\n",
      "          vf_loss: 0.010824108516750303\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.805671798331397\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.008963999178086036\n",
      "          policy_loss: -0.05519056088468503\n",
      "          total_loss: -0.04356134679567601\n",
      "          vf_explained_var: 0.15519218146800995\n",
      "          vf_loss: 0.010939329764022702\n",
      "    num_agent_steps_sampled: 106400\n",
      "    num_agent_steps_trained: 106400\n",
      "    num_steps_sampled: 53200\n",
      "    num_steps_trained: 53200\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.936\n",
      "    gpu_util_percent0: 0.0604\n",
      "    ram_util_percent: 54.0\n",
      "    vram_util_percent0: 0.2208046875\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.10000000149011612\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.21\n",
      "    agent_1: -0.19499999977648258\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05180541762726335\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.9077869610919675\n",
      "    mean_inference_ms: 1.780001483924417\n",
      "    mean_raw_obs_processing_ms: 0.15821160679407323\n",
      "  time_since_restore: 352.98010754585266\n",
      "  time_this_iter_s: 18.678350925445557\n",
      "  time_total_s: 352.98010754585266\n",
      "  timers:\n",
      "    learn_throughput: 659.319\n",
      "    learn_time_ms: 4246.806\n",
      "    load_throughput: 96997.591\n",
      "    load_time_ms: 28.867\n",
      "    sample_throughput: 195.761\n",
      "    sample_time_ms: 14303.157\n",
      "    update_time_ms: 5.464\n",
      "  timestamp: 1658495271\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 53200\n",
      "  training_iteration: 19\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">          352.98</td><td style=\"text-align: right;\">53200</td><td style=\"text-align: right;\">  -0.405</td><td style=\"text-align: right;\">                 0.1</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            230.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 112000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-08-10\n",
      "  done: false\n",
      "  episode_len_mean: 216.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.10000000149011612\n",
      "  episode_reward_mean: -0.4239999997615814\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 240\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.672430906267393\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.017203447378133226\n",
      "          policy_loss: -0.05222217739910342\n",
      "          total_loss: -0.043001247137518864\n",
      "          vf_explained_var: -0.09072025865316391\n",
      "          vf_loss: 0.004940464446138192\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.8060602453492938\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010990247531977092\n",
      "          policy_loss: -0.05154619923615385\n",
      "          total_loss: -0.039593237191184016\n",
      "          vf_explained_var: -0.03938562050461769\n",
      "          vf_loss: 0.006079710754073901\n",
      "    num_agent_steps_sampled: 112000\n",
      "    num_agent_steps_trained: 112000\n",
      "    num_steps_sampled: 56000\n",
      "    num_steps_trained: 56000\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.864000000000001\n",
      "    gpu_util_percent0: 0.0556\n",
      "    ram_util_percent: 54.004\n",
      "    vram_util_percent0: 0.22111328125\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.10000000149011612\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.22\n",
      "    agent_1: -0.20399999976158142\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051824024554249844\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.9079614976779617\n",
      "    mean_inference_ms: 1.7813248966118322\n",
      "    mean_raw_obs_processing_ms: 0.1587678321737178\n",
      "  time_since_restore: 371.72524189949036\n",
      "  time_this_iter_s: 18.745134353637695\n",
      "  time_total_s: 371.72524189949036\n",
      "  timers:\n",
      "    learn_throughput: 659.155\n",
      "    learn_time_ms: 4247.864\n",
      "    load_throughput: 96846.413\n",
      "    load_time_ms: 28.912\n",
      "    sample_throughput: 195.659\n",
      "    sample_time_ms: 14310.612\n",
      "    update_time_ms: 5.484\n",
      "  timestamp: 1658495290\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 56000\n",
      "  training_iteration: 20\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         371.725</td><td style=\"text-align: right;\">56000</td><td style=\"text-align: right;\">  -0.424</td><td style=\"text-align: right;\">                 0.1</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             216.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 117600\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-08-29\n",
      "  done: false\n",
      "  episode_len_mean: 216.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.10000000149011612\n",
      "  episode_reward_mean: -0.5039999997615814\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 252\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.6588120070241748\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.013903334791062724\n",
      "          policy_loss: -0.0608640379907142\n",
      "          total_loss: -0.052797716339617703\n",
      "          vf_explained_var: -0.27215686440467834\n",
      "          vf_loss: 0.006338625804606376\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.810074115083331\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.009097522150064834\n",
      "          policy_loss: -0.04769584485135662\n",
      "          total_loss: -0.037506053947210946\n",
      "          vf_explained_var: -0.003775165183469653\n",
      "          vf_loss: 0.0064543159661254665\n",
      "    num_agent_steps_sampled: 117600\n",
      "    num_agent_steps_trained: 117600\n",
      "    num_steps_sampled: 58800\n",
      "    num_steps_trained: 58800\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.927999999999998\n",
      "    gpu_util_percent0: 0.0564\n",
      "    ram_util_percent: 54.008\n",
      "    vram_util_percent0: 0.219828125\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.10000000149011612\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.26\n",
      "    agent_1: -0.24399999976158143\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051840523621415086\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.908073928446549\n",
      "    mean_inference_ms: 1.7825622377901562\n",
      "    mean_raw_obs_processing_ms: 0.15924543747072728\n",
      "  time_since_restore: 390.58963203430176\n",
      "  time_this_iter_s: 18.8643901348114\n",
      "  time_total_s: 390.58963203430176\n",
      "  timers:\n",
      "    learn_throughput: 656.903\n",
      "    learn_time_ms: 4262.422\n",
      "    load_throughput: 97131.966\n",
      "    load_time_ms: 28.827\n",
      "    sample_throughput: 195.463\n",
      "    sample_time_ms: 14324.958\n",
      "    update_time_ms: 5.503\n",
      "  timestamp: 1658495309\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 58800\n",
      "  training_iteration: 21\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">          390.59</td><td style=\"text-align: right;\">58800</td><td style=\"text-align: right;\">  -0.504</td><td style=\"text-align: right;\">                 0.1</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            216.31</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 123200\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-08-48\n",
      "  done: false\n",
      "  episode_len_mean: 218.98\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.10000000149011612\n",
      "  episode_reward_mean: -0.5239999997615814\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 262\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.6618598827293942\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.0160256173422982\n",
      "          policy_loss: -0.04735753886351241\n",
      "          total_loss: -0.03851708044073478\n",
      "          vf_explained_var: 0.12486032396554947\n",
      "          vf_loss: 0.005522944439213634\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.7671008833817075\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010309570063533537\n",
      "          policy_loss: -0.061378272065277475\n",
      "          total_loss: -0.05003928049700335\n",
      "          vf_explained_var: 0.24133023619651794\n",
      "          vf_loss: 0.006223881234507731\n",
      "    num_agent_steps_sampled: 123200\n",
      "    num_agent_steps_trained: 123200\n",
      "    num_steps_sampled: 61600\n",
      "    num_steps_trained: 61600\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.891666666666667\n",
      "    gpu_util_percent0: 0.04958333333333333\n",
      "    ram_util_percent: 54.24583333333334\n",
      "    vram_util_percent0: 0.2206705729166667\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.10000000149011612\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.27\n",
      "    agent_1: -0.25399999976158144\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051853613273190706\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.9081979706265066\n",
      "    mean_inference_ms: 1.783412286327103\n",
      "    mean_raw_obs_processing_ms: 0.15954002063661804\n",
      "  time_since_restore: 409.2816860675812\n",
      "  time_this_iter_s: 18.69205403327942\n",
      "  time_total_s: 409.2816860675812\n",
      "  timers:\n",
      "    learn_throughput: 654.335\n",
      "    learn_time_ms: 4279.15\n",
      "    load_throughput: 97087.882\n",
      "    load_time_ms: 28.84\n",
      "    sample_throughput: 195.447\n",
      "    sample_time_ms: 14326.107\n",
      "    update_time_ms: 5.523\n",
      "  timestamp: 1658495328\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 61600\n",
      "  training_iteration: 22\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    22</td><td style=\"text-align: right;\">         409.282</td><td style=\"text-align: right;\">61600</td><td style=\"text-align: right;\">  -0.524</td><td style=\"text-align: right;\">                 0.1</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            218.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 128800\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-09-06\n",
      "  done: false\n",
      "  episode_len_mean: 218.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.10000000149011612\n",
      "  episode_reward_mean: -0.5229999997466802\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 276\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.6494862834612527\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.016594173642793475\n",
      "          policy_loss: -0.12077929475509293\n",
      "          total_loss: -0.11181505461007916\n",
      "          vf_explained_var: 0.005220929626375437\n",
      "          vf_loss: 0.005050092848943591\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.7732538148051216\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010745956320484775\n",
      "          policy_loss: -0.10550979246424201\n",
      "          total_loss: -0.09393226170158457\n",
      "          vf_explained_var: 0.3002100884914398\n",
      "          vf_loss: 0.005666407494919397\n",
      "    num_agent_steps_sampled: 128800\n",
      "    num_agent_steps_trained: 128800\n",
      "    num_steps_sampled: 64400\n",
      "    num_steps_trained: 64400\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.868\n",
      "    gpu_util_percent0: 0.0516\n",
      "    ram_util_percent: 54.148\n",
      "    vram_util_percent0: 0.22001562500000002\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.10000000149011612\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.27\n",
      "    agent_1: -0.25299999974668025\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0518708424354493\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.9081837662430132\n",
      "    mean_inference_ms: 1.784560234929104\n",
      "    mean_raw_obs_processing_ms: 0.16004790843667877\n",
      "  time_since_restore: 428.1898021697998\n",
      "  time_this_iter_s: 18.908116102218628\n",
      "  time_total_s: 428.1898021697998\n",
      "  timers:\n",
      "    learn_throughput: 650.994\n",
      "    learn_time_ms: 4301.114\n",
      "    load_throughput: 96933.863\n",
      "    load_time_ms: 28.886\n",
      "    sample_throughput: 195.177\n",
      "    sample_time_ms: 14345.93\n",
      "    update_time_ms: 5.515\n",
      "  timestamp: 1658495346\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 64400\n",
      "  training_iteration: 23\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">          428.19</td><td style=\"text-align: right;\">64400</td><td style=\"text-align: right;\">  -0.523</td><td style=\"text-align: right;\">                 0.1</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            218.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 134400\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-09-25\n",
      "  done: false\n",
      "  episode_len_mean: 227.59\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.10000000149011612\n",
      "  episode_reward_mean: -0.4649999997764826\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 286\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.6499138055812743\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.016827860801355055\n",
      "          policy_loss: -0.1396757485345006\n",
      "          total_loss: -0.13121524705101842\n",
      "          vf_explained_var: -0.13648110628128052\n",
      "          vf_loss: 0.0032790474806028043\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.758085798649561\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.012172670068758159\n",
      "          policy_loss: -0.12723107848710025\n",
      "          total_loss: -0.11484089444690783\n",
      "          vf_explained_var: 0.10241521894931793\n",
      "          vf_loss: 0.0038954517691482103\n",
      "    num_agent_steps_sampled: 134400\n",
      "    num_agent_steps_trained: 134400\n",
      "    num_steps_sampled: 67200\n",
      "    num_steps_trained: 67200\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.884000000000001\n",
      "    gpu_util_percent0: 0.0484\n",
      "    ram_util_percent: 54.04\n",
      "    vram_util_percent0: 0.219921875\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.10000000149011612\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.24\n",
      "    agent_1: -0.22499999977648258\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05188419356395645\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.908278081627499\n",
      "    mean_inference_ms: 1.7855189755373428\n",
      "    mean_raw_obs_processing_ms: 0.16027493067349505\n",
      "  time_since_restore: 447.1002402305603\n",
      "  time_this_iter_s: 18.910438060760498\n",
      "  time_total_s: 447.1002402305603\n",
      "  timers:\n",
      "    learn_throughput: 648.671\n",
      "    learn_time_ms: 4316.516\n",
      "    load_throughput: 97242.553\n",
      "    load_time_ms: 28.794\n",
      "    sample_throughput: 195.025\n",
      "    sample_time_ms: 14357.13\n",
      "    update_time_ms: 5.551\n",
      "  timestamp: 1658495365\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 67200\n",
      "  training_iteration: 24\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    24</td><td style=\"text-align: right;\">           447.1</td><td style=\"text-align: right;\">67200</td><td style=\"text-align: right;\">  -0.465</td><td style=\"text-align: right;\">                 0.1</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            227.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 140000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-09-44\n",
      "  done: false\n",
      "  episode_len_mean: 228.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.10000000149011612\n",
      "  episode_reward_mean: -0.4649999997764826\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 297\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.6546784412293207\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.015671445105402956\n",
      "          policy_loss: -0.06950696947318474\n",
      "          total_loss: -0.06085405331534622\n",
      "          vf_explained_var: -0.16215577721595764\n",
      "          vf_loss: 0.005484571104413287\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.728767509971346\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011164238144314952\n",
      "          policy_loss: -0.07464556156732474\n",
      "          total_loss: -0.0628383187757295\n",
      "          vf_explained_var: 0.10524056106805801\n",
      "          vf_loss: 0.0050752851417374106\n",
      "    num_agent_steps_sampled: 140000\n",
      "    num_agent_steps_trained: 140000\n",
      "    num_steps_sampled: 70000\n",
      "    num_steps_trained: 70000\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.851999999999999\n",
      "    gpu_util_percent0: 0.056799999999999996\n",
      "    ram_util_percent: 54.02\n",
      "    vram_util_percent0: 0.21992578124999998\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.10000000149011612\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.24\n",
      "    agent_1: -0.22499999977648258\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05190062000481141\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.9084219216831935\n",
      "    mean_inference_ms: 1.7865916042269185\n",
      "    mean_raw_obs_processing_ms: 0.1604047647532472\n",
      "  time_since_restore: 465.9444751739502\n",
      "  time_this_iter_s: 18.844234943389893\n",
      "  time_total_s: 465.9444751739502\n",
      "  timers:\n",
      "    learn_throughput: 645.564\n",
      "    learn_time_ms: 4337.293\n",
      "    load_throughput: 96902.59\n",
      "    load_time_ms: 28.895\n",
      "    sample_throughput: 195.057\n",
      "    sample_time_ms: 14354.795\n",
      "    update_time_ms: 5.532\n",
      "  timestamp: 1658495384\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 70000\n",
      "  training_iteration: 25\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         465.944</td><td style=\"text-align: right;\">70000</td><td style=\"text-align: right;\">  -0.465</td><td style=\"text-align: right;\">                 0.1</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             228.7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 145600\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-10-03\n",
      "  done: false\n",
      "  episode_len_mean: 234.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.10000000149011612\n",
      "  episode_reward_mean: -0.44399999976158144\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 307\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.6584222799255732\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.017161087348841236\n",
      "          policy_loss: -0.08632071796914499\n",
      "          total_loss: -0.07691592561726186\n",
      "          vf_explained_var: -0.2883186936378479\n",
      "          vf_loss: 0.005509157799938943\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.7204626160008565\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011520146313487285\n",
      "          policy_loss: -0.09454628152854552\n",
      "          total_loss: -0.08248428562776444\n",
      "          vf_explained_var: -0.1748742163181305\n",
      "          vf_loss: 0.004776663389271735\n",
      "    num_agent_steps_sampled: 145600\n",
      "    num_agent_steps_trained: 145600\n",
      "    num_steps_sampled: 72800\n",
      "    num_steps_trained: 72800\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.864000000000001\n",
      "    gpu_util_percent0: 0.057999999999999996\n",
      "    ram_util_percent: 54.071999999999996\n",
      "    vram_util_percent0: 0.21880078125000002\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.10000000149011612\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.23\n",
      "    agent_1: -0.21399999976158143\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05191662278007101\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.908517959791112\n",
      "    mean_inference_ms: 1.787541822722947\n",
      "    mean_raw_obs_processing_ms: 0.16052533236768377\n",
      "  time_since_restore: 484.69834899902344\n",
      "  time_this_iter_s: 18.753873825073242\n",
      "  time_total_s: 484.69834899902344\n",
      "  timers:\n",
      "    learn_throughput: 644.264\n",
      "    learn_time_ms: 4346.045\n",
      "    load_throughput: 96532.61\n",
      "    load_time_ms: 29.006\n",
      "    sample_throughput: 195.043\n",
      "    sample_time_ms: 14355.819\n",
      "    update_time_ms: 5.503\n",
      "  timestamp: 1658495403\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 72800\n",
      "  training_iteration: 26\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    26</td><td style=\"text-align: right;\">         484.698</td><td style=\"text-align: right;\">72800</td><td style=\"text-align: right;\">  -0.444</td><td style=\"text-align: right;\">                 0.1</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            234.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 151200\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-10-22\n",
      "  done: false\n",
      "  episode_len_mean: 237.61\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.10000000149011612\n",
      "  episode_reward_mean: -0.4039999997615814\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 315\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.6188585545335497\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.016997170127222246\n",
      "          policy_loss: -0.08125027488018705\n",
      "          total_loss: -0.07201079516060072\n",
      "          vf_explained_var: -0.20331910252571106\n",
      "          vf_loss: 0.005224236939760712\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.717887195093291\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.013322163112573606\n",
      "          policy_loss: -0.07843235966200675\n",
      "          total_loss: -0.06470939718461263\n",
      "          vf_explained_var: -0.0887146145105362\n",
      "          vf_loss: 0.004370946061542435\n",
      "    num_agent_steps_sampled: 151200\n",
      "    num_agent_steps_trained: 151200\n",
      "    num_steps_sampled: 75600\n",
      "    num_steps_trained: 75600\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.104166666666667\n",
      "    gpu_util_percent0: 0.049999999999999996\n",
      "    ram_util_percent: 54.20000000000001\n",
      "    vram_util_percent0: 0.2175944010416667\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.10000000149011612\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.21\n",
      "    agent_1: -0.19399999976158142\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05193043593057141\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.9087891949966003\n",
      "    mean_inference_ms: 1.78831114133484\n",
      "    mean_raw_obs_processing_ms: 0.1604989205026615\n",
      "  time_since_restore: 503.37255477905273\n",
      "  time_this_iter_s: 18.674205780029297\n",
      "  time_total_s: 503.37255477905273\n",
      "  timers:\n",
      "    learn_throughput: 645.819\n",
      "    learn_time_ms: 4335.58\n",
      "    load_throughput: 96228.877\n",
      "    load_time_ms: 29.097\n",
      "    sample_throughput: 194.972\n",
      "    sample_time_ms: 14361.051\n",
      "    update_time_ms: 5.514\n",
      "  timestamp: 1658495422\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 75600\n",
      "  training_iteration: 27\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         503.373</td><td style=\"text-align: right;\">75600</td><td style=\"text-align: right;\">  -0.404</td><td style=\"text-align: right;\">                 0.1</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            237.61</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 156800\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-10-41\n",
      "  done: false\n",
      "  episode_len_mean: 251.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.10000000149011612\n",
      "  episode_reward_mean: -0.3429999997466803\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 326\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.6064666282562983\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.017107807157097584\n",
      "          policy_loss: -0.07098511594016427\n",
      "          total_loss: -0.061262580886510216\n",
      "          vf_explained_var: -0.2338351011276245\n",
      "          vf_loss: 0.006430573972379318\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.712306193652607\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011386385331375768\n",
      "          policy_loss: -0.08412066479407561\n",
      "          total_loss: -0.07134801555077631\n",
      "          vf_explained_var: -0.018170345574617386\n",
      "          vf_loss: 0.00717749743229693\n",
      "    num_agent_steps_sampled: 156800\n",
      "    num_agent_steps_trained: 156800\n",
      "    num_steps_sampled: 78400\n",
      "    num_steps_trained: 78400\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.876\n",
      "    gpu_util_percent0: 0.0524\n",
      "    ram_util_percent: 54.19200000000001\n",
      "    vram_util_percent0: 0.21650781249999998\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.10000000149011612\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.18\n",
      "    agent_1: -0.16299999974668025\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05195202963956519\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.909447143543598\n",
      "    mean_inference_ms: 1.7894029918467618\n",
      "    mean_raw_obs_processing_ms: 0.16039204564194687\n",
      "  time_since_restore: 522.1689903736115\n",
      "  time_this_iter_s: 18.796435594558716\n",
      "  time_total_s: 522.1689903736115\n",
      "  timers:\n",
      "    learn_throughput: 646.652\n",
      "    learn_time_ms: 4329.998\n",
      "    load_throughput: 96316.004\n",
      "    load_time_ms: 29.071\n",
      "    sample_throughput: 194.721\n",
      "    sample_time_ms: 14379.52\n",
      "    update_time_ms: 5.482\n",
      "  timestamp: 1658495441\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 78400\n",
      "  training_iteration: 28\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    28</td><td style=\"text-align: right;\">         522.169</td><td style=\"text-align: right;\">78400</td><td style=\"text-align: right;\">  -0.343</td><td style=\"text-align: right;\">                 0.1</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            251.64</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 162400\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-10-59\n",
      "  done: false\n",
      "  episode_len_mean: 259.17\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.10000000149011612\n",
      "  episode_reward_mean: -0.36099999971687796\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 335\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.5650897409234727\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.019514164364475044\n",
      "          policy_loss: -0.07586705979580681\n",
      "          total_loss: -0.06549244523159273\n",
      "          vf_explained_var: -0.11498187482357025\n",
      "          vf_loss: 0.0048082473487219045\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.724324901898702\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.012724333270965307\n",
      "          policy_loss: -0.07000496062902953\n",
      "          total_loss: -0.056660634737194675\n",
      "          vf_explained_var: 0.14343510568141937\n",
      "          vf_loss: 0.005004318106200246\n",
      "    num_agent_steps_sampled: 162400\n",
      "    num_agent_steps_trained: 162400\n",
      "    num_steps_sampled: 81200\n",
      "    num_steps_trained: 81200\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.841666666666668\n",
      "    gpu_util_percent0: 0.050416666666666665\n",
      "    ram_util_percent: 54.116666666666674\n",
      "    vram_util_percent0: 0.2173421223958333\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.10000000149011612\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.19\n",
      "    agent_1: -0.17099999971687793\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05196818711144177\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.9100586931795975\n",
      "    mean_inference_ms: 1.7901502669796243\n",
      "    mean_raw_obs_processing_ms: 0.1601840398623427\n",
      "  time_since_restore: 540.6433727741241\n",
      "  time_this_iter_s: 18.474382400512695\n",
      "  time_total_s: 540.6433727741241\n",
      "  timers:\n",
      "    learn_throughput: 649.481\n",
      "    learn_time_ms: 4311.136\n",
      "    load_throughput: 96444.774\n",
      "    load_time_ms: 29.032\n",
      "    sample_throughput: 194.744\n",
      "    sample_time_ms: 14377.869\n",
      "    update_time_ms: 5.453\n",
      "  timestamp: 1658495459\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 81200\n",
      "  training_iteration: 29\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         540.643</td><td style=\"text-align: right;\">81200</td><td style=\"text-align: right;\">  -0.361</td><td style=\"text-align: right;\">                 0.1</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            259.17</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 168000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-11-18\n",
      "  done: false\n",
      "  episode_len_mean: 265.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.10000000149011612\n",
      "  episode_reward_mean: -0.3989999996870756\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 347\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.5693922291199365\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.014085141005019336\n",
      "          policy_loss: -0.10213280880738498\n",
      "          total_loss: -0.09404573146024459\n",
      "          vf_explained_var: -0.2279123067855835\n",
      "          vf_loss: 0.006032220937153657\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.724022621200198\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010110713165708538\n",
      "          policy_loss: -0.1011363595385947\n",
      "          total_loss: -0.09016801971648376\n",
      "          vf_explained_var: -0.2115946263074875\n",
      "          vf_loss: 0.005682322209135496\n",
      "    num_agent_steps_sampled: 168000\n",
      "    num_agent_steps_trained: 168000\n",
      "    num_steps_sampled: 84000\n",
      "    num_steps_trained: 84000\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.88\n",
      "    gpu_util_percent0: 0.062\n",
      "    ram_util_percent: 54.132\n",
      "    vram_util_percent0: 0.21746484374999997\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.10000000149011612\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.21\n",
      "    agent_1: -0.18899999968707562\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05199015498102648\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.910845671779531\n",
      "    mean_inference_ms: 1.791017655955824\n",
      "    mean_raw_obs_processing_ms: 0.1599178279418805\n",
      "  time_since_restore: 559.2604458332062\n",
      "  time_this_iter_s: 18.61707305908203\n",
      "  time_total_s: 559.2604458332062\n",
      "  timers:\n",
      "    learn_throughput: 652.852\n",
      "    learn_time_ms: 4288.876\n",
      "    load_throughput: 96512.857\n",
      "    load_time_ms: 29.012\n",
      "    sample_throughput: 194.619\n",
      "    sample_time_ms: 14387.078\n",
      "    update_time_ms: 5.47\n",
      "  timestamp: 1658495478\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 84000\n",
      "  training_iteration: 30\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">          559.26</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\">  -0.399</td><td style=\"text-align: right;\">                 0.1</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            265.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 173600\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-11-37\n",
      "  done: false\n",
      "  episode_len_mean: 268.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.10000000149011612\n",
      "  episode_reward_mean: -0.35899999968707563\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 357\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.5746873226903735\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.0179493925419144\n",
      "          policy_loss: -0.07294340592177052\n",
      "          total_loss: -0.06212310748482456\n",
      "          vf_explained_var: 0.0281378086656332\n",
      "          vf_loss: 0.008325369085339065\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.7013879872503734\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011441461707539074\n",
      "          policy_loss: -0.07191757359431081\n",
      "          total_loss: -0.05910037521278441\n",
      "          vf_explained_var: -0.1272420585155487\n",
      "          vf_loss: 0.0071344798529718815\n",
      "    num_agent_steps_sampled: 173600\n",
      "    num_agent_steps_trained: 173600\n",
      "    num_steps_sampled: 86800\n",
      "    num_steps_trained: 86800\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.8999999999999995\n",
      "    gpu_util_percent0: 0.049999999999999996\n",
      "    ram_util_percent: 54.1\n",
      "    vram_util_percent0: 0.2163736979166667\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.10000000149011612\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.19\n",
      "    agent_1: -0.1689999996870756\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.052009169311542036\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.911498299141197\n",
      "    mean_inference_ms: 1.7917223920540841\n",
      "    mean_raw_obs_processing_ms: 0.15969238499588154\n",
      "  time_since_restore: 577.9804999828339\n",
      "  time_this_iter_s: 18.720054149627686\n",
      "  time_total_s: 577.9804999828339\n",
      "  timers:\n",
      "    learn_throughput: 654.826\n",
      "    learn_time_ms: 4275.948\n",
      "    load_throughput: 96387.624\n",
      "    load_time_ms: 29.049\n",
      "    sample_throughput: 194.6\n",
      "    sample_time_ms: 14388.489\n",
      "    update_time_ms: 3.244\n",
      "  timestamp: 1658495497\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 86800\n",
      "  training_iteration: 31\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">          577.98</td><td style=\"text-align: right;\">86800</td><td style=\"text-align: right;\">  -0.359</td><td style=\"text-align: right;\">                 0.1</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            268.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 179200\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-11-55\n",
      "  done: false\n",
      "  episode_len_mean: 263.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.20000000298023224\n",
      "  episode_reward_mean: -0.39599999964237215\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 371\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.558931073262578\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.017643119402871214\n",
      "          policy_loss: -0.08156819855052856\n",
      "          total_loss: -0.07106506086988486\n",
      "          vf_explained_var: -0.1210523247718811\n",
      "          vf_loss: 0.007838423952004328\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.7023586751449677\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.014049371121474625\n",
      "          policy_loss: -0.08393389496095119\n",
      "          total_loss: -0.0690398902988818\n",
      "          vf_explained_var: -0.07486648857593536\n",
      "          vf_loss: 0.005619522089892555\n",
      "    num_agent_steps_sampled: 179200\n",
      "    num_agent_steps_trained: 179200\n",
      "    num_steps_sampled: 89600\n",
      "    num_steps_trained: 89600\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.864\n",
      "    gpu_util_percent0: 0.052000000000000005\n",
      "    ram_util_percent: 54.1\n",
      "    vram_util_percent0: 0.2172890625\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.20000000298023224\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.21\n",
      "    agent_1: -0.18599999964237213\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05203536962433722\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.9123704222004405\n",
      "    mean_inference_ms: 1.792588158411407\n",
      "    mean_raw_obs_processing_ms: 0.15943009209169684\n",
      "  time_since_restore: 596.8661806583405\n",
      "  time_this_iter_s: 18.885680675506592\n",
      "  time_total_s: 596.8661806583405\n",
      "  timers:\n",
      "    learn_throughput: 655.004\n",
      "    learn_time_ms: 4274.781\n",
      "    load_throughput: 89808.776\n",
      "    load_time_ms: 31.177\n",
      "    sample_throughput: 194.356\n",
      "    sample_time_ms: 14406.568\n",
      "    update_time_ms: 3.232\n",
      "  timestamp: 1658495515\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 89600\n",
      "  training_iteration: 32\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">         596.866</td><td style=\"text-align: right;\">89600</td><td style=\"text-align: right;\">  -0.396</td><td style=\"text-align: right;\">                 0.2</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            263.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 184800\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-12-14\n",
      "  done: false\n",
      "  episode_len_mean: 258.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.20000000298023224\n",
      "  episode_reward_mean: -0.39599999964237215\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 382\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.559173184491339\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01791442465031558\n",
      "          policy_loss: -0.0582026739298488\n",
      "          total_loss: -0.048249729273395915\n",
      "          vf_explained_var: 0.14417465031147003\n",
      "          vf_loss: 0.005880847292982729\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.706104177094641\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.012162653029838353\n",
      "          policy_loss: -0.04291548415044201\n",
      "          total_loss: -0.029949050515867947\n",
      "          vf_explained_var: 0.19450271129608154\n",
      "          vf_loss: 0.005507337653571518\n",
      "    num_agent_steps_sampled: 184800\n",
      "    num_agent_steps_trained: 184800\n",
      "    num_steps_sampled: 92400\n",
      "    num_steps_trained: 92400\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.88\n",
      "    gpu_util_percent0: 0.0492\n",
      "    ram_util_percent: 54.168\n",
      "    vram_util_percent0: 0.21685546875\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.20000000298023224\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.21\n",
      "    agent_1: -0.18599999964237213\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05205480800566727\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.913145714036003\n",
      "    mean_inference_ms: 1.793165844749837\n",
      "    mean_raw_obs_processing_ms: 0.1592130948826073\n",
      "  time_since_restore: 615.7884166240692\n",
      "  time_this_iter_s: 18.92223596572876\n",
      "  time_total_s: 615.7884166240692\n",
      "  timers:\n",
      "    learn_throughput: 656.09\n",
      "    learn_time_ms: 4267.706\n",
      "    load_throughput: 89469.01\n",
      "    load_time_ms: 31.296\n",
      "    sample_throughput: 194.251\n",
      "    sample_time_ms: 14414.33\n",
      "    update_time_ms: 3.211\n",
      "  timestamp: 1658495534\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 92400\n",
      "  training_iteration: 33\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">         615.788</td><td style=\"text-align: right;\">92400</td><td style=\"text-align: right;\">  -0.396</td><td style=\"text-align: right;\">                 0.2</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            258.03</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 190400\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-12-33\n",
      "  done: false\n",
      "  episode_len_mean: 258.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.20000000298023224\n",
      "  episode_reward_mean: -0.354999999627471\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 395\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.5169475915886106\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.020100642261974843\n",
      "          policy_loss: -0.08484527047368742\n",
      "          total_loss: -0.07401422115175851\n",
      "          vf_explained_var: 0.0316477008163929\n",
      "          vf_loss: 0.005216950419708155\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.698287110953104\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.014156816887291635\n",
      "          policy_loss: -0.08520049813385067\n",
      "          total_loss: -0.07035179063608493\n",
      "          vf_explained_var: -0.021947123110294342\n",
      "          vf_loss: 0.005178654026253969\n",
      "    num_agent_steps_sampled: 190400\n",
      "    num_agent_steps_trained: 190400\n",
      "    num_steps_sampled: 95200\n",
      "    num_steps_trained: 95200\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.8919999999999995\n",
      "    gpu_util_percent0: 0.0532\n",
      "    ram_util_percent: 54.18\n",
      "    vram_util_percent0: 0.217265625\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.20000000298023224\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.19\n",
      "    agent_1: -0.16499999962747097\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0520758785028133\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.9139544075208987\n",
      "    mean_inference_ms: 1.7936278523129854\n",
      "    mean_raw_obs_processing_ms: 0.1590360269330001\n",
      "  time_since_restore: 634.528790473938\n",
      "  time_this_iter_s: 18.740373849868774\n",
      "  time_total_s: 634.528790473938\n",
      "  timers:\n",
      "    learn_throughput: 657.586\n",
      "    learn_time_ms: 4257.997\n",
      "    load_throughput: 89191.916\n",
      "    load_time_ms: 31.393\n",
      "    sample_throughput: 194.356\n",
      "    sample_time_ms: 14406.538\n",
      "    update_time_ms: 3.165\n",
      "  timestamp: 1658495553\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 95200\n",
      "  training_iteration: 34\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    34</td><td style=\"text-align: right;\">         634.529</td><td style=\"text-align: right;\">95200</td><td style=\"text-align: right;\">  -0.355</td><td style=\"text-align: right;\">                 0.2</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            258.31</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 196000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-12-52\n",
      "  done: false\n",
      "  episode_len_mean: 256.42\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.5000000074505806\n",
      "  episode_reward_mean: -0.36999999955296514\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 406\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.5442846977994558\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.020034096011001385\n",
      "          policy_loss: -0.046280792647762393\n",
      "          total_loss: -0.03503892106735813\n",
      "          vf_explained_var: 0.007872899062931538\n",
      "          vf_loss: 0.006517153195870508\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.679442116192409\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.014449564087891739\n",
      "          policy_loss: -0.06354038351933664\n",
      "          total_loss: -0.04814804807427295\n",
      "          vf_explained_var: 0.08720879256725311\n",
      "          vf_loss: 0.005872455561724824\n",
      "    num_agent_steps_sampled: 196000\n",
      "    num_agent_steps_trained: 196000\n",
      "    num_steps_sampled: 98000\n",
      "    num_steps_trained: 98000\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.837500000000001\n",
      "    gpu_util_percent0: 0.048749999999999995\n",
      "    ram_util_percent: 54.19166666666667\n",
      "    vram_util_percent0: 0.21753743489583333\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.5000000074505806\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.2\n",
      "    agent_1: -0.16999999955296516\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.052091369314624665\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.9146000675602597\n",
      "    mean_inference_ms: 1.7938755331591414\n",
      "    mean_raw_obs_processing_ms: 0.158916442023518\n",
      "  time_since_restore: 653.0836853981018\n",
      "  time_this_iter_s: 18.55489492416382\n",
      "  time_total_s: 653.0836853981018\n",
      "  timers:\n",
      "    learn_throughput: 659.501\n",
      "    learn_time_ms: 4245.632\n",
      "    load_throughput: 89474.736\n",
      "    load_time_ms: 31.294\n",
      "    sample_throughput: 194.583\n",
      "    sample_time_ms: 14389.773\n",
      "    update_time_ms: 3.163\n",
      "  timestamp: 1658495572\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 98000\n",
      "  training_iteration: 35\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         653.084</td><td style=\"text-align: right;\">98000</td><td style=\"text-align: right;\">   -0.37</td><td style=\"text-align: right;\">                 0.5</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            256.42</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 201600\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-13-10\n",
      "  done: false\n",
      "  episode_len_mean: 246.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.5000000074505806\n",
      "  episode_reward_mean: -0.328999999538064\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 417\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.490496815670104\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.017002431124330118\n",
      "          policy_loss: -0.12944666207199806\n",
      "          total_loss: -0.1206999073086384\n",
      "          vf_explained_var: -0.21182221174240112\n",
      "          vf_loss: 0.0036577651024431576\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.6634966901370456\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011986231232081465\n",
      "          policy_loss: -0.1350377537620218\n",
      "          total_loss: -0.12314339550392192\n",
      "          vf_explained_var: -0.17579717934131622\n",
      "          vf_loss: 0.002899957874130147\n",
      "    num_agent_steps_sampled: 201600\n",
      "    num_agent_steps_trained: 201600\n",
      "    num_steps_sampled: 100800\n",
      "    num_steps_trained: 100800\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.912000000000001\n",
      "    gpu_util_percent0: 0.06119999999999999\n",
      "    ram_util_percent: 54.16\n",
      "    vram_util_percent0: 0.21685156249999998\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.5000000074505806\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.18\n",
      "    agent_1: -0.148999999538064\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.052104240336317546\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.9149766671539714\n",
      "    mean_inference_ms: 1.7940605031153172\n",
      "    mean_raw_obs_processing_ms: 0.15887758753963385\n",
      "  time_since_restore: 671.6597926616669\n",
      "  time_this_iter_s: 18.576107263565063\n",
      "  time_total_s: 671.6597926616669\n",
      "  timers:\n",
      "    learn_throughput: 659.39\n",
      "    learn_time_ms: 4246.349\n",
      "    load_throughput: 89833.576\n",
      "    load_time_ms: 31.169\n",
      "    sample_throughput: 194.807\n",
      "    sample_time_ms: 14373.174\n",
      "    update_time_ms: 3.17\n",
      "  timestamp: 1658495590\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 100800\n",
      "  training_iteration: 36\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">          671.66</td><td style=\"text-align: right;\">100800</td><td style=\"text-align: right;\">  -0.329</td><td style=\"text-align: right;\">                 0.5</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            246.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 207200\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-13-29\n",
      "  done: false\n",
      "  episode_len_mean: 248.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.5000000074505806\n",
      "  episode_reward_mean: -0.3259999994933605\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 426\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.5419940870432627\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01760605788320291\n",
      "          policy_loss: -0.10576676992890757\n",
      "          total_loss: -0.09653868121014446\n",
      "          vf_explained_var: -0.16778893768787384\n",
      "          vf_loss: 0.004231375541470091\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.656922127519335\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.013821977993383255\n",
      "          policy_loss: -0.11600088213440142\n",
      "          total_loss: -0.10145408695123999\n",
      "          vf_explained_var: 0.20052984356880188\n",
      "          vf_loss: 0.005223529054741708\n",
      "    num_agent_steps_sampled: 207200\n",
      "    num_agent_steps_trained: 207200\n",
      "    num_steps_sampled: 103600\n",
      "    num_steps_trained: 103600\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.854166666666667\n",
      "    gpu_util_percent0: 0.04833333333333333\n",
      "    ram_util_percent: 54.15416666666667\n",
      "    vram_util_percent0: 0.21733398437499998\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.5000000074505806\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.18\n",
      "    agent_1: -0.1459999994933605\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05211267855678301\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.9150881147656253\n",
      "    mean_inference_ms: 1.7941840533715054\n",
      "    mean_raw_obs_processing_ms: 0.1588226003732918\n",
      "  time_since_restore: 690.2460837364197\n",
      "  time_this_iter_s: 18.586291074752808\n",
      "  time_total_s: 690.2460837364197\n",
      "  timers:\n",
      "    learn_throughput: 658.514\n",
      "    learn_time_ms: 4251.995\n",
      "    load_throughput: 90010.042\n",
      "    load_time_ms: 31.108\n",
      "    sample_throughput: 195.005\n",
      "    sample_time_ms: 14358.631\n",
      "    update_time_ms: 3.2\n",
      "  timestamp: 1658495609\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 103600\n",
      "  training_iteration: 37\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">         690.246</td><td style=\"text-align: right;\">103600</td><td style=\"text-align: right;\">  -0.326</td><td style=\"text-align: right;\">                 0.5</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            248.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 212800\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-13-48\n",
      "  done: false\n",
      "  episode_len_mean: 247.2\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.5000000074505806\n",
      "  episode_reward_mean: -0.3079999995231628\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 436\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.5430538405974707\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01802498277875636\n",
      "          policy_loss: -0.08349325844236384\n",
      "          total_loss: -0.0735669409559705\n",
      "          vf_explained_var: -0.1615581214427948\n",
      "          vf_loss: 0.005627900416411215\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.6639491582200643\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.0132913974336808\n",
      "          policy_loss: -0.07530806108843535\n",
      "          total_loss: -0.06108749193438035\n",
      "          vf_explained_var: -0.03317782282829285\n",
      "          vf_loss: 0.005815237312899193\n",
      "    num_agent_steps_sampled: 212800\n",
      "    num_agent_steps_trained: 212800\n",
      "    num_steps_sampled: 106400\n",
      "    num_steps_trained: 106400\n",
      "  iterations_since_restore: 38\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.956\n",
      "    gpu_util_percent0: 0.0572\n",
      "    ram_util_percent: 54.431999999999995\n",
      "    vram_util_percent0: 0.21670312500000002\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.5000000074505806\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.17\n",
      "    agent_1: -0.13799999952316283\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05212043427410052\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.915164905009573\n",
      "    mean_inference_ms: 1.7942717967564372\n",
      "    mean_raw_obs_processing_ms: 0.15878959846623109\n",
      "  time_since_restore: 708.8198626041412\n",
      "  time_this_iter_s: 18.573778867721558\n",
      "  time_total_s: 708.8198626041412\n",
      "  timers:\n",
      "    learn_throughput: 658.695\n",
      "    learn_time_ms: 4250.828\n",
      "    load_throughput: 90019.149\n",
      "    load_time_ms: 31.104\n",
      "    sample_throughput: 195.297\n",
      "    sample_time_ms: 14337.129\n",
      "    update_time_ms: 3.24\n",
      "  timestamp: 1658495628\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 106400\n",
      "  training_iteration: 38\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    38</td><td style=\"text-align: right;\">          708.82</td><td style=\"text-align: right;\">106400</td><td style=\"text-align: right;\">  -0.308</td><td style=\"text-align: right;\">                 0.5</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             247.2</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 218400\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-14-06\n",
      "  done: false\n",
      "  episode_len_mean: 247.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.5000000074505806\n",
      "  episode_reward_mean: -0.22799999952316286\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 448\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.552229491727693\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.017525495816559527\n",
      "          policy_loss: -0.07725257123456686\n",
      "          total_loss: -0.06779534253707555\n",
      "          vf_explained_var: -0.3147861361503601\n",
      "          vf_loss: 0.005012634263732166\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.6638199403172447\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.012143756631825039\n",
      "          policy_loss: -0.08927885760974841\n",
      "          total_loss: -0.07623803383969407\n",
      "          vf_explained_var: -0.05679909512400627\n",
      "          vf_loss: 0.005723443808424885\n",
      "    num_agent_steps_sampled: 218400\n",
      "    num_agent_steps_trained: 218400\n",
      "    num_steps_sampled: 109200\n",
      "    num_steps_trained: 109200\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.920833333333333\n",
      "    gpu_util_percent0: 0.049999999999999996\n",
      "    ram_util_percent: 54.2875\n",
      "    vram_util_percent0: 0.2174723307291667\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.5000000074505806\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.13\n",
      "    agent_1: -0.09799999952316284\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.052127381911829786\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.915095105716422\n",
      "    mean_inference_ms: 1.7943156421884143\n",
      "    mean_raw_obs_processing_ms: 0.1587488294587276\n",
      "  time_since_restore: 727.3050084114075\n",
      "  time_this_iter_s: 18.485145807266235\n",
      "  time_total_s: 727.3050084114075\n",
      "  timers:\n",
      "    learn_throughput: 658.009\n",
      "    learn_time_ms: 4255.258\n",
      "    load_throughput: 89997.212\n",
      "    load_time_ms: 31.112\n",
      "    sample_throughput: 195.346\n",
      "    sample_time_ms: 14333.532\n",
      "    update_time_ms: 3.228\n",
      "  timestamp: 1658495646\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 109200\n",
      "  training_iteration: 39\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">         727.305</td><td style=\"text-align: right;\">109200</td><td style=\"text-align: right;\">  -0.228</td><td style=\"text-align: right;\">                 0.5</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            247.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 224000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-14-24\n",
      "  done: false\n",
      "  episode_len_mean: 249.59\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.5000000074505806\n",
      "  episode_reward_mean: -0.24499999947845935\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 459\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.5202439916985377\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.014422379820131765\n",
      "          policy_loss: -0.06374443485895297\n",
      "          total_loss: -0.0549803993032713\n",
      "          vf_explained_var: -0.018866093829274178\n",
      "          vf_loss: 0.007424995918275575\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.640703967639378\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010068099673611618\n",
      "          policy_loss: -0.05513649633974724\n",
      "          total_loss: -0.04347597975616476\n",
      "          vf_explained_var: -0.2778116464614868\n",
      "          vf_loss: 0.007680983063610689\n",
      "    num_agent_steps_sampled: 224000\n",
      "    num_agent_steps_trained: 224000\n",
      "    num_steps_sampled: 112000\n",
      "    num_steps_trained: 112000\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.833333333333333\n",
      "    gpu_util_percent0: 0.052083333333333336\n",
      "    ram_util_percent: 54.20000000000001\n",
      "    vram_util_percent0: 0.21757812499999998\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.5000000074505806\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.14\n",
      "    agent_1: -0.10499999947845935\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05212936444892046\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.914666248350904\n",
      "    mean_inference_ms: 1.7941784103931844\n",
      "    mean_raw_obs_processing_ms: 0.1587195162897323\n",
      "  time_since_restore: 745.5809142589569\n",
      "  time_this_iter_s: 18.27590584754944\n",
      "  time_total_s: 745.5809142589569\n",
      "  timers:\n",
      "    learn_throughput: 656.64\n",
      "    learn_time_ms: 4264.135\n",
      "    load_throughput: 89902.276\n",
      "    load_time_ms: 31.145\n",
      "    sample_throughput: 195.938\n",
      "    sample_time_ms: 14290.211\n",
      "    update_time_ms: 3.221\n",
      "  timestamp: 1658495664\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 112000\n",
      "  training_iteration: 40\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         745.581</td><td style=\"text-align: right;\">112000</td><td style=\"text-align: right;\">  -0.245</td><td style=\"text-align: right;\">                 0.5</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            249.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 229600\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-14-43\n",
      "  done: false\n",
      "  episode_len_mean: 262.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.6000000014901161\n",
      "  episode_reward_mean: -0.25999999947845936\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 467\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.5172340174516044\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.0175433557489981\n",
      "          policy_loss: -0.0740895742835294\n",
      "          total_loss: -0.06452307791083253\n",
      "          vf_explained_var: -0.04463737830519676\n",
      "          vf_loss: 0.005257516311584844\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.6482223130407787\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.014063805670887582\n",
      "          policy_loss: -0.07781023629026354\n",
      "          total_loss: -0.06322577752039901\n",
      "          vf_explained_var: -0.17078456282615662\n",
      "          vf_loss: 0.004630377010817895\n",
      "    num_agent_steps_sampled: 229600\n",
      "    num_agent_steps_trained: 229600\n",
      "    num_steps_sampled: 114800\n",
      "    num_steps_trained: 114800\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.8708333333333345\n",
      "    gpu_util_percent0: 0.050833333333333335\n",
      "    ram_util_percent: 54.175000000000004\n",
      "    vram_util_percent0: 0.21675618489583334\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.6000000014901161\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.15\n",
      "    agent_1: -0.10999999947845936\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.052127328673434586\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.914223625443618\n",
      "    mean_inference_ms: 1.7939753234763907\n",
      "    mean_raw_obs_processing_ms: 0.15861840422192416\n",
      "  time_since_restore: 763.8690938949585\n",
      "  time_this_iter_s: 18.288179636001587\n",
      "  time_total_s: 763.8690938949585\n",
      "  timers:\n",
      "    learn_throughput: 657.377\n",
      "    learn_time_ms: 4259.349\n",
      "    load_throughput: 89918.865\n",
      "    load_time_ms: 31.139\n",
      "    sample_throughput: 196.47\n",
      "    sample_time_ms: 14251.518\n",
      "    update_time_ms: 3.228\n",
      "  timestamp: 1658495683\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 114800\n",
      "  training_iteration: 41\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">         763.869</td><td style=\"text-align: right;\">114800</td><td style=\"text-align: right;\">   -0.26</td><td style=\"text-align: right;\">                 0.6</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            262.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 235200\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-15-01\n",
      "  done: false\n",
      "  episode_len_mean: 254.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.6000000014901161\n",
      "  episode_reward_mean: -0.2569999994337559\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 482\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.5766247574772154\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.016309220499407435\n",
      "          policy_loss: -0.06953729184498939\n",
      "          total_loss: -0.05962187754526634\n",
      "          vf_explained_var: -0.07978679984807968\n",
      "          vf_loss: 0.00808560434511275\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.6216694762309394\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011895994787613972\n",
      "          policy_loss: -0.07624075129431385\n",
      "          total_loss: -0.06277887099921438\n",
      "          vf_explained_var: -0.059245575219392776\n",
      "          vf_loss: 0.007582653750086181\n",
      "    num_agent_steps_sampled: 235200\n",
      "    num_agent_steps_trained: 235200\n",
      "    num_steps_sampled: 117600\n",
      "    num_steps_trained: 117600\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.8500000000000005\n",
      "    gpu_util_percent0: 0.050833333333333335\n",
      "    ram_util_percent: 54.20000000000001\n",
      "    vram_util_percent0: 0.21753743489583333\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.6000000014901161\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.15\n",
      "    agent_1: -0.10699999943375588\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05211859505783245\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.9128841008577306\n",
      "    mean_inference_ms: 1.7933804169196086\n",
      "    mean_raw_obs_processing_ms: 0.15851175715437116\n",
      "  time_since_restore: 782.042044878006\n",
      "  time_this_iter_s: 18.172950983047485\n",
      "  time_total_s: 782.042044878006\n",
      "  timers:\n",
      "    learn_throughput: 659.797\n",
      "    learn_time_ms: 4243.731\n",
      "    load_throughput: 96495.887\n",
      "    load_time_ms: 29.017\n",
      "    sample_throughput: 197.212\n",
      "    sample_time_ms: 14197.885\n",
      "    update_time_ms: 3.203\n",
      "  timestamp: 1658495701\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 117600\n",
      "  training_iteration: 42\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    42</td><td style=\"text-align: right;\">         782.042</td><td style=\"text-align: right;\">117600</td><td style=\"text-align: right;\">  -0.257</td><td style=\"text-align: right;\">                 0.6</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            254.03</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 240800\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-15-19\n",
      "  done: false\n",
      "  episode_len_mean: 255.08\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.6000000014901161\n",
      "  episode_reward_mean: -0.3539999993890524\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 493\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.560613671938578\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.018929982585693495\n",
      "          policy_loss: -0.04315542191865721\n",
      "          total_loss: -0.031991560494394174\n",
      "          vf_explained_var: 0.2709145247936249\n",
      "          vf_loss: 0.007889793907885351\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.6225763154881343\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01124912050946858\n",
      "          policy_loss: -0.07140359521976539\n",
      "          total_loss: -0.058975584078801886\n",
      "          vf_explained_var: -0.09407002478837967\n",
      "          vf_loss: 0.0064789774403921216\n",
      "    num_agent_steps_sampled: 240800\n",
      "    num_agent_steps_trained: 240800\n",
      "    num_steps_sampled: 120400\n",
      "    num_steps_trained: 120400\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.870833333333334\n",
      "    gpu_util_percent0: 0.05125\n",
      "    ram_util_percent: 54.20000000000001\n",
      "    vram_util_percent0: 0.2169189453125\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.6000000014901161\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.2\n",
      "    agent_1: -0.1539999993890524\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.052111025464531446\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.9118533811861935\n",
      "    mean_inference_ms: 1.7928863013562957\n",
      "    mean_raw_obs_processing_ms: 0.15839851181047832\n",
      "  time_since_restore: 800.4964962005615\n",
      "  time_this_iter_s: 18.454451322555542\n",
      "  time_total_s: 800.4964962005615\n",
      "  timers:\n",
      "    learn_throughput: 661.309\n",
      "    learn_time_ms: 4234.03\n",
      "    load_throughput: 97069.506\n",
      "    load_time_ms: 28.845\n",
      "    sample_throughput: 197.726\n",
      "    sample_time_ms: 14161.037\n",
      "    update_time_ms: 3.204\n",
      "  timestamp: 1658495719\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 120400\n",
      "  training_iteration: 43\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    43</td><td style=\"text-align: right;\">         800.496</td><td style=\"text-align: right;\">120400</td><td style=\"text-align: right;\">  -0.354</td><td style=\"text-align: right;\">                 0.6</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            255.08</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 246400\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-15-38\n",
      "  done: false\n",
      "  episode_len_mean: 264.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.6000000014901161\n",
      "  episode_reward_mean: -0.3709999992698431\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 502\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.562326506489799\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01741083831581433\n",
      "          policy_loss: -0.07975081937980749\n",
      "          total_loss: -0.07033950384495602\n",
      "          vf_explained_var: 0.03813742473721504\n",
      "          vf_loss: 0.005057213604102019\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.6305474511214664\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.012057577498644487\n",
      "          policy_loss: -0.06326927248959519\n",
      "          total_loss: -0.05047424436443613\n",
      "          vf_explained_var: 0.015254832804203033\n",
      "          vf_loss: 0.005228283465623376\n",
      "    num_agent_steps_sampled: 246400\n",
      "    num_agent_steps_trained: 246400\n",
      "    num_steps_sampled: 123200\n",
      "    num_steps_trained: 123200\n",
      "  iterations_since_restore: 44\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.883333333333333\n",
      "    gpu_util_percent0: 0.06125\n",
      "    ram_util_percent: 54.1625\n",
      "    vram_util_percent0: 0.2169189453125\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.6000000014901161\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.21\n",
      "    agent_1: -0.1609999992698431\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.052102363217165305\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.9109622440796517\n",
      "    mean_inference_ms: 1.792403473499872\n",
      "    mean_raw_obs_processing_ms: 0.15827637017520946\n",
      "  time_since_restore: 818.6771812438965\n",
      "  time_this_iter_s: 18.18068504333496\n",
      "  time_total_s: 818.6771812438965\n",
      "  timers:\n",
      "    learn_throughput: 663.149\n",
      "    learn_time_ms: 4222.279\n",
      "    load_throughput: 97263.251\n",
      "    load_time_ms: 28.788\n",
      "    sample_throughput: 198.346\n",
      "    sample_time_ms: 14116.761\n",
      "    update_time_ms: 3.23\n",
      "  timestamp: 1658495738\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 123200\n",
      "  training_iteration: 44\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    44</td><td style=\"text-align: right;\">         818.677</td><td style=\"text-align: right;\">123200</td><td style=\"text-align: right;\">  -0.371</td><td style=\"text-align: right;\">                 0.6</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            264.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 252000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-15-56\n",
      "  done: false\n",
      "  episode_len_mean: 257.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.6000000014901161\n",
      "  episode_reward_mean: -0.39199999928474427\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 513\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.541603785895166\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.0198965465197885\n",
      "          policy_loss: -0.09048553773027379\n",
      "          total_loss: -0.07985551491756701\n",
      "          vf_explained_var: 0.11580798774957657\n",
      "          vf_loss: 0.004963677507995661\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.635182357969738\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.012962782754341155\n",
      "          policy_loss: -0.07714106811909005\n",
      "          total_loss: -0.0635398203518153\n",
      "          vf_explained_var: 0.11241675913333893\n",
      "          vf_loss: 0.004951217932102736\n",
      "    num_agent_steps_sampled: 252000\n",
      "    num_agent_steps_trained: 252000\n",
      "    num_steps_sampled: 126000\n",
      "    num_steps_trained: 126000\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.867999999999999\n",
      "    gpu_util_percent0: 0.0632\n",
      "    ram_util_percent: 54.144\n",
      "    vram_util_percent0: 0.21748437499999998\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.6000000014901161\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.22\n",
      "    agent_1: -0.17199999928474427\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.052093001495418054\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.909947380140354\n",
      "    mean_inference_ms: 1.7918937583951506\n",
      "    mean_raw_obs_processing_ms: 0.1581338587249893\n",
      "  time_since_restore: 837.2711915969849\n",
      "  time_this_iter_s: 18.59401035308838\n",
      "  time_total_s: 837.2711915969849\n",
      "  timers:\n",
      "    learn_throughput: 665.025\n",
      "    learn_time_ms: 4210.367\n",
      "    load_throughput: 97266.634\n",
      "    load_time_ms: 28.787\n",
      "    sample_throughput: 198.127\n",
      "    sample_time_ms: 14132.374\n",
      "    update_time_ms: 3.238\n",
      "  timestamp: 1658495756\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 126000\n",
      "  training_iteration: 45\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         837.271</td><td style=\"text-align: right;\">126000</td><td style=\"text-align: right;\">  -0.392</td><td style=\"text-align: right;\">                 0.6</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            257.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 257600\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-16-15\n",
      "  done: false\n",
      "  episode_len_mean: 258.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.6000000014901161\n",
      "  episode_reward_mean: -0.4559999993443489\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 524\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.5526060362656913\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.016006814088793557\n",
      "          policy_loss: -0.12056829695661907\n",
      "          total_loss: -0.11121510773470315\n",
      "          vf_explained_var: 0.004399345256388187\n",
      "          vf_loss: 0.0068837549363143765\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.651449585954348\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01082727822492021\n",
      "          policy_loss: -0.10196543287297356\n",
      "          total_loss: -0.08951523364660152\n",
      "          vf_explained_var: 0.12038587033748627\n",
      "          vf_loss: 0.007780819588896563\n",
      "    num_agent_steps_sampled: 257600\n",
      "    num_agent_steps_trained: 257600\n",
      "    num_steps_sampled: 128800\n",
      "    num_steps_trained: 128800\n",
      "  iterations_since_restore: 46\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.888\n",
      "    gpu_util_percent0: 0.055600000000000004\n",
      "    ram_util_percent: 54.20000000000001\n",
      "    vram_util_percent0: 0.21686718750000003\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.6000000014901161\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.25\n",
      "    agent_1: -0.2059999993443489\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.052085075576319466\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.9091441272264826\n",
      "    mean_inference_ms: 1.7914554388013824\n",
      "    mean_raw_obs_processing_ms: 0.15802349448359818\n",
      "  time_since_restore: 856.1045651435852\n",
      "  time_this_iter_s: 18.833373546600342\n",
      "  time_total_s: 856.1045651435852\n",
      "  timers:\n",
      "    learn_throughput: 666.681\n",
      "    learn_time_ms: 4199.908\n",
      "    load_throughput: 97053.542\n",
      "    load_time_ms: 28.85\n",
      "    sample_throughput: 197.663\n",
      "    sample_time_ms: 14165.544\n",
      "    update_time_ms: 3.237\n",
      "  timestamp: 1658495775\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 128800\n",
      "  training_iteration: 46\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    46</td><td style=\"text-align: right;\">         856.105</td><td style=\"text-align: right;\">128800</td><td style=\"text-align: right;\">  -0.456</td><td style=\"text-align: right;\">                 0.6</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            258.37</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 263200\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-16-33\n",
      "  done: false\n",
      "  episode_len_mean: 250.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.6000000014901161\n",
      "  episode_reward_mean: -0.4509999992698431\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 537\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.5535265845911845\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.019876670367274557\n",
      "          policy_loss: -0.09883898741307869\n",
      "          total_loss: -0.08830260675503745\n",
      "          vf_explained_var: 0.025027384981513023\n",
      "          vf_loss: 0.004738895129516355\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.655436505873998\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.013851850014277044\n",
      "          policy_loss: -0.07934480646924515\n",
      "          total_loss: -0.06498388935315666\n",
      "          vf_explained_var: -0.03804932162165642\n",
      "          vf_loss: 0.0046058732103639545\n",
      "    num_agent_steps_sampled: 263200\n",
      "    num_agent_steps_trained: 263200\n",
      "    num_steps_sampled: 131600\n",
      "    num_steps_trained: 131600\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.775000000000001\n",
      "    gpu_util_percent0: 0.06166666666666667\n",
      "    ram_util_percent: 54.20000000000001\n",
      "    vram_util_percent0: 0.21734619140625\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.6000000014901161\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.25\n",
      "    agent_1: -0.2009999992698431\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05207195680010007\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.908119041984645\n",
      "    mean_inference_ms: 1.7907287043873996\n",
      "    mean_raw_obs_processing_ms: 0.15796369412157904\n",
      "  time_since_restore: 874.3596725463867\n",
      "  time_this_iter_s: 18.255107402801514\n",
      "  time_total_s: 874.3596725463867\n",
      "  timers:\n",
      "    learn_throughput: 667.779\n",
      "    learn_time_ms: 4193.004\n",
      "    load_throughput: 97414.522\n",
      "    load_time_ms: 28.743\n",
      "    sample_throughput: 198.032\n",
      "    sample_time_ms: 14139.157\n",
      "    update_time_ms: 3.248\n",
      "  timestamp: 1658495793\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 131600\n",
      "  training_iteration: 47\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    47</td><td style=\"text-align: right;\">          874.36</td><td style=\"text-align: right;\">131600</td><td style=\"text-align: right;\">  -0.451</td><td style=\"text-align: right;\">                 0.6</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            250.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 268800\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-16-52\n",
      "  done: false\n",
      "  episode_len_mean: 247.23\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.6000000014901161\n",
      "  episode_reward_mean: -0.4709999992698431\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 550\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.578293881246022\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01659970144929443\n",
      "          policy_loss: -0.06197898907772642\n",
      "          total_loss: -0.05235112090535733\n",
      "          vf_explained_var: -0.021040689200162888\n",
      "          vf_loss: 0.006852122239555077\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.64960186680158\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.013199865772978433\n",
      "          policy_loss: -0.08533717595786411\n",
      "          total_loss: -0.07121002757395174\n",
      "          vf_explained_var: 0.018863370642066002\n",
      "          vf_loss: 0.005792811000511782\n",
      "    num_agent_steps_sampled: 268800\n",
      "    num_agent_steps_trained: 268800\n",
      "    num_steps_sampled: 134400\n",
      "    num_steps_trained: 134400\n",
      "  iterations_since_restore: 48\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.883333333333333\n",
      "    gpu_util_percent0: 0.05833333333333333\n",
      "    ram_util_percent: 54.20000000000001\n",
      "    vram_util_percent0: 0.21656901041666665\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.6000000014901161\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.26\n",
      "    agent_1: -0.2109999992698431\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05205627630097481\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.9071840793572994\n",
      "    mean_inference_ms: 1.7898940236505008\n",
      "    mean_raw_obs_processing_ms: 0.15792090623348623\n",
      "  time_since_restore: 892.7399320602417\n",
      "  time_this_iter_s: 18.38025951385498\n",
      "  time_total_s: 892.7399320602417\n",
      "  timers:\n",
      "    learn_throughput: 668.063\n",
      "    learn_time_ms: 4191.221\n",
      "    load_throughput: 97340.32\n",
      "    load_time_ms: 28.765\n",
      "    sample_throughput: 198.28\n",
      "    sample_time_ms: 14121.459\n",
      "    update_time_ms: 3.235\n",
      "  timestamp: 1658495812\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 134400\n",
      "  training_iteration: 48\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    48</td><td style=\"text-align: right;\">          892.74</td><td style=\"text-align: right;\">134400</td><td style=\"text-align: right;\">  -0.471</td><td style=\"text-align: right;\">                 0.6</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            247.23</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 274400\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-17-10\n",
      "  done: false\n",
      "  episode_len_mean: 228.62\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.4000000134110451\n",
      "  episode_reward_mean: -0.43699999928474426\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 566\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.495536576424326\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.016704642253541593\n",
      "          policy_loss: -0.06521626107427264\n",
      "          total_loss: -0.055263521534514096\n",
      "          vf_explained_var: -0.05802230164408684\n",
      "          vf_loss: 0.007531483897071753\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.6002391690299627\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.012578046202624784\n",
      "          policy_loss: -0.07450163183828062\n",
      "          total_loss: -0.06098713523360424\n",
      "          vf_explained_var: 0.18568554520606995\n",
      "          vf_loss: 0.0057603791603815764\n",
      "    num_agent_steps_sampled: 274400\n",
      "    num_agent_steps_trained: 274400\n",
      "    num_steps_sampled: 137200\n",
      "    num_steps_trained: 137200\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.883333333333333\n",
      "    gpu_util_percent0: 0.051666666666666666\n",
      "    ram_util_percent: 54.208333333333336\n",
      "    vram_util_percent0: 0.2173136393229167\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.4000000134110451\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.24\n",
      "    agent_1: -0.19699999928474426\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.052037081707794\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.906241144577357\n",
      "    mean_inference_ms: 1.7889536015680982\n",
      "    mean_raw_obs_processing_ms: 0.15800659889412424\n",
      "  time_since_restore: 911.1213998794556\n",
      "  time_this_iter_s: 18.381467819213867\n",
      "  time_total_s: 911.1213998794556\n",
      "  timers:\n",
      "    learn_throughput: 668.414\n",
      "    learn_time_ms: 4189.023\n",
      "    load_throughput: 97411.209\n",
      "    load_time_ms: 28.744\n",
      "    sample_throughput: 198.398\n",
      "    sample_time_ms: 14113.046\n",
      "    update_time_ms: 3.246\n",
      "  timestamp: 1658495830\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 137200\n",
      "  training_iteration: 49\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    49</td><td style=\"text-align: right;\">         911.121</td><td style=\"text-align: right;\">137200</td><td style=\"text-align: right;\">  -0.437</td><td style=\"text-align: right;\">                 0.4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            228.62</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 280000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-17-28\n",
      "  done: false\n",
      "  episode_len_mean: 227.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.4000000134110451\n",
      "  episode_reward_mean: -0.40099999934434893\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 580\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.5116587636016665\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.018944668415526483\n",
      "          policy_loss: -0.09157087119057819\n",
      "          total_loss: -0.08148162234777244\n",
      "          vf_explained_var: -0.1142626479268074\n",
      "          vf_loss: 0.004743049067959267\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.617443018016361\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010789696538552454\n",
      "          policy_loss: -0.08151586347507657\n",
      "          total_loss: -0.06995230575975733\n",
      "          vf_explained_var: 0.19275839626789093\n",
      "          vf_loss: 0.00531666855489935\n",
      "    num_agent_steps_sampled: 280000\n",
      "    num_agent_steps_trained: 280000\n",
      "    num_steps_sampled: 140000\n",
      "    num_steps_trained: 140000\n",
      "  iterations_since_restore: 50\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.870833333333333\n",
      "    gpu_util_percent0: 0.052083333333333336\n",
      "    ram_util_percent: 54.291666666666664\n",
      "    vram_util_percent0: 0.21752115885416665\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.4000000134110451\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.22\n",
      "    agent_1: -0.1809999993443489\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05202224323001886\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.9055178039831286\n",
      "    mean_inference_ms: 1.7881808998504125\n",
      "    mean_raw_obs_processing_ms: 0.1581008836938809\n",
      "  time_since_restore: 929.362087726593\n",
      "  time_this_iter_s: 18.24068784713745\n",
      "  time_total_s: 929.362087726593\n",
      "  timers:\n",
      "    learn_throughput: 668.362\n",
      "    learn_time_ms: 4189.344\n",
      "    load_throughput: 97415.411\n",
      "    load_time_ms: 28.743\n",
      "    sample_throughput: 198.455\n",
      "    sample_time_ms: 14108.982\n",
      "    update_time_ms: 3.228\n",
      "  timestamp: 1658495848\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 140000\n",
      "  training_iteration: 50\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         929.362</td><td style=\"text-align: right;\">140000</td><td style=\"text-align: right;\">  -0.401</td><td style=\"text-align: right;\">                 0.4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            227.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 285600\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-17-47\n",
      "  done: false\n",
      "  episode_len_mean: 231.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.4000000134110451\n",
      "  episode_reward_mean: -0.31699999928474426\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 590\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.509705444176992\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.023407865945702173\n",
      "          policy_loss: -0.09760923273322641\n",
      "          total_loss: -0.08607538752091516\n",
      "          vf_explained_var: 0.07454434782266617\n",
      "          vf_loss: 0.002494102700265579\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.614916295522735\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.014196045568718134\n",
      "          policy_loss: -0.09741398340182579\n",
      "          total_loss: -0.08313603738566772\n",
      "          vf_explained_var: 0.06977628916501999\n",
      "          vf_loss: 0.0033383426866965323\n",
      "    num_agent_steps_sampled: 285600\n",
      "    num_agent_steps_trained: 285600\n",
      "    num_steps_sampled: 142800\n",
      "    num_steps_trained: 142800\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.841666666666666\n",
      "    gpu_util_percent0: 0.04958333333333333\n",
      "    ram_util_percent: 54.225\n",
      "    vram_util_percent0: 0.21675618489583334\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.4000000134110451\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.18\n",
      "    agent_1: -0.13699999928474427\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05200916455857692\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.904915401168804\n",
      "    mean_inference_ms: 1.78757191028675\n",
      "    mean_raw_obs_processing_ms: 0.1581463946839225\n",
      "  time_since_restore: 947.6062824726105\n",
      "  time_this_iter_s: 18.244194746017456\n",
      "  time_total_s: 947.6062824726105\n",
      "  timers:\n",
      "    learn_throughput: 665.591\n",
      "    learn_time_ms: 4206.786\n",
      "    load_throughput: 97476.861\n",
      "    load_time_ms: 28.725\n",
      "    sample_throughput: 198.766\n",
      "    sample_time_ms: 14086.922\n",
      "    update_time_ms: 3.225\n",
      "  timestamp: 1658495867\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 142800\n",
      "  training_iteration: 51\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    51</td><td style=\"text-align: right;\">         947.606</td><td style=\"text-align: right;\">142800</td><td style=\"text-align: right;\">  -0.317</td><td style=\"text-align: right;\">                 0.4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            231.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 291200\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-18-05\n",
      "  done: false\n",
      "  episode_len_mean: 219.77\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.30000000447034836\n",
      "  episode_reward_mean: -0.33999999940395353\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 604\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.5277910246735527\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011837177982883721\n",
      "          policy_loss: -0.05327598651915434\n",
      "          total_loss: -0.042612234474204126\n",
      "          vf_explained_var: -0.21591199934482574\n",
      "          vf_loss: 0.008099167413612511\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.63115124688262\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010610091109815478\n",
      "          policy_loss: -0.06213220183937145\n",
      "          total_loss: -0.05023816409465369\n",
      "          vf_explained_var: 0.05670015141367912\n",
      "          vf_loss: 0.006789068151452479\n",
      "    num_agent_steps_sampled: 291200\n",
      "    num_agent_steps_trained: 291200\n",
      "    num_steps_sampled: 145600\n",
      "    num_steps_trained: 145600\n",
      "  iterations_since_restore: 52\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.925\n",
      "    gpu_util_percent0: 0.0525\n",
      "    ram_util_percent: 54.20000000000001\n",
      "    vram_util_percent0: 0.2176798502604167\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.30000000447034836\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.19\n",
      "    agent_1: -0.14999999940395356\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05199045153621908\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.904017609393749\n",
      "    mean_inference_ms: 1.7867207541577066\n",
      "    mean_raw_obs_processing_ms: 0.15829542553229609\n",
      "  time_since_restore: 965.8171045780182\n",
      "  time_this_iter_s: 18.210822105407715\n",
      "  time_total_s: 965.8171045780182\n",
      "  timers:\n",
      "    learn_throughput: 664.164\n",
      "    learn_time_ms: 4215.828\n",
      "    load_throughput: 97722.464\n",
      "    load_time_ms: 28.653\n",
      "    sample_throughput: 198.843\n",
      "    sample_time_ms: 14081.454\n",
      "    update_time_ms: 3.26\n",
      "  timestamp: 1658495885\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 145600\n",
      "  training_iteration: 52\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    52</td><td style=\"text-align: right;\">         965.817</td><td style=\"text-align: right;\">145600</td><td style=\"text-align: right;\">   -0.34</td><td style=\"text-align: right;\">                 0.3</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            219.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 296800\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-18-23\n",
      "  done: false\n",
      "  episode_len_mean: 224.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.30000000447034836\n",
      "  episode_reward_mean: -0.362999999448657\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 613\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.5379851843629564\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.015556075209240976\n",
      "          policy_loss: -0.09100630459752643\n",
      "          total_loss: -0.0784347188448356\n",
      "          vf_explained_var: 0.05519115924835205\n",
      "          vf_loss: 0.005595355433407738\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.60904399341061\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01334056774523216\n",
      "          policy_loss: -0.08668799347720951\n",
      "          total_loss: -0.07255352460474353\n",
      "          vf_explained_var: 0.16072270274162292\n",
      "          vf_loss: 0.005363904975335269\n",
      "    num_agent_steps_sampled: 296800\n",
      "    num_agent_steps_trained: 296800\n",
      "    num_steps_sampled: 148400\n",
      "    num_steps_trained: 148400\n",
      "  iterations_since_restore: 53\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.895833333333333\n",
      "    gpu_util_percent0: 0.050833333333333335\n",
      "    ram_util_percent: 54.21666666666667\n",
      "    vram_util_percent0: 0.216943359375\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.30000000447034836\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.2\n",
      "    agent_1: -0.16299999944865703\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05197621263354085\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.9034422820121844\n",
      "    mean_inference_ms: 1.7860682835622135\n",
      "    mean_raw_obs_processing_ms: 0.1583706134760411\n",
      "  time_since_restore: 984.1000080108643\n",
      "  time_this_iter_s: 18.28290343284607\n",
      "  time_total_s: 984.1000080108643\n",
      "  timers:\n",
      "    learn_throughput: 663.895\n",
      "    learn_time_ms: 4217.533\n",
      "    load_throughput: 97524.619\n",
      "    load_time_ms: 28.711\n",
      "    sample_throughput: 199.112\n",
      "    sample_time_ms: 14062.418\n",
      "    update_time_ms: 3.269\n",
      "  timestamp: 1658495903\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 148400\n",
      "  training_iteration: 53\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    53</td><td style=\"text-align: right;\">           984.1</td><td style=\"text-align: right;\">148400</td><td style=\"text-align: right;\">  -0.363</td><td style=\"text-align: right;\">                 0.3</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            224.39</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 302400\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-18-42\n",
      "  done: false\n",
      "  episode_len_mean: 216.38\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.30000000447034836\n",
      "  episode_reward_mean: -0.32299999944865704\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 627\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.5428472530274164\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.013428545299898629\n",
      "          policy_loss: -0.07034363011119976\n",
      "          total_loss: -0.05885202194414368\n",
      "          vf_explained_var: 0.07395371794700623\n",
      "          vf_loss: 0.00707318489540263\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.629515331415903\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011420852474880196\n",
      "          policy_loss: -0.0753494451554226\n",
      "          total_loss: -0.06253843281107645\n",
      "          vf_explained_var: 0.016088349744677544\n",
      "          vf_loss: 0.0070903308038569295\n",
      "    num_agent_steps_sampled: 302400\n",
      "    num_agent_steps_trained: 302400\n",
      "    num_steps_sampled: 151200\n",
      "    num_steps_trained: 151200\n",
      "  iterations_since_restore: 54\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.066666666666667\n",
      "    gpu_util_percent0: 0.051666666666666666\n",
      "    ram_util_percent: 54.47083333333334\n",
      "    vram_util_percent0: 0.2173095703125\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.30000000447034836\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.18\n",
      "    agent_1: -0.14299999944865704\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05195088485384385\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.9023726896442015\n",
      "    mean_inference_ms: 1.7848694132932967\n",
      "    mean_raw_obs_processing_ms: 0.15852114864367922\n",
      "  time_since_restore: 1002.447683095932\n",
      "  time_this_iter_s: 18.34767508506775\n",
      "  time_total_s: 1002.447683095932\n",
      "  timers:\n",
      "    learn_throughput: 663.105\n",
      "    learn_time_ms: 4222.557\n",
      "    load_throughput: 97531.747\n",
      "    load_time_ms: 28.709\n",
      "    sample_throughput: 198.951\n",
      "    sample_time_ms: 14073.802\n",
      "    update_time_ms: 3.24\n",
      "  timestamp: 1658495922\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 151200\n",
      "  training_iteration: 54\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    54</td><td style=\"text-align: right;\">         1002.45</td><td style=\"text-align: right;\">151200</td><td style=\"text-align: right;\">  -0.323</td><td style=\"text-align: right;\">                 0.3</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            216.38</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 308000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-19-00\n",
      "  done: false\n",
      "  episode_len_mean: 223.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.30000000447034836\n",
      "  episode_reward_mean: -0.3619999994337559\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 636\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.5539936196236384\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.014055950207674407\n",
      "          policy_loss: -0.09758686506271867\n",
      "          total_loss: -0.08657887803879237\n",
      "          vf_explained_var: 0.08548411726951599\n",
      "          vf_loss: 0.004362562791703524\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.6532514414616992\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.012127941839366512\n",
      "          policy_loss: -0.08219337413348035\n",
      "          total_loss: -0.06972290822886862\n",
      "          vf_explained_var: 0.2385559380054474\n",
      "          vf_loss: 0.004127845802681155\n",
      "    num_agent_steps_sampled: 308000\n",
      "    num_agent_steps_trained: 308000\n",
      "    num_steps_sampled: 154000\n",
      "    num_steps_trained: 154000\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.929166666666667\n",
      "    gpu_util_percent0: 0.050833333333333335\n",
      "    ram_util_percent: 54.429166666666674\n",
      "    vram_util_percent0: 0.21753743489583333\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.30000000447034836\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.2\n",
      "    agent_1: -0.16199999943375587\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05193472598573716\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.901641929287398\n",
      "    mean_inference_ms: 1.7841402104550996\n",
      "    mean_raw_obs_processing_ms: 0.15857465387804848\n",
      "  time_since_restore: 1020.5710628032684\n",
      "  time_this_iter_s: 18.123379707336426\n",
      "  time_total_s: 1020.5710628032684\n",
      "  timers:\n",
      "    learn_throughput: 662.704\n",
      "    learn_time_ms: 4225.111\n",
      "    load_throughput: 97271.145\n",
      "    load_time_ms: 28.786\n",
      "    sample_throughput: 199.658\n",
      "    sample_time_ms: 14023.957\n",
      "    update_time_ms: 3.265\n",
      "  timestamp: 1658495940\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 154000\n",
      "  training_iteration: 55\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    55</td><td style=\"text-align: right;\">         1020.57</td><td style=\"text-align: right;\">154000</td><td style=\"text-align: right;\">  -0.362</td><td style=\"text-align: right;\">                 0.3</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            223.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 313600\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-19-18\n",
      "  done: false\n",
      "  episode_len_mean: 228.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.30000000447034836\n",
      "  episode_reward_mean: -0.3979999992996454\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 649\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.527798640586081\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.015667195585000144\n",
      "          policy_loss: -0.05035991903087918\n",
      "          total_loss: -0.037750616077090626\n",
      "          vf_explained_var: 0.12830474972724915\n",
      "          vf_loss: 0.005453013128273943\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.634853409159751\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01223438347596599\n",
      "          policy_loss: -0.06814435192583394\n",
      "          total_loss: -0.05510360008754235\n",
      "          vf_explained_var: 0.14593498408794403\n",
      "          vf_loss: 0.005430135475935891\n",
      "    num_agent_steps_sampled: 313600\n",
      "    num_agent_steps_trained: 313600\n",
      "    num_steps_sampled: 156800\n",
      "    num_steps_trained: 156800\n",
      "  iterations_since_restore: 56\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.879166666666666\n",
      "    gpu_util_percent0: 0.050833333333333335\n",
      "    ram_util_percent: 54.29583333333333\n",
      "    vram_util_percent0: 0.2169148763020833\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.30000000447034836\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.22\n",
      "    agent_1: -0.1779999992996454\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05191185922682022\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.900547736412713\n",
      "    mean_inference_ms: 1.7831121065049615\n",
      "    mean_raw_obs_processing_ms: 0.15864959773277268\n",
      "  time_since_restore: 1038.7970871925354\n",
      "  time_this_iter_s: 18.226024389266968\n",
      "  time_total_s: 1038.7970871925354\n",
      "  timers:\n",
      "    learn_throughput: 663.658\n",
      "    learn_time_ms: 4219.043\n",
      "    load_throughput: 97359.284\n",
      "    load_time_ms: 28.759\n",
      "    sample_throughput: 200.403\n",
      "    sample_time_ms: 13971.868\n",
      "    update_time_ms: 3.263\n",
      "  timestamp: 1658495958\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 156800\n",
      "  training_iteration: 56\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    56</td><td style=\"text-align: right;\">          1038.8</td><td style=\"text-align: right;\">156800</td><td style=\"text-align: right;\">  -0.398</td><td style=\"text-align: right;\">                 0.3</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            228.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 319200\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-19-36\n",
      "  done: false\n",
      "  episode_len_mean: 230.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.20000000298023224\n",
      "  episode_reward_mean: -0.3559999992698431\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 662\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.5046677532650174\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.016846531777569854\n",
      "          policy_loss: -0.13336157057936016\n",
      "          total_loss: -0.12071075406068606\n",
      "          vf_explained_var: 0.0536140538752079\n",
      "          vf_loss: 0.003019093617608416\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.59605058885756\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.013611509013101698\n",
      "          policy_loss: -0.12523896440117305\n",
      "          total_loss: -0.11166249215429638\n",
      "          vf_explained_var: 0.0679229348897934\n",
      "          vf_loss: 0.002982128263586977\n",
      "    num_agent_steps_sampled: 319200\n",
      "    num_agent_steps_trained: 319200\n",
      "    num_steps_sampled: 159600\n",
      "    num_steps_trained: 159600\n",
      "  iterations_since_restore: 57\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.8625\n",
      "    gpu_util_percent0: 0.050833333333333335\n",
      "    ram_util_percent: 54.21666666666667\n",
      "    vram_util_percent0: 0.21753743489583333\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.20000000298023224\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.2\n",
      "    agent_1: -0.1559999992698431\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051889886542443885\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.899510335185722\n",
      "    mean_inference_ms: 1.7820636716073903\n",
      "    mean_raw_obs_processing_ms: 0.15867440044687772\n",
      "  time_since_restore: 1057.1323821544647\n",
      "  time_this_iter_s: 18.33529496192932\n",
      "  time_total_s: 1057.1323821544647\n",
      "  timers:\n",
      "    learn_throughput: 663.741\n",
      "    learn_time_ms: 4218.512\n",
      "    load_throughput: 97065.574\n",
      "    load_time_ms: 28.846\n",
      "    sample_throughput: 200.282\n",
      "    sample_time_ms: 13980.283\n",
      "    update_time_ms: 3.216\n",
      "  timestamp: 1658495976\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 159600\n",
      "  training_iteration: 57\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         1057.13</td><td style=\"text-align: right;\">159600</td><td style=\"text-align: right;\">  -0.356</td><td style=\"text-align: right;\">                 0.2</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            230.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 324800\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-19-55\n",
      "  done: false\n",
      "  episode_len_mean: 238.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.30000000447034836\n",
      "  episode_reward_mean: -0.32899999916553496\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 672\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.4549226941806928\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01724171587507622\n",
      "          policy_loss: -0.06568216440427932\n",
      "          total_loss: -0.052209000594476015\n",
      "          vf_explained_var: -0.08389094471931458\n",
      "          vf_loss: 0.004461471418748954\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.590999965866407\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.012475557723859654\n",
      "          policy_loss: -0.08000972836487649\n",
      "          total_loss: -0.06723809252746703\n",
      "          vf_explained_var: -0.07257338613271713\n",
      "          vf_loss: 0.003921366553835729\n",
      "    num_agent_steps_sampled: 324800\n",
      "    num_agent_steps_trained: 324800\n",
      "    num_steps_sampled: 162400\n",
      "    num_steps_trained: 162400\n",
      "  iterations_since_restore: 58\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.866666666666667\n",
      "    gpu_util_percent0: 0.041666666666666664\n",
      "    ram_util_percent: 54.208333333333336\n",
      "    vram_util_percent0: 0.21675618489583334\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.30000000447034836\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.19\n",
      "    agent_1: -0.13899999916553496\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05187269440961618\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8987428564072695\n",
      "    mean_inference_ms: 1.7812616923536349\n",
      "    mean_raw_obs_processing_ms: 0.15865252975869365\n",
      "  time_since_restore: 1075.444473028183\n",
      "  time_this_iter_s: 18.31209087371826\n",
      "  time_total_s: 1075.444473028183\n",
      "  timers:\n",
      "    learn_throughput: 661.706\n",
      "    learn_time_ms: 4231.486\n",
      "    load_throughput: 96975.805\n",
      "    load_time_ms: 28.873\n",
      "    sample_throughput: 200.568\n",
      "    sample_time_ms: 13960.343\n",
      "    update_time_ms: 3.203\n",
      "  timestamp: 1658495995\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 162400\n",
      "  training_iteration: 58\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    58</td><td style=\"text-align: right;\">         1075.44</td><td style=\"text-align: right;\">162400</td><td style=\"text-align: right;\">  -0.329</td><td style=\"text-align: right;\">                 0.3</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            238.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 330400\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-20-13\n",
      "  done: false\n",
      "  episode_len_mean: 241.94\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.30000000447034836\n",
      "  episode_reward_mean: -0.3849999991059303\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 683\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.501080869209199\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.014163828448057362\n",
      "          policy_loss: -0.07648701627303603\n",
      "          total_loss: -0.06446392624634643\n",
      "          vf_explained_var: 0.013321950100362301\n",
      "          vf_loss: 0.00696656001534956\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.5863827644359496\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011092762328780674\n",
      "          policy_loss: -0.07408908463548869\n",
      "          total_loss: -0.061515007295557075\n",
      "          vf_explained_var: 0.04557731747627258\n",
      "          vf_loss: 0.007299329662124538\n",
      "    num_agent_steps_sampled: 330400\n",
      "    num_agent_steps_trained: 330400\n",
      "    num_steps_sampled: 165200\n",
      "    num_steps_trained: 165200\n",
      "  iterations_since_restore: 59\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.892\n",
      "    gpu_util_percent0: 0.062400000000000004\n",
      "    ram_util_percent: 54.268\n",
      "    vram_util_percent0: 0.21680078125000002\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.30000000447034836\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.22\n",
      "    agent_1: -0.16499999910593033\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051853455688773586\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.897900468311265\n",
      "    mean_inference_ms: 1.7803627880637034\n",
      "    mean_raw_obs_processing_ms: 0.15861406573007808\n",
      "  time_since_restore: 1093.7971329689026\n",
      "  time_this_iter_s: 18.352659940719604\n",
      "  time_total_s: 1093.7971329689026\n",
      "  timers:\n",
      "    learn_throughput: 659.592\n",
      "    learn_time_ms: 4245.05\n",
      "    load_throughput: 96637.383\n",
      "    load_time_ms: 28.974\n",
      "    sample_throughput: 200.847\n",
      "    sample_time_ms: 13940.973\n",
      "    update_time_ms: 3.217\n",
      "  timestamp: 1658496013\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 165200\n",
      "  training_iteration: 59\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    59</td><td style=\"text-align: right;\">          1093.8</td><td style=\"text-align: right;\">165200</td><td style=\"text-align: right;\">  -0.385</td><td style=\"text-align: right;\">                 0.3</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            241.94</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 336000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-20-32\n",
      "  done: false\n",
      "  episode_len_mean: 250.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.30000000447034836\n",
      "  episode_reward_mean: -0.3849999991059303\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 691\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.472183715019907\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.017316980109950454\n",
      "          policy_loss: -0.06126305621139528\n",
      "          total_loss: -0.04856268184569975\n",
      "          vf_explained_var: -0.11866450309753418\n",
      "          vf_loss: 0.002114770241557077\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.580037241890317\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.0144057885911155\n",
      "          policy_loss: -0.08852126796097894\n",
      "          total_loss: -0.07430063235750865\n",
      "          vf_explained_var: 0.07114227861166\n",
      "          vf_loss: 0.0025345879630982553\n",
      "    num_agent_steps_sampled: 336000\n",
      "    num_agent_steps_trained: 336000\n",
      "    num_steps_sampled: 168000\n",
      "    num_steps_trained: 168000\n",
      "  iterations_since_restore: 60\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.129166666666666\n",
      "    gpu_util_percent0: 0.05541666666666667\n",
      "    ram_util_percent: 54.27916666666667\n",
      "    vram_util_percent0: 0.2173502604166667\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.30000000447034836\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.22\n",
      "    agent_1: -0.16499999910593033\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05184154452758477\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.897415148556512\n",
      "    mean_inference_ms: 1.779784131862344\n",
      "    mean_raw_obs_processing_ms: 0.1585684103265594\n",
      "  time_since_restore: 1112.4225220680237\n",
      "  time_this_iter_s: 18.625389099121094\n",
      "  time_total_s: 1112.4225220680237\n",
      "  timers:\n",
      "    learn_throughput: 657.604\n",
      "    learn_time_ms: 4257.881\n",
      "    load_throughput: 96678.034\n",
      "    load_time_ms: 28.962\n",
      "    sample_throughput: 200.476\n",
      "    sample_time_ms: 13966.736\n",
      "    update_time_ms: 3.218\n",
      "  timestamp: 1658496032\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 168000\n",
      "  training_iteration: 60\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    60</td><td style=\"text-align: right;\">         1112.42</td><td style=\"text-align: right;\">168000</td><td style=\"text-align: right;\">  -0.385</td><td style=\"text-align: right;\">                 0.3</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            250.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 341600\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-20-51\n",
      "  done: false\n",
      "  episode_len_mean: 254.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.4000000059604645\n",
      "  episode_reward_mean: -0.3619999990612268\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 704\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.5250976582368216\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.013344346150107541\n",
      "          policy_loss: -0.09534750092175923\n",
      "          total_loss: -0.08465715481205345\n",
      "          vf_explained_var: -0.14217694103717804\n",
      "          vf_loss: 0.004945042288714571\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.611591415036292\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010784032126621443\n",
      "          policy_loss: -0.08859780456632074\n",
      "          total_loss: -0.0770276041399657\n",
      "          vf_explained_var: -0.23727060854434967\n",
      "          vf_loss: 0.005344850660808983\n",
      "    num_agent_steps_sampled: 341600\n",
      "    num_agent_steps_trained: 341600\n",
      "    num_steps_sampled: 170800\n",
      "    num_steps_trained: 170800\n",
      "  iterations_since_restore: 61\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.867999999999999\n",
      "    gpu_util_percent0: 0.056799999999999996\n",
      "    ram_util_percent: 54.275999999999996\n",
      "    vram_util_percent0: 0.21670312500000002\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.4000000059604645\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.21\n",
      "    agent_1: -0.15199999906122685\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051826142490149996\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.896832460291092\n",
      "    mean_inference_ms: 1.779021613818112\n",
      "    mean_raw_obs_processing_ms: 0.15848730698645164\n",
      "  time_since_restore: 1131.304608821869\n",
      "  time_this_iter_s: 18.882086753845215\n",
      "  time_total_s: 1131.304608821869\n",
      "  timers:\n",
      "    learn_throughput: 657.328\n",
      "    learn_time_ms: 4259.67\n",
      "    load_throughput: 96435.587\n",
      "    load_time_ms: 29.035\n",
      "    sample_throughput: 199.594\n",
      "    sample_time_ms: 14028.479\n",
      "    update_time_ms: 3.219\n",
      "  timestamp: 1658496051\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 170800\n",
      "  training_iteration: 61\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    61</td><td style=\"text-align: right;\">          1131.3</td><td style=\"text-align: right;\">170800</td><td style=\"text-align: right;\">  -0.362</td><td style=\"text-align: right;\">                 0.4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            254.03</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 347200\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-21-09\n",
      "  done: false\n",
      "  episode_len_mean: 248.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.4000000059604645\n",
      "  episode_reward_mean: -0.2949999989569187\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 714\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.539495966973759\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.016447834071367032\n",
      "          policy_loss: -0.09119692539319485\n",
      "          total_loss: -0.07865093679876216\n",
      "          vf_explained_var: -0.14632312953472137\n",
      "          vf_loss: 0.003614809212671216\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.598345094493457\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.012982822388483785\n",
      "          policy_loss: -0.08519328812724866\n",
      "          total_loss: -0.07224203631908278\n",
      "          vf_explained_var: 0.05604758858680725\n",
      "          vf_loss: 0.0029947555399086837\n",
      "    num_agent_steps_sampled: 347200\n",
      "    num_agent_steps_trained: 347200\n",
      "    num_steps_sampled: 173600\n",
      "    num_steps_trained: 173600\n",
      "  iterations_since_restore: 62\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.875\n",
      "    gpu_util_percent0: 0.048749999999999995\n",
      "    ram_util_percent: 54.20000000000001\n",
      "    vram_util_percent0: 0.21745605468749998\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.4000000059604645\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.18\n",
      "    agent_1: -0.11499999895691872\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05181707482875037\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.896452606452778\n",
      "    mean_inference_ms: 1.7785773321238123\n",
      "    mean_raw_obs_processing_ms: 0.15844290956620655\n",
      "  time_since_restore: 1149.961873292923\n",
      "  time_this_iter_s: 18.657264471054077\n",
      "  time_total_s: 1149.961873292923\n",
      "  timers:\n",
      "    learn_throughput: 657.211\n",
      "    learn_time_ms: 4260.425\n",
      "    load_throughput: 96300.525\n",
      "    load_time_ms: 29.076\n",
      "    sample_throughput: 198.973\n",
      "    sample_time_ms: 14072.256\n",
      "    update_time_ms: 3.219\n",
      "  timestamp: 1658496069\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 173600\n",
      "  training_iteration: 62\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    62</td><td style=\"text-align: right;\">         1149.96</td><td style=\"text-align: right;\">173600</td><td style=\"text-align: right;\">  -0.295</td><td style=\"text-align: right;\">                 0.4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             248.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 352800\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-21-28\n",
      "  done: false\n",
      "  episode_len_mean: 253.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.4000000059604645\n",
      "  episode_reward_mean: -0.31199999891221525\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 726\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.5159235575369427\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011974426566059135\n",
      "          policy_loss: -0.10173942335781508\n",
      "          total_loss: -0.09197854777613386\n",
      "          vf_explained_var: -0.29501649737358093\n",
      "          vf_loss: 0.005213816704088545\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.611345356418973\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01106337753882924\n",
      "          policy_loss: -0.10779532071319409\n",
      "          total_loss: -0.09624477556436821\n",
      "          vf_explained_var: -0.043083809316158295\n",
      "          vf_loss: 0.0044910080345982265\n",
      "    num_agent_steps_sampled: 352800\n",
      "    num_agent_steps_trained: 352800\n",
      "    num_steps_sampled: 176400\n",
      "    num_steps_trained: 176400\n",
      "  iterations_since_restore: 63\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.848\n",
      "    gpu_util_percent0: 0.0572\n",
      "    ram_util_percent: 54.248000000000005\n",
      "    vram_util_percent0: 0.21668750000000003\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.4000000059604645\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.19\n",
      "    agent_1: -0.12199999891221523\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05180951715337125\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.896093391807492\n",
      "    mean_inference_ms: 1.7781922597410051\n",
      "    mean_raw_obs_processing_ms: 0.15837076329609318\n",
      "  time_since_restore: 1168.619276046753\n",
      "  time_this_iter_s: 18.657402753829956\n",
      "  time_total_s: 1168.619276046753\n",
      "  timers:\n",
      "    learn_throughput: 656.826\n",
      "    learn_time_ms: 4262.925\n",
      "    load_throughput: 96335.124\n",
      "    load_time_ms: 29.065\n",
      "    sample_throughput: 198.481\n",
      "    sample_time_ms: 14107.168\n",
      "    update_time_ms: 3.22\n",
      "  timestamp: 1658496088\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 176400\n",
      "  training_iteration: 63\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    63</td><td style=\"text-align: right;\">         1168.62</td><td style=\"text-align: right;\">176400</td><td style=\"text-align: right;\">  -0.312</td><td style=\"text-align: right;\">                 0.4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            253.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 358400\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-21-47\n",
      "  done: false\n",
      "  episode_len_mean: 241.54\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.4000000059604645\n",
      "  episode_reward_mean: -0.2719999989122152\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 741\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.4998341287885393\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.015004512344308631\n",
      "          policy_loss: -0.1313681442497043\n",
      "          total_loss: -0.11999679731421306\n",
      "          vf_explained_var: -0.42077040672302246\n",
      "          vf_loss: 0.0033046624810681565\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.608210985149656\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.013498699395958522\n",
      "          policy_loss: -0.1310066020132148\n",
      "          total_loss: -0.11736835001327217\n",
      "          vf_explained_var: 0.1085706427693367\n",
      "          vf_loss: 0.0034949706142859732\n",
      "    num_agent_steps_sampled: 358400\n",
      "    num_agent_steps_trained: 358400\n",
      "    num_steps_sampled: 179200\n",
      "    num_steps_trained: 179200\n",
      "  iterations_since_restore: 64\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.888000000000001\n",
      "    gpu_util_percent0: 0.0516\n",
      "    ram_util_percent: 54.292\n",
      "    vram_util_percent0: 0.21648437500000003\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.4000000059604645\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.17\n",
      "    agent_1: -0.10199999891221523\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05180593576754149\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.895979511736141\n",
      "    mean_inference_ms: 1.7779623405764462\n",
      "    mean_raw_obs_processing_ms: 0.15837861376823295\n",
      "  time_since_restore: 1187.3903937339783\n",
      "  time_this_iter_s: 18.771117687225342\n",
      "  time_total_s: 1187.3903937339783\n",
      "  timers:\n",
      "    learn_throughput: 657.273\n",
      "    learn_time_ms: 4260.026\n",
      "    load_throughput: 95997.149\n",
      "    load_time_ms: 29.168\n",
      "    sample_throughput: 197.847\n",
      "    sample_time_ms: 14152.329\n",
      "    update_time_ms: 3.23\n",
      "  timestamp: 1658496107\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 179200\n",
      "  training_iteration: 64\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">         1187.39</td><td style=\"text-align: right;\">179200</td><td style=\"text-align: right;\">  -0.272</td><td style=\"text-align: right;\">                 0.4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            241.54</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 364000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-22-06\n",
      "  done: false\n",
      "  episode_len_mean: 249.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.4000000059604645\n",
      "  episode_reward_mean: -0.2899999988824129\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 750\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.555158026871227\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01189848473099571\n",
      "          policy_loss: -0.0702672064113098\n",
      "          total_loss: -0.06114414894476622\n",
      "          vf_explained_var: -0.046882908791303635\n",
      "          vf_loss: 0.0036022276478130757\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.609489592767897\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.00989832894618095\n",
      "          policy_loss: -0.06900165136539727\n",
      "          total_loss: -0.05869976794643402\n",
      "          vf_explained_var: -0.046188004314899445\n",
      "          vf_loss: 0.004250120724680615\n",
      "    num_agent_steps_sampled: 364000\n",
      "    num_agent_steps_trained: 364000\n",
      "    num_steps_sampled: 182000\n",
      "    num_steps_trained: 182000\n",
      "  iterations_since_restore: 65\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.966666666666668\n",
      "    gpu_util_percent0: 0.050833333333333335\n",
      "    ram_util_percent: 54.29999999999999\n",
      "    vram_util_percent0: 0.21749267578125\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.4000000059604645\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.18\n",
      "    agent_1: -0.10999999888241291\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051807040202458784\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8960301462091014\n",
      "    mean_inference_ms: 1.7779650810379195\n",
      "    mean_raw_obs_processing_ms: 0.15835799020478902\n",
      "  time_since_restore: 1206.0851700305939\n",
      "  time_this_iter_s: 18.6947762966156\n",
      "  time_total_s: 1206.0851700305939\n",
      "  timers:\n",
      "    learn_throughput: 657.896\n",
      "    learn_time_ms: 4255.991\n",
      "    load_throughput: 96109.884\n",
      "    load_time_ms: 29.133\n",
      "    sample_throughput: 196.996\n",
      "    sample_time_ms: 14213.502\n",
      "    update_time_ms: 3.207\n",
      "  timestamp: 1658496126\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 182000\n",
      "  training_iteration: 65\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">         1206.09</td><td style=\"text-align: right;\">182000</td><td style=\"text-align: right;\">   -0.29</td><td style=\"text-align: right;\">                 0.4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             249.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 369600\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-22-24\n",
      "  done: false\n",
      "  episode_len_mean: 254.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.4000000059604645\n",
      "  episode_reward_mean: -0.3309999988973141\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 761\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.5234366548912868\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.015184392684680128\n",
      "          policy_loss: -0.06195224448345557\n",
      "          total_loss: -0.05012297768206779\n",
      "          vf_explained_var: -0.13108770549297333\n",
      "          vf_loss: 0.004254775396475452\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.6078275584039234\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011881795784328875\n",
      "          policy_loss: -0.055637385430021\n",
      "          total_loss: -0.04324471037610541\n",
      "          vf_explained_var: -0.246718630194664\n",
      "          vf_loss: 0.00455452656535185\n",
      "    num_agent_steps_sampled: 369600\n",
      "    num_agent_steps_trained: 369600\n",
      "    num_steps_sampled: 184800\n",
      "    num_steps_trained: 184800\n",
      "  iterations_since_restore: 66\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.928\n",
      "    gpu_util_percent0: 0.0616\n",
      "    ram_util_percent: 54.29599999999999\n",
      "    vram_util_percent0: 0.21669921875\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.4000000059604645\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.2\n",
      "    agent_1: -0.13099999889731406\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05181176326254455\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8961736476899476\n",
      "    mean_inference_ms: 1.7781041380759819\n",
      "    mean_raw_obs_processing_ms: 0.1583200206370807\n",
      "  time_since_restore: 1224.6985783576965\n",
      "  time_this_iter_s: 18.61340832710266\n",
      "  time_total_s: 1224.6985783576965\n",
      "  timers:\n",
      "    learn_throughput: 657.792\n",
      "    learn_time_ms: 4256.663\n",
      "    load_throughput: 96220.756\n",
      "    load_time_ms: 29.1\n",
      "    sample_throughput: 196.467\n",
      "    sample_time_ms: 14251.748\n",
      "    update_time_ms: 3.186\n",
      "  timestamp: 1658496144\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 184800\n",
      "  training_iteration: 66\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    66</td><td style=\"text-align: right;\">          1224.7</td><td style=\"text-align: right;\">184800</td><td style=\"text-align: right;\">  -0.331</td><td style=\"text-align: right;\">                 0.4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            254.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 375200\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-22-43\n",
      "  done: false\n",
      "  episode_len_mean: 243.95\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.4000000059604645\n",
      "  episode_reward_mean: -0.35199999891221523\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 775\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.4502314974864325\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.013590110537993416\n",
      "          policy_loss: -0.05489961622706254\n",
      "          total_loss: -0.04387041003376778\n",
      "          vf_explained_var: -0.019635405391454697\n",
      "          vf_loss: 0.0052973122985263\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.552120581269264\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.012367633902395269\n",
      "          policy_loss: -0.06318574824226311\n",
      "          total_loss: -0.04997160724209001\n",
      "          vf_explained_var: 0.11183693259954453\n",
      "          vf_loss: 0.005446494923273262\n",
      "    num_agent_steps_sampled: 375200\n",
      "    num_agent_steps_trained: 375200\n",
      "    num_steps_sampled: 187600\n",
      "    num_steps_trained: 187600\n",
      "  iterations_since_restore: 67\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.864\n",
      "    gpu_util_percent0: 0.0592\n",
      "    ram_util_percent: 54.251999999999995\n",
      "    vram_util_percent0: 0.2173046875\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.4000000059604645\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.21\n",
      "    agent_1: -0.14199999891221524\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05182209771973202\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.89652429480103\n",
      "    mean_inference_ms: 1.7784883126038664\n",
      "    mean_raw_obs_processing_ms: 0.15833826805309406\n",
      "  time_since_restore: 1243.4471428394318\n",
      "  time_this_iter_s: 18.74856448173523\n",
      "  time_total_s: 1243.4471428394318\n",
      "  timers:\n",
      "    learn_throughput: 658.804\n",
      "    learn_time_ms: 4250.124\n",
      "    load_throughput: 96180.488\n",
      "    load_time_ms: 29.112\n",
      "    sample_throughput: 195.846\n",
      "    sample_time_ms: 14296.961\n",
      "    update_time_ms: 3.193\n",
      "  timestamp: 1658496163\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 187600\n",
      "  training_iteration: 67\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    67</td><td style=\"text-align: right;\">         1243.45</td><td style=\"text-align: right;\">187600</td><td style=\"text-align: right;\">  -0.352</td><td style=\"text-align: right;\">                 0.4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            243.95</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 380800\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-23-02\n",
      "  done: false\n",
      "  episode_len_mean: 248.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.4000000059604645\n",
      "  episode_reward_mean: -0.29199999891221523\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 784\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.495018408412025\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01599488072886072\n",
      "          policy_loss: -0.08274827508579446\n",
      "          total_loss: -0.07071766746996998\n",
      "          vf_explained_var: 0.05002939701080322\n",
      "          vf_loss: 0.003060532450988484\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.586869383142108\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.014916819510987793\n",
      "          policy_loss: -0.10335967189651758\n",
      "          total_loss: -0.08873793393944479\n",
      "          vf_explained_var: 0.29810816049575806\n",
      "          vf_loss: 0.0022288918232031087\n",
      "    num_agent_steps_sampled: 380800\n",
      "    num_agent_steps_trained: 380800\n",
      "    num_steps_sampled: 190400\n",
      "    num_steps_trained: 190400\n",
      "  iterations_since_restore: 68\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.862500000000001\n",
      "    gpu_util_percent0: 0.05958333333333333\n",
      "    ram_util_percent: 54.29999999999999\n",
      "    vram_util_percent0: 0.21656494140624996\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.4000000059604645\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.18\n",
      "    agent_1: -0.11199999891221524\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051829165501073096\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8967640686798184\n",
      "    mean_inference_ms: 1.7787512131397836\n",
      "    mean_raw_obs_processing_ms: 0.15833927054726207\n",
      "  time_since_restore: 1261.8876099586487\n",
      "  time_this_iter_s: 18.44046711921692\n",
      "  time_total_s: 1261.8876099586487\n",
      "  timers:\n",
      "    learn_throughput: 659.697\n",
      "    learn_time_ms: 4244.375\n",
      "    load_throughput: 96265.95\n",
      "    load_time_ms: 29.086\n",
      "    sample_throughput: 195.594\n",
      "    sample_time_ms: 14315.379\n",
      "    update_time_ms: 3.218\n",
      "  timestamp: 1658496182\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 190400\n",
      "  training_iteration: 68\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    68</td><td style=\"text-align: right;\">         1261.89</td><td style=\"text-align: right;\">190400</td><td style=\"text-align: right;\">  -0.292</td><td style=\"text-align: right;\">                 0.4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            248.47</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 386400\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-23-20\n",
      "  done: false\n",
      "  episode_len_mean: 241.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.4000000059604645\n",
      "  episode_reward_mean: -0.2709999988973141\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 796\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.475428982149987\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01894469342501392\n",
      "          policy_loss: -0.08394922389925341\n",
      "          total_loss: -0.07009326603344691\n",
      "          vf_explained_var: -0.046874068677425385\n",
      "          vf_loss: 0.001932491939312188\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.527384726064546\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.014668008221835676\n",
      "          policy_loss: -0.09252134241354984\n",
      "          total_loss: -0.07814808141085364\n",
      "          vf_explained_var: 0.1421690285205841\n",
      "          vf_loss: 0.002159217257940327\n",
      "    num_agent_steps_sampled: 386400\n",
      "    num_agent_steps_trained: 386400\n",
      "    num_steps_sampled: 193200\n",
      "    num_steps_trained: 193200\n",
      "  iterations_since_restore: 69\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.895833333333333\n",
      "    gpu_util_percent0: 0.048749999999999995\n",
      "    ram_util_percent: 54.29999999999999\n",
      "    vram_util_percent0: 0.216650390625\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.4000000059604645\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.17\n",
      "    agent_1: -0.10099999889731408\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05183803585174628\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8969145727535204\n",
      "    mean_inference_ms: 1.7790963910781965\n",
      "    mean_raw_obs_processing_ms: 0.1583754457713958\n",
      "  time_since_restore: 1280.503536939621\n",
      "  time_this_iter_s: 18.61592698097229\n",
      "  time_total_s: 1280.503536939621\n",
      "  timers:\n",
      "    learn_throughput: 660.285\n",
      "    learn_time_ms: 4240.595\n",
      "    load_throughput: 89885.556\n",
      "    load_time_ms: 31.151\n",
      "    sample_throughput: 195.177\n",
      "    sample_time_ms: 14345.927\n",
      "    update_time_ms: 3.237\n",
      "  timestamp: 1658496200\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 193200\n",
      "  training_iteration: 69\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    69</td><td style=\"text-align: right;\">          1280.5</td><td style=\"text-align: right;\">193200</td><td style=\"text-align: right;\">  -0.271</td><td style=\"text-align: right;\">                 0.4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            241.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 392000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-23-39\n",
      "  done: false\n",
      "  episode_len_mean: 242.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.30000001192092896\n",
      "  episode_reward_mean: -0.2549999989569187\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 808\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.503230799521719\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.012874128858025065\n",
      "          policy_loss: -0.06624584515174363\n",
      "          total_loss: -0.05629141563016068\n",
      "          vf_explained_var: -0.22925713658332825\n",
      "          vf_loss: 0.0038250243683034483\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.5623557439872195\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011442631864400041\n",
      "          policy_loss: -0.07051135033829528\n",
      "          total_loss: -0.058784608243426885\n",
      "          vf_explained_var: -0.1442875862121582\n",
      "          vf_loss: 0.0038531884632296474\n",
      "    num_agent_steps_sampled: 392000\n",
      "    num_agent_steps_trained: 392000\n",
      "    num_steps_sampled: 196000\n",
      "    num_steps_trained: 196000\n",
      "  iterations_since_restore: 70\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.939999999999999\n",
      "    gpu_util_percent0: 0.053599999999999995\n",
      "    ram_util_percent: 54.46799999999999\n",
      "    vram_util_percent0: 0.21748437499999998\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.30000001192092896\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.16\n",
      "    agent_1: -0.09499999895691871\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05184760080598789\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8970391378748754\n",
      "    mean_inference_ms: 1.7794254604886748\n",
      "    mean_raw_obs_processing_ms: 0.1584132481313996\n",
      "  time_since_restore: 1299.2819623947144\n",
      "  time_this_iter_s: 18.778425455093384\n",
      "  time_total_s: 1299.2819623947144\n",
      "  timers:\n",
      "    learn_throughput: 660.851\n",
      "    learn_time_ms: 4236.964\n",
      "    load_throughput: 89699.162\n",
      "    load_time_ms: 31.215\n",
      "    sample_throughput: 194.927\n",
      "    sample_time_ms: 14364.368\n",
      "    update_time_ms: 3.246\n",
      "  timestamp: 1658496219\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 196000\n",
      "  training_iteration: 70\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    70</td><td style=\"text-align: right;\">         1299.28</td><td style=\"text-align: right;\">196000</td><td style=\"text-align: right;\">  -0.255</td><td style=\"text-align: right;\">                 0.3</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            242.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 397600\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-23-58\n",
      "  done: false\n",
      "  episode_len_mean: 242.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.30000001192092896\n",
      "  episode_reward_mean: -0.25699999898672105\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 818\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.497096912491889\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.012455179358534473\n",
      "          policy_loss: -0.07020506888234411\n",
      "          total_loss: -0.06065107339444304\n",
      "          vf_explained_var: -0.2612075209617615\n",
      "          vf_loss: 0.003571595778859254\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.5952550165709996\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010757058206814628\n",
      "          policy_loss: -0.0731915484149275\n",
      "          total_loss: -0.06207454369203853\n",
      "          vf_explained_var: -0.0834336131811142\n",
      "          vf_loss: 0.004108734555692146\n",
      "    num_agent_steps_sampled: 397600\n",
      "    num_agent_steps_trained: 397600\n",
      "    num_steps_sampled: 198800\n",
      "    num_steps_trained: 198800\n",
      "  iterations_since_restore: 71\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.88\n",
      "    gpu_util_percent0: 0.055200000000000006\n",
      "    ram_util_percent: 54.523999999999994\n",
      "    vram_util_percent0: 0.21670312500000002\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.30000001192092896\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.16\n",
      "    agent_1: -0.09699999898672104\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051853757669382626\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.897120605432633\n",
      "    mean_inference_ms: 1.7796383782963787\n",
      "    mean_raw_obs_processing_ms: 0.15843502587017563\n",
      "  time_since_restore: 1317.9548020362854\n",
      "  time_this_iter_s: 18.672839641571045\n",
      "  time_total_s: 1317.9548020362854\n",
      "  timers:\n",
      "    learn_throughput: 660.565\n",
      "    learn_time_ms: 4238.795\n",
      "    load_throughput: 89771.5\n",
      "    load_time_ms: 31.19\n",
      "    sample_throughput: 195.24\n",
      "    sample_time_ms: 14341.333\n",
      "    update_time_ms: 3.275\n",
      "  timestamp: 1658496238\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 198800\n",
      "  training_iteration: 71\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    71</td><td style=\"text-align: right;\">         1317.95</td><td style=\"text-align: right;\">198800</td><td style=\"text-align: right;\">  -0.257</td><td style=\"text-align: right;\">                 0.3</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            242.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 403200\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-24-16\n",
      "  done: false\n",
      "  episode_len_mean: 244.58\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.20000000298023224\n",
      "  episode_reward_mean: -0.3009999991208315\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 830\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.4715289885089513\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.013318390480361758\n",
      "          policy_loss: -0.06567326805142264\n",
      "          total_loss: -0.05438059027697558\n",
      "          vf_explained_var: -0.24787071347236633\n",
      "          vf_loss: 0.00665646401986513\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.579787646021162\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010948441094509183\n",
      "          policy_loss: -0.08146861070972158\n",
      "          total_loss: -0.06998981275856272\n",
      "          vf_explained_var: -0.018448758870363235\n",
      "          vf_loss: 0.004576830941005028\n",
      "    num_agent_steps_sampled: 403200\n",
      "    num_agent_steps_trained: 403200\n",
      "    num_steps_sampled: 201600\n",
      "    num_steps_trained: 201600\n",
      "  iterations_since_restore: 72\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.9416666666666655\n",
      "    gpu_util_percent0: 0.0525\n",
      "    ram_util_percent: 54.32916666666667\n",
      "    vram_util_percent0: 0.21778157552083333\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.20000000298023224\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.18\n",
      "    agent_1: -0.1209999991208315\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051861361383894806\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8972440969742648\n",
      "    mean_inference_ms: 1.7799261586192319\n",
      "    mean_raw_obs_processing_ms: 0.15844852243175878\n",
      "  time_since_restore: 1336.6835615634918\n",
      "  time_this_iter_s: 18.72875952720642\n",
      "  time_total_s: 1336.6835615634918\n",
      "  timers:\n",
      "    learn_throughput: 661.201\n",
      "    learn_time_ms: 4234.719\n",
      "    load_throughput: 89585.784\n",
      "    load_time_ms: 31.255\n",
      "    sample_throughput: 195.089\n",
      "    sample_time_ms: 14352.432\n",
      "    update_time_ms: 3.272\n",
      "  timestamp: 1658496256\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 201600\n",
      "  training_iteration: 72\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    72</td><td style=\"text-align: right;\">         1336.68</td><td style=\"text-align: right;\">201600</td><td style=\"text-align: right;\">  -0.301</td><td style=\"text-align: right;\">                 0.2</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            244.58</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 408800\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-24-35\n",
      "  done: false\n",
      "  episode_len_mean: 242.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.20000000298023224\n",
      "  episode_reward_mean: -0.32399999916553496\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 845\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.4309028827008747\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.014226615537694007\n",
      "          policy_loss: -0.07034142709259565\n",
      "          total_loss: -0.05796348330442838\n",
      "          vf_explained_var: -0.27214041352272034\n",
      "          vf_loss: 0.007761797223100162\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.566998923108691\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011089360767378067\n",
      "          policy_loss: -0.07189488031768373\n",
      "          total_loss: -0.0593874142726972\n",
      "          vf_explained_var: -0.2122970074415207\n",
      "          vf_loss: 0.0070958765240253085\n",
      "    num_agent_steps_sampled: 408800\n",
      "    num_agent_steps_trained: 408800\n",
      "    num_steps_sampled: 204400\n",
      "    num_steps_trained: 204400\n",
      "  iterations_since_restore: 73\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.88\n",
      "    gpu_util_percent0: 0.057999999999999996\n",
      "    ram_util_percent: 54.328\n",
      "    vram_util_percent0: 0.21692187500000004\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.20000000298023224\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.19\n",
      "    agent_1: -0.13399999916553498\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05186850123352066\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8973068673047044\n",
      "    mean_inference_ms: 1.780165019498022\n",
      "    mean_raw_obs_processing_ms: 0.1584810542592679\n",
      "  time_since_restore: 1355.2303161621094\n",
      "  time_this_iter_s: 18.546754598617554\n",
      "  time_total_s: 1355.2303161621094\n",
      "  timers:\n",
      "    learn_throughput: 660.761\n",
      "    learn_time_ms: 4237.539\n",
      "    load_throughput: 89660.334\n",
      "    load_time_ms: 31.229\n",
      "    sample_throughput: 195.28\n",
      "    sample_time_ms: 14338.402\n",
      "    update_time_ms: 3.297\n",
      "  timestamp: 1658496275\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 204400\n",
      "  training_iteration: 73\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    73</td><td style=\"text-align: right;\">         1355.23</td><td style=\"text-align: right;\">204400</td><td style=\"text-align: right;\">  -0.324</td><td style=\"text-align: right;\">                 0.2</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            242.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 414400\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-24-53\n",
      "  done: false\n",
      "  episode_len_mean: 228.51\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.20000000298023224\n",
      "  episode_reward_mean: -0.2669999992102385\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 858\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.4202680630343303\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.016908499343029567\n",
      "          policy_loss: -0.10692070879269436\n",
      "          total_loss: -0.093994566233873\n",
      "          vf_explained_var: -0.00914869736880064\n",
      "          vf_loss: 0.0035721859099534675\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.574558988213539\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01316136179793839\n",
      "          policy_loss: -0.10389892334344386\n",
      "          total_loss: -0.09048331781247855\n",
      "          vf_explained_var: -0.11791767179965973\n",
      "          vf_loss: 0.0037824138228946444\n",
      "    num_agent_steps_sampled: 414400\n",
      "    num_agent_steps_trained: 414400\n",
      "    num_steps_sampled: 207200\n",
      "    num_steps_trained: 207200\n",
      "  iterations_since_restore: 74\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.866666666666667\n",
      "    gpu_util_percent0: 0.05625\n",
      "    ram_util_percent: 54.29999999999999\n",
      "    vram_util_percent0: 0.2163736979166667\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.20000000298023224\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.16\n",
      "    agent_1: -0.10699999921023845\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05186970481770412\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8972534474390894\n",
      "    mean_inference_ms: 1.7802095383380452\n",
      "    mean_raw_obs_processing_ms: 0.1585419749480671\n",
      "  time_since_restore: 1373.593311548233\n",
      "  time_this_iter_s: 18.362995386123657\n",
      "  time_total_s: 1373.593311548233\n",
      "  timers:\n",
      "    learn_throughput: 660.174\n",
      "    learn_time_ms: 4241.307\n",
      "    load_throughput: 89938.697\n",
      "    load_time_ms: 31.132\n",
      "    sample_throughput: 195.889\n",
      "    sample_time_ms: 14293.796\n",
      "    update_time_ms: 3.294\n",
      "  timestamp: 1658496293\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 207200\n",
      "  training_iteration: 74\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    74</td><td style=\"text-align: right;\">         1373.59</td><td style=\"text-align: right;\">207200</td><td style=\"text-align: right;\">  -0.267</td><td style=\"text-align: right;\">                 0.2</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            228.51</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 420000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-25-12\n",
      "  done: false\n",
      "  episode_len_mean: 232.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.20000000298023224\n",
      "  episode_reward_mean: -0.2309999992698431\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 870\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.4196380220708393\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.016308466288203077\n",
      "          policy_loss: -0.08176157713097339\n",
      "          total_loss: -0.06989248633553438\n",
      "          vf_explained_var: -0.08581551164388657\n",
      "          vf_loss: 0.0018385852795940341\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.55715578297774\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.014211605822256576\n",
      "          policy_loss: -0.09057890838188801\n",
      "          total_loss: -0.07689555853375211\n",
      "          vf_explained_var: 0.06292883306741714\n",
      "          vf_loss: 0.0015279788362282229\n",
      "    num_agent_steps_sampled: 420000\n",
      "    num_agent_steps_trained: 420000\n",
      "    num_steps_sampled: 210000\n",
      "    num_steps_trained: 210000\n",
      "  iterations_since_restore: 75\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.895833333333333\n",
      "    gpu_util_percent0: 0.05125\n",
      "    ram_util_percent: 54.29583333333333\n",
      "    vram_util_percent0: 0.2175333658854167\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.20000000298023224\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.14\n",
      "    agent_1: -0.0909999992698431\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051867723609209646\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.89706191254309\n",
      "    mean_inference_ms: 1.7801289830982439\n",
      "    mean_raw_obs_processing_ms: 0.1585763839162387\n",
      "  time_since_restore: 1391.9345338344574\n",
      "  time_this_iter_s: 18.341222286224365\n",
      "  time_total_s: 1391.9345338344574\n",
      "  timers:\n",
      "    learn_throughput: 658.875\n",
      "    learn_time_ms: 4249.667\n",
      "    load_throughput: 89775.891\n",
      "    load_time_ms: 31.189\n",
      "    sample_throughput: 196.49\n",
      "    sample_time_ms: 14250.071\n",
      "    update_time_ms: 3.271\n",
      "  timestamp: 1658496312\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 210000\n",
      "  training_iteration: 75\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    75</td><td style=\"text-align: right;\">         1391.93</td><td style=\"text-align: right;\">210000</td><td style=\"text-align: right;\">  -0.231</td><td style=\"text-align: right;\">                 0.2</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            232.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 425600\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-25-30\n",
      "  done: false\n",
      "  episode_len_mean: 244.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.20000000298023224\n",
      "  episode_reward_mean: -0.23199999928474427\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 878\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.421853599448999\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.0169643816393427\n",
      "          policy_loss: -0.06279769848763922\n",
      "          total_loss: -0.05017783221930066\n",
      "          vf_explained_var: 0.13996250927448273\n",
      "          vf_loss: 0.0025801044990831066\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.5810130210149858\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010622458199899012\n",
      "          policy_loss: -0.060579487719223835\n",
      "          total_loss: -0.05014262609377814\n",
      "          vf_explained_var: 0.1845502257347107\n",
      "          vf_loss: 0.0025344844487422266\n",
      "    num_agent_steps_sampled: 425600\n",
      "    num_agent_steps_trained: 425600\n",
      "    num_steps_sampled: 212800\n",
      "    num_steps_trained: 212800\n",
      "  iterations_since_restore: 76\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.875\n",
      "    gpu_util_percent0: 0.052083333333333336\n",
      "    ram_util_percent: 54.29999999999999\n",
      "    vram_util_percent0: 0.21675618489583334\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.20000000298023224\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.14\n",
      "    agent_1: -0.09199999928474427\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05186538730042807\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.896899377454579\n",
      "    mean_inference_ms: 1.7800220573717846\n",
      "    mean_raw_obs_processing_ms: 0.15857626384188986\n",
      "  time_since_restore: 1410.1759581565857\n",
      "  time_this_iter_s: 18.241424322128296\n",
      "  time_total_s: 1410.1759581565857\n",
      "  timers:\n",
      "    learn_throughput: 658.207\n",
      "    learn_time_ms: 4253.978\n",
      "    load_throughput: 89588.517\n",
      "    load_time_ms: 31.254\n",
      "    sample_throughput: 197.067\n",
      "    sample_time_ms: 14208.401\n",
      "    update_time_ms: 3.296\n",
      "  timestamp: 1658496330\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 212800\n",
      "  training_iteration: 76\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    76</td><td style=\"text-align: right;\">         1410.18</td><td style=\"text-align: right;\">212800</td><td style=\"text-align: right;\">  -0.232</td><td style=\"text-align: right;\">                 0.2</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             244.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 431200\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-25-48\n",
      "  done: false\n",
      "  episode_len_mean: 245.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.20000000298023224\n",
      "  episode_reward_mean: -0.21299999929964542\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 885\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.464175975038892\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01754584726036741\n",
      "          policy_loss: -0.09601300888295684\n",
      "          total_loss: -0.08350887618629106\n",
      "          vf_explained_var: 0.24732926487922668\n",
      "          vf_loss: 0.0010550532220544742\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.6062625447909036\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.014826440130947376\n",
      "          policy_loss: -0.0771389882360208\n",
      "          total_loss: -0.0630574715327211\n",
      "          vf_explained_var: 0.015214741230010986\n",
      "          vf_loss: 0.0009677647240096121\n",
      "    num_agent_steps_sampled: 431200\n",
      "    num_agent_steps_trained: 431200\n",
      "    num_steps_sampled: 215600\n",
      "    num_steps_trained: 215600\n",
      "  iterations_since_restore: 77\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.900000000000001\n",
      "    gpu_util_percent0: 0.050833333333333335\n",
      "    ram_util_percent: 54.29999999999999\n",
      "    vram_util_percent0: 0.21753743489583333\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.20000000298023224\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.13\n",
      "    agent_1: -0.08299999929964542\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05186310345420689\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.896770602594399\n",
      "    mean_inference_ms: 1.7799111188791812\n",
      "    mean_raw_obs_processing_ms: 0.15855967562467407\n",
      "  time_since_restore: 1428.4209418296814\n",
      "  time_this_iter_s: 18.244983673095703\n",
      "  time_total_s: 1428.4209418296814\n",
      "  timers:\n",
      "    learn_throughput: 657.972\n",
      "    learn_time_ms: 4255.501\n",
      "    load_throughput: 84277.61\n",
      "    load_time_ms: 33.224\n",
      "    sample_throughput: 197.78\n",
      "    sample_time_ms: 14157.127\n",
      "    update_time_ms: 3.31\n",
      "  timestamp: 1658496348\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 215600\n",
      "  training_iteration: 77\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    77</td><td style=\"text-align: right;\">         1428.42</td><td style=\"text-align: right;\">215600</td><td style=\"text-align: right;\">  -0.213</td><td style=\"text-align: right;\">                 0.2</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            245.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 436800\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-26-07\n",
      "  done: false\n",
      "  episode_len_mean: 249.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.4000000059604645\n",
      "  episode_reward_mean: -0.2719999992847443\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 897\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.4311419413203286\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01172589153029097\n",
      "          policy_loss: -0.03788661282201896\n",
      "          total_loss: -0.02838740655958342\n",
      "          vf_explained_var: -0.033285368233919144\n",
      "          vf_loss: 0.004898295428574784\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.497996042172114\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010609758338734999\n",
      "          policy_loss: -0.05036478573206945\n",
      "          total_loss: -0.03908277265976982\n",
      "          vf_explained_var: -0.1552218645811081\n",
      "          vf_loss: 0.004884801342649596\n",
      "    num_agent_steps_sampled: 436800\n",
      "    num_agent_steps_trained: 436800\n",
      "    num_steps_sampled: 218400\n",
      "    num_steps_trained: 218400\n",
      "  iterations_since_restore: 78\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.895833333333333\n",
      "    gpu_util_percent0: 0.052083333333333336\n",
      "    ram_util_percent: 54.30416666666665\n",
      "    vram_util_percent0: 0.21717529296875002\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.4000000059604645\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.16\n",
      "    agent_1: -0.11199999928474426\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05185782501040219\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8965657804009766\n",
      "    mean_inference_ms: 1.7796674202529978\n",
      "    mean_raw_obs_processing_ms: 0.1585299336708207\n",
      "  time_since_restore: 1446.8018169403076\n",
      "  time_this_iter_s: 18.38087511062622\n",
      "  time_total_s: 1446.8018169403076\n",
      "  timers:\n",
      "    learn_throughput: 659.713\n",
      "    learn_time_ms: 4244.267\n",
      "    load_throughput: 84023.278\n",
      "    load_time_ms: 33.324\n",
      "    sample_throughput: 197.707\n",
      "    sample_time_ms: 14162.352\n",
      "    update_time_ms: 3.292\n",
      "  timestamp: 1658496367\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 218400\n",
      "  training_iteration: 78\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    78</td><td style=\"text-align: right;\">          1446.8</td><td style=\"text-align: right;\">218400</td><td style=\"text-align: right;\">  -0.272</td><td style=\"text-align: right;\">                 0.4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            249.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 442400\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-26-25\n",
      "  done: false\n",
      "  episode_len_mean: 252.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.4000000059604645\n",
      "  episode_reward_mean: -0.24999999925494193\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 907\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.4317064781983695\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.012808299292991982\n",
      "          policy_loss: -0.04383038451175837\n",
      "          total_loss: -0.03458313111245627\n",
      "          vf_explained_var: -0.019341083243489265\n",
      "          vf_loss: 0.0018622934221422752\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.5442209797246114\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01153470974876509\n",
      "          policy_loss: -0.08058149080967442\n",
      "          total_loss: -0.06960180635563856\n",
      "          vf_explained_var: -0.09203223884105682\n",
      "          vf_loss: 0.001436203320102842\n",
      "    num_agent_steps_sampled: 442400\n",
      "    num_agent_steps_trained: 442400\n",
      "    num_steps_sampled: 221200\n",
      "    num_steps_trained: 221200\n",
      "  iterations_since_restore: 79\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.864\n",
      "    gpu_util_percent0: 0.0604\n",
      "    ram_util_percent: 54.29999999999999\n",
      "    vram_util_percent0: 0.21646875000000002\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.4000000059604645\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.15\n",
      "    agent_1: -0.09999999925494193\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051851493032107426\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8963790621645193\n",
      "    mean_inference_ms: 1.7794183382489865\n",
      "    mean_raw_obs_processing_ms: 0.15848604699266322\n",
      "  time_since_restore: 1465.3215200901031\n",
      "  time_this_iter_s: 18.519703149795532\n",
      "  time_total_s: 1465.3215200901031\n",
      "  timers:\n",
      "    learn_throughput: 661.267\n",
      "    learn_time_ms: 4234.296\n",
      "    load_throughput: 89811.18\n",
      "    load_time_ms: 31.177\n",
      "    sample_throughput: 197.672\n",
      "    sample_time_ms: 14164.875\n",
      "    update_time_ms: 3.268\n",
      "  timestamp: 1658496385\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 221200\n",
      "  training_iteration: 79\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    79</td><td style=\"text-align: right;\">         1465.32</td><td style=\"text-align: right;\">221200</td><td style=\"text-align: right;\">   -0.25</td><td style=\"text-align: right;\">                 0.4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            252.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 448000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-26-44\n",
      "  done: false\n",
      "  episode_len_mean: 248.32\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.4000000059604645\n",
      "  episode_reward_mean: -0.25299999929964545\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 918\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.4030161606413976\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.014672290955484587\n",
      "          policy_loss: -0.09009748038979956\n",
      "          total_loss: -0.07918921789219201\n",
      "          vf_explained_var: 0.03826260566711426\n",
      "          vf_loss: 0.002579040860187628\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.5274572258903865\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010223406637589753\n",
      "          policy_loss: -0.08157042959473551\n",
      "          total_loss: -0.07138923321014902\n",
      "          vf_explained_var: -0.18982776999473572\n",
      "          vf_loss: 0.002880225465733453\n",
      "    num_agent_steps_sampled: 448000\n",
      "    num_agent_steps_trained: 448000\n",
      "    num_steps_sampled: 224000\n",
      "    num_steps_trained: 224000\n",
      "  iterations_since_restore: 80\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.8625\n",
      "    gpu_util_percent0: 0.05791666666666667\n",
      "    ram_util_percent: 54.29999999999999\n",
      "    vram_util_percent0: 0.2173502604166667\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.4000000059604645\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.15\n",
      "    agent_1: -0.10299999929964543\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05184467106624285\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8961519698056937\n",
      "    mean_inference_ms: 1.7791246205634628\n",
      "    mean_raw_obs_processing_ms: 0.15844827471355397\n",
      "  time_since_restore: 1483.745744228363\n",
      "  time_this_iter_s: 18.424224138259888\n",
      "  time_total_s: 1483.745744228363\n",
      "  timers:\n",
      "    learn_throughput: 662.793\n",
      "    learn_time_ms: 4224.55\n",
      "    load_throughput: 90070.445\n",
      "    load_time_ms: 31.087\n",
      "    sample_throughput: 198.028\n",
      "    sample_time_ms: 14139.418\n",
      "    update_time_ms: 3.268\n",
      "  timestamp: 1658496404\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 224000\n",
      "  training_iteration: 80\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">         1483.75</td><td style=\"text-align: right;\">224000</td><td style=\"text-align: right;\">  -0.253</td><td style=\"text-align: right;\">                 0.4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            248.32</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 453600\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-27-02\n",
      "  done: false\n",
      "  episode_len_mean: 251.58\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.4000000059604645\n",
      "  episode_reward_mean: -0.2719999992847443\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 931\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.4324494309368587\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.012578197318171008\n",
      "          policy_loss: -0.10021350016939409\n",
      "          total_loss: -0.0897140506034096\n",
      "          vf_explained_var: 0.22646404802799225\n",
      "          vf_loss: 0.005930428263820345\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.5291464704842794\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010815082067298361\n",
      "          policy_loss: -0.10089954934526413\n",
      "          total_loss: -0.0891092747582921\n",
      "          vf_explained_var: 0.14133243262767792\n",
      "          vf_loss: 0.005786569768783271\n",
      "    num_agent_steps_sampled: 453600\n",
      "    num_agent_steps_trained: 453600\n",
      "    num_steps_sampled: 226800\n",
      "    num_steps_trained: 226800\n",
      "  iterations_since_restore: 81\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.845833333333334\n",
      "    gpu_util_percent0: 0.04958333333333333\n",
      "    ram_util_percent: 54.29999999999999\n",
      "    vram_util_percent0: 0.21675618489583334\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.4000000059604645\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.16\n",
      "    agent_1: -0.11199999928474426\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051834015430192386\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8957437620226005\n",
      "    mean_inference_ms: 1.7786480483801546\n",
      "    mean_raw_obs_processing_ms: 0.1584082723290615\n",
      "  time_since_restore: 1502.2334878444672\n",
      "  time_this_iter_s: 18.487743616104126\n",
      "  time_total_s: 1502.2334878444672\n",
      "  timers:\n",
      "    learn_throughput: 664.901\n",
      "    learn_time_ms: 4211.155\n",
      "    load_throughput: 83870.744\n",
      "    load_time_ms: 33.385\n",
      "    sample_throughput: 198.128\n",
      "    sample_time_ms: 14132.293\n",
      "    update_time_ms: 3.241\n",
      "  timestamp: 1658496422\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 226800\n",
      "  training_iteration: 81\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    81</td><td style=\"text-align: right;\">         1502.23</td><td style=\"text-align: right;\">226800</td><td style=\"text-align: right;\">  -0.272</td><td style=\"text-align: right;\">                 0.4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            251.58</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 459200\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-27-21\n",
      "  done: false\n",
      "  episode_len_mean: 260.59\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.5000000074505806\n",
      "  episode_reward_mean: -0.2849999991804361\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 938\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.4187372717119398\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.017900793070380002\n",
      "          policy_loss: -0.09227972108294039\n",
      "          total_loss: -0.0788565357704231\n",
      "          vf_explained_var: -0.02260340377688408\n",
      "          vf_loss: 0.002864761242660409\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.540679774114064\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.015422599912897269\n",
      "          policy_loss: -0.10569257533539453\n",
      "          total_loss: -0.09032089945546995\n",
      "          vf_explained_var: 0.09395214915275574\n",
      "          vf_loss: 0.0028710341717842745\n",
      "    num_agent_steps_sampled: 459200\n",
      "    num_agent_steps_trained: 459200\n",
      "    num_steps_sampled: 229600\n",
      "    num_steps_trained: 229600\n",
      "  iterations_since_restore: 82\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.908333333333332\n",
      "    gpu_util_percent0: 0.050416666666666665\n",
      "    ram_util_percent: 54.29999999999999\n",
      "    vram_util_percent0: 0.2177978515625\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.5000000074505806\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.17\n",
      "    agent_1: -0.11499999918043613\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051827360257461194\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.895543571856831\n",
      "    mean_inference_ms: 1.7783773497675248\n",
      "    mean_raw_obs_processing_ms: 0.1583421210471316\n",
      "  time_since_restore: 1520.616815328598\n",
      "  time_this_iter_s: 18.38332748413086\n",
      "  time_total_s: 1520.616815328598\n",
      "  timers:\n",
      "    learn_throughput: 665.565\n",
      "    learn_time_ms: 4206.951\n",
      "    load_throughput: 84065.861\n",
      "    load_time_ms: 33.307\n",
      "    sample_throughput: 198.554\n",
      "    sample_time_ms: 14101.935\n",
      "    update_time_ms: 3.24\n",
      "  timestamp: 1658496441\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 229600\n",
      "  training_iteration: 82\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    82</td><td style=\"text-align: right;\">         1520.62</td><td style=\"text-align: right;\">229600</td><td style=\"text-align: right;\">  -0.285</td><td style=\"text-align: right;\">                 0.5</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            260.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 464800\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-27-39\n",
      "  done: false\n",
      "  episode_len_mean: 269.26\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.5000000074505806\n",
      "  episode_reward_mean: -0.283999999165535\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 951\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.3709550645379793\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.014295243577648623\n",
      "          policy_loss: -0.05636176428392251\n",
      "          total_loss: -0.04442662510020801\n",
      "          vf_explained_var: -0.13405822217464447\n",
      "          vf_loss: 0.006279640057549668\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.490926689335278\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011857236056308739\n",
      "          policy_loss: -0.05981723677888069\n",
      "          total_loss: -0.04685041208410569\n",
      "          vf_explained_var: -0.0801936537027359\n",
      "          vf_loss: 0.00612484557859716\n",
      "    num_agent_steps_sampled: 464800\n",
      "    num_agent_steps_trained: 464800\n",
      "    num_steps_sampled: 232400\n",
      "    num_steps_trained: 232400\n",
      "  iterations_since_restore: 83\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.816\n",
      "    gpu_util_percent0: 0.0644\n",
      "    ram_util_percent: 54.29999999999999\n",
      "    vram_util_percent0: 0.21692187500000004\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.5000000074505806\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.17\n",
      "    agent_1: -0.11399999916553498\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05181490714183124\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8950972854690313\n",
      "    mean_inference_ms: 1.777872202706572\n",
      "    mean_raw_obs_processing_ms: 0.15822919807438512\n",
      "  time_since_restore: 1538.9686434268951\n",
      "  time_this_iter_s: 18.35182809829712\n",
      "  time_total_s: 1538.9686434268951\n",
      "  timers:\n",
      "    learn_throughput: 664.459\n",
      "    learn_time_ms: 4213.957\n",
      "    load_throughput: 84014.081\n",
      "    load_time_ms: 33.328\n",
      "    sample_throughput: 198.97\n",
      "    sample_time_ms: 14072.477\n",
      "    update_time_ms: 3.217\n",
      "  timestamp: 1658496459\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 232400\n",
      "  training_iteration: 83\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    83</td><td style=\"text-align: right;\">         1538.97</td><td style=\"text-align: right;\">232400</td><td style=\"text-align: right;\">  -0.284</td><td style=\"text-align: right;\">                 0.5</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            269.26</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 470400\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-27-57\n",
      "  done: false\n",
      "  episode_len_mean: 275.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.5000000074505806\n",
      "  episode_reward_mean: -0.32399999916553496\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 961\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.400373719277836\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01296850845430031\n",
      "          policy_loss: -0.08883195994037253\n",
      "          total_loss: -0.07858115287672263\n",
      "          vf_explained_var: -0.15191949903964996\n",
      "          vf_loss: 0.004346907532938833\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.5459682501497722\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011596984190451213\n",
      "          policy_loss: -0.09967984729779086\n",
      "          total_loss: -0.08782204211179521\n",
      "          vf_explained_var: -0.15102888643741608\n",
      "          vf_loss: 0.003767240833873248\n",
      "    num_agent_steps_sampled: 470400\n",
      "    num_agent_steps_trained: 470400\n",
      "    num_steps_sampled: 235200\n",
      "    num_steps_trained: 235200\n",
      "  iterations_since_restore: 84\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.856521739130434\n",
      "    gpu_util_percent0: 0.051304347826086956\n",
      "    ram_util_percent: 54.29999999999998\n",
      "    vram_util_percent0: 0.2164104959239131\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.5000000074505806\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.19\n",
      "    agent_1: -0.13399999916553498\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051805079701257084\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.894709946256188\n",
      "    mean_inference_ms: 1.7774529184062655\n",
      "    mean_raw_obs_processing_ms: 0.1581241667070719\n",
      "  time_since_restore: 1557.1093566417694\n",
      "  time_this_iter_s: 18.140713214874268\n",
      "  time_total_s: 1557.1093566417694\n",
      "  timers:\n",
      "    learn_throughput: 663.94\n",
      "    learn_time_ms: 4217.247\n",
      "    load_throughput: 84046.368\n",
      "    load_time_ms: 33.315\n",
      "    sample_throughput: 199.333\n",
      "    sample_time_ms: 14046.853\n",
      "    update_time_ms: 3.237\n",
      "  timestamp: 1658496477\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 235200\n",
      "  training_iteration: 84\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    84</td><td style=\"text-align: right;\">         1557.11</td><td style=\"text-align: right;\">235200</td><td style=\"text-align: right;\">  -0.324</td><td style=\"text-align: right;\">                 0.5</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             275.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 476000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-28-16\n",
      "  done: false\n",
      "  episode_len_mean: 271.61\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.5000000074505806\n",
      "  episode_reward_mean: -0.42199999913573266\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 971\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.3666069273437773\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.012973463828201208\n",
      "          policy_loss: -0.054309499847780295\n",
      "          total_loss: -0.043327115200684475\n",
      "          vf_explained_var: 0.03722235560417175\n",
      "          vf_loss: 0.006384614757258706\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.524669185990379\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01161176922916692\n",
      "          policy_loss: -0.07314512551307589\n",
      "          total_loss: -0.05966877062946012\n",
      "          vf_explained_var: 0.1544433832168579\n",
      "          vf_loss: 0.008320181093863323\n",
      "    num_agent_steps_sampled: 476000\n",
      "    num_agent_steps_trained: 476000\n",
      "    num_steps_sampled: 238000\n",
      "    num_steps_trained: 238000\n",
      "  iterations_since_restore: 85\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.925\n",
      "    gpu_util_percent0: 0.05291666666666667\n",
      "    ram_util_percent: 54.35833333333333\n",
      "    vram_util_percent0: 0.21796874999999996\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.5000000074505806\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.24\n",
      "    agent_1: -0.18199999913573264\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05179511139332863\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.894340383384539\n",
      "    mean_inference_ms: 1.7770104575694574\n",
      "    mean_raw_obs_processing_ms: 0.15800809074227634\n",
      "  time_since_restore: 1575.346007347107\n",
      "  time_this_iter_s: 18.236650705337524\n",
      "  time_total_s: 1575.346007347107\n",
      "  timers:\n",
      "    learn_throughput: 664.74\n",
      "    learn_time_ms: 4212.174\n",
      "    load_throughput: 84262.251\n",
      "    load_time_ms: 33.23\n",
      "    sample_throughput: 199.413\n",
      "    sample_time_ms: 14041.225\n",
      "    update_time_ms: 3.259\n",
      "  timestamp: 1658496496\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 238000\n",
      "  training_iteration: 85\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    85</td><td style=\"text-align: right;\">         1575.35</td><td style=\"text-align: right;\">238000</td><td style=\"text-align: right;\">  -0.422</td><td style=\"text-align: right;\">                 0.5</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            271.61</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 481600\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-28-34\n",
      "  done: false\n",
      "  episode_len_mean: 259.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.5000000074505806\n",
      "  episode_reward_mean: -0.41999999910593033\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 983\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.38830565625713\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.016421557448316564\n",
      "          policy_loss: -0.06709938317986339\n",
      "          total_loss: -0.05487334206450863\n",
      "          vf_explained_var: 0.03860878944396973\n",
      "          vf_loss: 0.0025782183882776216\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.4958643863598504\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.014319820464876802\n",
      "          policy_loss: -0.10186646089589756\n",
      "          total_loss: -0.08767953216530648\n",
      "          vf_explained_var: 0.4200289249420166\n",
      "          vf_loss: 0.0025838568733542204\n",
      "    num_agent_steps_sampled: 481600\n",
      "    num_agent_steps_trained: 481600\n",
      "    num_steps_sampled: 240800\n",
      "    num_steps_trained: 240800\n",
      "  iterations_since_restore: 86\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.970833333333334\n",
      "    gpu_util_percent0: 0.05375\n",
      "    ram_util_percent: 54.387499999999996\n",
      "    vram_util_percent0: 0.21675618489583334\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.5000000074505806\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.24\n",
      "    agent_1: -0.17999999910593034\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05178386373889006\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.893865716665466\n",
      "    mean_inference_ms: 1.7765429849854755\n",
      "    mean_raw_obs_processing_ms: 0.15792516222351605\n",
      "  time_since_restore: 1593.5486238002777\n",
      "  time_this_iter_s: 18.202616453170776\n",
      "  time_total_s: 1593.5486238002777\n",
      "  timers:\n",
      "    learn_throughput: 664.954\n",
      "    learn_time_ms: 4210.819\n",
      "    load_throughput: 84155.618\n",
      "    load_time_ms: 33.272\n",
      "    sample_throughput: 199.451\n",
      "    sample_time_ms: 14038.541\n",
      "    update_time_ms: 3.246\n",
      "  timestamp: 1658496514\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 240800\n",
      "  training_iteration: 86\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    86</td><td style=\"text-align: right;\">         1593.55</td><td style=\"text-align: right;\">240800</td><td style=\"text-align: right;\">   -0.42</td><td style=\"text-align: right;\">                 0.5</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            259.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 487200\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-28-52\n",
      "  done: false\n",
      "  episode_len_mean: 263.61\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.5000000074505806\n",
      "  episode_reward_mean: -0.43999999910593035\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 993\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.43857080524876\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.013761130857177768\n",
      "          policy_loss: -0.052524983772205554\n",
      "          total_loss: -0.041909906254399436\n",
      "          vf_explained_var: 0.09046171605587006\n",
      "          vf_loss: 0.0037351061718514495\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.565912520601636\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010618742683703008\n",
      "          policy_loss: -0.06767535926850801\n",
      "          total_loss: -0.056646384303717025\n",
      "          vf_explained_var: 0.0725288838148117\n",
      "          vf_loss: 0.004217446214971417\n",
      "    num_agent_steps_sampled: 487200\n",
      "    num_agent_steps_trained: 487200\n",
      "    num_steps_sampled: 243600\n",
      "    num_steps_trained: 243600\n",
      "  iterations_since_restore: 87\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.864\n",
      "    gpu_util_percent0: 0.06319999999999999\n",
      "    ram_util_percent: 54.58\n",
      "    vram_util_percent0: 0.21748437499999998\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.5000000074505806\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.25\n",
      "    agent_1: -0.18999999910593032\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05177507280557764\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.893483900114976\n",
      "    mean_inference_ms: 1.7761701856359482\n",
      "    mean_raw_obs_processing_ms: 0.15786488981868327\n",
      "  time_since_restore: 1611.8852529525757\n",
      "  time_this_iter_s: 18.336629152297974\n",
      "  time_total_s: 1611.8852529525757\n",
      "  timers:\n",
      "    learn_throughput: 664.377\n",
      "    learn_time_ms: 4214.474\n",
      "    load_throughput: 89596.992\n",
      "    load_time_ms: 31.251\n",
      "    sample_throughput: 199.387\n",
      "    sample_time_ms: 14043.065\n",
      "    update_time_ms: 3.258\n",
      "  timestamp: 1658496532\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 243600\n",
      "  training_iteration: 87\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    87</td><td style=\"text-align: right;\">         1611.89</td><td style=\"text-align: right;\">243600</td><td style=\"text-align: right;\">   -0.44</td><td style=\"text-align: right;\">                 0.5</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            263.61</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 492800\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-29-11\n",
      "  done: false\n",
      "  episode_len_mean: 258.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.5000000074505806\n",
      "  episode_reward_mean: -0.3999999991059303\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1004\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.440415444828215\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.018456901407037464\n",
      "          policy_loss: -0.11248582218481065\n",
      "          total_loss: -0.0992134332853831\n",
      "          vf_explained_var: -0.004307758063077927\n",
      "          vf_loss: 0.001269392982760023\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.5226328003974188\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.015392631417377208\n",
      "          policy_loss: -0.12368183751615495\n",
      "          total_loss: -0.10868866725442265\n",
      "          vf_explained_var: 0.08387628197669983\n",
      "          vf_loss: 0.0018546672959235452\n",
      "    num_agent_steps_sampled: 492800\n",
      "    num_agent_steps_trained: 492800\n",
      "    num_steps_sampled: 246400\n",
      "    num_steps_trained: 246400\n",
      "  iterations_since_restore: 88\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.904166666666666\n",
      "    gpu_util_percent0: 0.04958333333333333\n",
      "    ram_util_percent: 54.44166666666667\n",
      "    vram_util_percent0: 0.21694742838541667\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.5000000074505806\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.23\n",
      "    agent_1: -0.16999999910593033\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05176535670916703\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.893076229053138\n",
      "    mean_inference_ms: 1.775740727889582\n",
      "    mean_raw_obs_processing_ms: 0.15780371626527195\n",
      "  time_since_restore: 1630.4787003993988\n",
      "  time_this_iter_s: 18.59344744682312\n",
      "  time_total_s: 1630.4787003993988\n",
      "  timers:\n",
      "    learn_throughput: 662.138\n",
      "    learn_time_ms: 4228.727\n",
      "    load_throughput: 89771.911\n",
      "    load_time_ms: 31.19\n",
      "    sample_throughput: 199.287\n",
      "    sample_time_ms: 14050.063\n",
      "    update_time_ms: 3.277\n",
      "  timestamp: 1658496551\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 246400\n",
      "  training_iteration: 88\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    88</td><td style=\"text-align: right;\">         1630.48</td><td style=\"text-align: right;\">246400</td><td style=\"text-align: right;\">    -0.4</td><td style=\"text-align: right;\">                 0.5</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            258.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 498400\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-29-29\n",
      "  done: false\n",
      "  episode_len_mean: 250.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.5000000074505806\n",
      "  episode_reward_mean: -0.37699999906122683\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 1019\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.4516920965342295\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010980525316526268\n",
      "          policy_loss: -0.05510525569219941\n",
      "          total_loss: -0.0469829797683425\n",
      "          vf_explained_var: 0.036186352372169495\n",
      "          vf_loss: 0.002587842729792014\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.4889406256732487\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.00957044410885487\n",
      "          policy_loss: -0.04917705231179882\n",
      "          total_loss: -0.039706589006874266\n",
      "          vf_explained_var: 0.28659138083457947\n",
      "          vf_loss: 0.002669576566057956\n",
      "    num_agent_steps_sampled: 498400\n",
      "    num_agent_steps_trained: 498400\n",
      "    num_steps_sampled: 249200\n",
      "    num_steps_trained: 249200\n",
      "  iterations_since_restore: 89\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.8125\n",
      "    gpu_util_percent0: 0.04958333333333333\n",
      "    ram_util_percent: 54.35\n",
      "    vram_util_percent0: 0.21652832031250002\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.5000000074505806\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.22\n",
      "    agent_1: -0.15699999906122686\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05175181454134268\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8924656434208873\n",
      "    mean_inference_ms: 1.7751648547589267\n",
      "    mean_raw_obs_processing_ms: 0.15776815265950297\n",
      "  time_since_restore: 1648.9362094402313\n",
      "  time_this_iter_s: 18.45750904083252\n",
      "  time_total_s: 1648.9362094402313\n",
      "  timers:\n",
      "    learn_throughput: 660.787\n",
      "    learn_time_ms: 4237.375\n",
      "    load_throughput: 89509.447\n",
      "    load_time_ms: 31.282\n",
      "    sample_throughput: 199.5\n",
      "    sample_time_ms: 14035.089\n",
      "    update_time_ms: 3.282\n",
      "  timestamp: 1658496569\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 249200\n",
      "  training_iteration: 89\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    89</td><td style=\"text-align: right;\">         1648.94</td><td style=\"text-align: right;\">249200</td><td style=\"text-align: right;\">  -0.377</td><td style=\"text-align: right;\">                 0.5</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            250.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 504000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-29-48\n",
      "  done: false\n",
      "  episode_len_mean: 259.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.5000000074505806\n",
      "  episode_reward_mean: -0.2909999989718199\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 1028\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.413637176865623\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.019097257967898396\n",
      "          policy_loss: -0.08882295695738798\n",
      "          total_loss: -0.07523678620124147\n",
      "          vf_explained_var: 0.24817952513694763\n",
      "          vf_loss: 0.0007623421438655073\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.526576775880087\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.017048463227101636\n",
      "          policy_loss: -0.10977044265988904\n",
      "          total_loss: -0.09342151798634968\n",
      "          vf_explained_var: 0.16143395006656647\n",
      "          vf_loss: 0.001002719242500288\n",
      "    num_agent_steps_sampled: 504000\n",
      "    num_agent_steps_trained: 504000\n",
      "    num_steps_sampled: 252000\n",
      "    num_steps_trained: 252000\n",
      "  iterations_since_restore: 90\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.841666666666668\n",
      "    gpu_util_percent0: 0.06083333333333333\n",
      "    ram_util_percent: 54.34166666666667\n",
      "    vram_util_percent0: 0.21753743489583333\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.5000000074505806\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.18\n",
      "    agent_1: -0.11099999897181988\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05174347620872322\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.892126444260258\n",
      "    mean_inference_ms: 1.7748253505900846\n",
      "    mean_raw_obs_processing_ms: 0.1577217806500535\n",
      "  time_since_restore: 1667.2216970920563\n",
      "  time_this_iter_s: 18.28548765182495\n",
      "  time_total_s: 1667.2216970920563\n",
      "  timers:\n",
      "    learn_throughput: 661.766\n",
      "    learn_time_ms: 4231.1\n",
      "    load_throughput: 89401.447\n",
      "    load_time_ms: 31.319\n",
      "    sample_throughput: 199.606\n",
      "    sample_time_ms: 14027.606\n",
      "    update_time_ms: 3.265\n",
      "  timestamp: 1658496588\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 252000\n",
      "  training_iteration: 90\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    90</td><td style=\"text-align: right;\">         1667.22</td><td style=\"text-align: right;\">252000</td><td style=\"text-align: right;\">  -0.291</td><td style=\"text-align: right;\">                 0.5</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            259.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 509600\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-30-07\n",
      "  done: false\n",
      "  episode_len_mean: 240.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.4000000059604645\n",
      "  episode_reward_mean: -0.21399999901652336\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 1042\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.4409832642191933\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.008695599425198824\n",
      "          policy_loss: -0.04999005135392681\n",
      "          total_loss: -0.04351525070039012\n",
      "          vf_explained_var: -0.08752164244651794\n",
      "          vf_loss: 0.002764171648517991\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.4415898592699143\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.008796635425851027\n",
      "          policy_loss: -0.0590585527980433\n",
      "          total_loss: -0.050080577921887325\n",
      "          vf_explained_var: -0.04414443299174309\n",
      "          vf_loss: 0.00341643407626266\n",
      "    num_agent_steps_sampled: 509600\n",
      "    num_agent_steps_trained: 509600\n",
      "    num_steps_sampled: 254800\n",
      "    num_steps_trained: 254800\n",
      "  iterations_since_restore: 91\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 9.112\n",
      "    gpu_util_percent0: 0.0548\n",
      "    ram_util_percent: 54.492000000000004\n",
      "    vram_util_percent0: 0.21686328124999998\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.4000000059604645\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.14\n",
      "    agent_1: -0.07399999901652336\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051732770637610184\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.891599214841724\n",
      "    mean_inference_ms: 1.7744053133544377\n",
      "    mean_raw_obs_processing_ms: 0.15771635702964587\n",
      "  time_since_restore: 1686.1036412715912\n",
      "  time_this_iter_s: 18.881944179534912\n",
      "  time_total_s: 1686.1036412715912\n",
      "  timers:\n",
      "    learn_throughput: 658.742\n",
      "    learn_time_ms: 4250.524\n",
      "    load_throughput: 96184.033\n",
      "    load_time_ms: 29.111\n",
      "    sample_throughput: 199.292\n",
      "    sample_time_ms: 14049.724\n",
      "    update_time_ms: 3.267\n",
      "  timestamp: 1658496607\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 254800\n",
      "  training_iteration: 91\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    91</td><td style=\"text-align: right;\">          1686.1</td><td style=\"text-align: right;\">254800</td><td style=\"text-align: right;\">  -0.214</td><td style=\"text-align: right;\">                 0.4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            240.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 515200\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-30-25\n",
      "  done: false\n",
      "  episode_len_mean: 238.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.6000000089406967\n",
      "  episode_reward_mean: -0.2629999988526106\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 1054\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.397060507819766\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010145901678504841\n",
      "          policy_loss: -0.05603697978048807\n",
      "          total_loss: -0.04756497887761465\n",
      "          vf_explained_var: -0.20655745267868042\n",
      "          vf_loss: 0.005308261115022192\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.4910644321214583\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.008699264990052427\n",
      "          policy_loss: -0.059803916074867756\n",
      "          total_loss: -0.05015125573014042\n",
      "          vf_explained_var: -0.0668945387005806\n",
      "          vf_loss: 0.005679138552035771\n",
      "    num_agent_steps_sampled: 515200\n",
      "    num_agent_steps_trained: 515200\n",
      "    num_steps_sampled: 257600\n",
      "    num_steps_trained: 257600\n",
      "  iterations_since_restore: 92\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.132\n",
      "    gpu_util_percent0: 0.0544\n",
      "    ram_util_percent: 54.4\n",
      "    vram_util_percent0: 0.21746874999999996\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.6000000089406967\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.17\n",
      "    agent_1: -0.0929999988526106\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05172532952396405\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8912299954385854\n",
      "    mean_inference_ms: 1.7741751444928335\n",
      "    mean_raw_obs_processing_ms: 0.15772212967483404\n",
      "  time_since_restore: 1704.8489582538605\n",
      "  time_this_iter_s: 18.745316982269287\n",
      "  time_total_s: 1704.8489582538605\n",
      "  timers:\n",
      "    learn_throughput: 656.825\n",
      "    learn_time_ms: 4262.933\n",
      "    load_throughput: 95968.83\n",
      "    load_time_ms: 29.176\n",
      "    sample_throughput: 198.957\n",
      "    sample_time_ms: 14073.363\n",
      "    update_time_ms: 3.261\n",
      "  timestamp: 1658496625\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 257600\n",
      "  training_iteration: 92\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    92</td><td style=\"text-align: right;\">         1704.85</td><td style=\"text-align: right;\">257600</td><td style=\"text-align: right;\">  -0.263</td><td style=\"text-align: right;\">                 0.6</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            238.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 520800\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-30-44\n",
      "  done: false\n",
      "  episode_len_mean: 239.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.6000000089406967\n",
      "  episode_reward_mean: -0.2989999987930059\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1064\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.3994367005569592\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.013091043840835235\n",
      "          policy_loss: -0.054182543562048865\n",
      "          total_loss: -0.04320583767236842\n",
      "          vf_explained_var: -0.22543932497501373\n",
      "          vf_loss: 0.006155628535062923\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.4380408135198413\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010826208949297242\n",
      "          policy_loss: -0.04895924526880706\n",
      "          total_loss: -0.036644344917953105\n",
      "          vf_explained_var: -0.1207282543182373\n",
      "          vf_loss: 0.0071442735703558784\n",
      "    num_agent_steps_sampled: 520800\n",
      "    num_agent_steps_trained: 520800\n",
      "    num_steps_sampled: 260400\n",
      "    num_steps_trained: 260400\n",
      "  iterations_since_restore: 93\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.916666666666667\n",
      "    gpu_util_percent0: 0.049166666666666664\n",
      "    ram_util_percent: 54.3125\n",
      "    vram_util_percent0: 0.2169840494791667\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.6000000089406967\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.19\n",
      "    agent_1: -0.10899999879300594\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051720255302673414\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8909585306872985\n",
      "    mean_inference_ms: 1.7740585764881345\n",
      "    mean_raw_obs_processing_ms: 0.15773227473057355\n",
      "  time_since_restore: 1723.2660467624664\n",
      "  time_this_iter_s: 18.417088508605957\n",
      "  time_total_s: 1723.2660467624664\n",
      "  timers:\n",
      "    learn_throughput: 657.843\n",
      "    learn_time_ms: 4256.335\n",
      "    load_throughput: 96005.389\n",
      "    load_time_ms: 29.165\n",
      "    sample_throughput: 198.731\n",
      "    sample_time_ms: 14089.409\n",
      "    update_time_ms: 3.282\n",
      "  timestamp: 1658496644\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 260400\n",
      "  training_iteration: 93\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    93</td><td style=\"text-align: right;\">         1723.27</td><td style=\"text-align: right;\">260400</td><td style=\"text-align: right;\">  -0.299</td><td style=\"text-align: right;\">                 0.6</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            239.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 526400\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-31-02\n",
      "  done: false\n",
      "  episode_len_mean: 245.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.6000000089406967\n",
      "  episode_reward_mean: -0.25299999870359896\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1074\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.4142573369400844\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.015174661047348447\n",
      "          policy_loss: -0.07456277396496651\n",
      "          total_loss: -0.0628154912930248\n",
      "          vf_explained_var: -0.06691458076238632\n",
      "          vf_loss: 0.003911946203732528\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.471823750507264\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011832555897063874\n",
      "          policy_loss: -0.0629456635291544\n",
      "          total_loss: -0.050255271040008494\n",
      "          vf_explained_var: -0.09558096528053284\n",
      "          vf_loss: 0.005383499821835444\n",
      "    num_agent_steps_sampled: 526400\n",
      "    num_agent_steps_trained: 526400\n",
      "    num_steps_sampled: 263200\n",
      "    num_steps_trained: 263200\n",
      "  iterations_since_restore: 94\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.8374999999999995\n",
      "    gpu_util_percent0: 0.05499999999999999\n",
      "    ram_util_percent: 54.32916666666667\n",
      "    vram_util_percent0: 0.21652832031250002\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.6000000089406967\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.17\n",
      "    agent_1: -0.08299999870359898\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051716230171015065\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.890702166780613\n",
      "    mean_inference_ms: 1.773982871826181\n",
      "    mean_raw_obs_processing_ms: 0.1577423691789033\n",
      "  time_since_restore: 1741.63294839859\n",
      "  time_this_iter_s: 18.366901636123657\n",
      "  time_total_s: 1741.63294839859\n",
      "  timers:\n",
      "    learn_throughput: 657.942\n",
      "    learn_time_ms: 4255.693\n",
      "    load_throughput: 96137.972\n",
      "    load_time_ms: 29.125\n",
      "    sample_throughput: 198.401\n",
      "    sample_time_ms: 14112.801\n",
      "    update_time_ms: 3.283\n",
      "  timestamp: 1658496662\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 263200\n",
      "  training_iteration: 94\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    94</td><td style=\"text-align: right;\">         1741.63</td><td style=\"text-align: right;\">263200</td><td style=\"text-align: right;\">  -0.253</td><td style=\"text-align: right;\">                 0.6</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             245.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 532000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-31-21\n",
      "  done: false\n",
      "  episode_len_mean: 247.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.6000000089406967\n",
      "  episode_reward_mean: -0.25099999867379663\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 1083\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.4006828566392264\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01664755218201716\n",
      "          policy_loss: -0.08534731379473842\n",
      "          total_loss: -0.07288621158390708\n",
      "          vf_explained_var: -0.04537387192249298\n",
      "          vf_loss: 0.002780078616957499\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.4814680885700953\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.014182430802519722\n",
      "          policy_loss: -0.08472782254431249\n",
      "          total_loss: -0.0705554067667219\n",
      "          vf_explained_var: 0.11323170363903046\n",
      "          vf_loss: 0.0029175267531432575\n",
      "    num_agent_steps_sampled: 532000\n",
      "    num_agent_steps_trained: 532000\n",
      "    num_steps_sampled: 266000\n",
      "    num_steps_trained: 266000\n",
      "  iterations_since_restore: 95\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.867999999999999\n",
      "    gpu_util_percent0: 0.0572\n",
      "    ram_util_percent: 54.29999999999999\n",
      "    vram_util_percent0: 0.21775781249999998\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.6000000089406967\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.17\n",
      "    agent_1: -0.08099999867379665\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05171335072941897\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8905216000935026\n",
      "    mean_inference_ms: 1.7739469875675604\n",
      "    mean_raw_obs_processing_ms: 0.15773597769249875\n",
      "  time_since_restore: 1760.2977604866028\n",
      "  time_this_iter_s: 18.664812088012695\n",
      "  time_total_s: 1760.2977604866028\n",
      "  timers:\n",
      "    learn_throughput: 655.994\n",
      "    learn_time_ms: 4268.332\n",
      "    load_throughput: 95901.12\n",
      "    load_time_ms: 29.197\n",
      "    sample_throughput: 197.978\n",
      "    sample_time_ms: 14143.016\n",
      "    update_time_ms: 3.27\n",
      "  timestamp: 1658496681\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 266000\n",
      "  training_iteration: 95\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    95</td><td style=\"text-align: right;\">          1760.3</td><td style=\"text-align: right;\">266000</td><td style=\"text-align: right;\">  -0.251</td><td style=\"text-align: right;\">                 0.6</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             247.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 537600\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-31-40\n",
      "  done: false\n",
      "  episode_len_mean: 242.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.6000000089406967\n",
      "  episode_reward_mean: -0.22599999859929085\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 1098\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.4032131958575476\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01656944568443057\n",
      "          policy_loss: -0.103751641273924\n",
      "          total_loss: -0.09153192049935958\n",
      "          vf_explained_var: 0.13379248976707458\n",
      "          vf_loss: 0.002261253303112469\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.4784988370679675\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.013533992464400383\n",
      "          policy_loss: -0.10542735857944492\n",
      "          total_loss: -0.09201560037798204\n",
      "          vf_explained_var: 0.11494709551334381\n",
      "          vf_loss: 0.002593650280210741\n",
      "    num_agent_steps_sampled: 537600\n",
      "    num_agent_steps_trained: 537600\n",
      "    num_steps_sampled: 268800\n",
      "    num_steps_trained: 268800\n",
      "  iterations_since_restore: 96\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.9719999999999995\n",
      "    gpu_util_percent0: 0.0572\n",
      "    ram_util_percent: 54.355999999999995\n",
      "    vram_util_percent0: 0.2165234375\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.6000000089406967\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.16\n",
      "    agent_1: -0.06599999859929084\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05171154458921648\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8903316598615105\n",
      "    mean_inference_ms: 1.7740067339862504\n",
      "    mean_raw_obs_processing_ms: 0.15778365736175767\n",
      "  time_since_restore: 1779.1041836738586\n",
      "  time_this_iter_s: 18.80642318725586\n",
      "  time_total_s: 1779.1041836738586\n",
      "  timers:\n",
      "    learn_throughput: 654.949\n",
      "    learn_time_ms: 4275.14\n",
      "    load_throughput: 96245.517\n",
      "    load_time_ms: 29.092\n",
      "    sample_throughput: 197.233\n",
      "    sample_time_ms: 14196.409\n",
      "    update_time_ms: 3.287\n",
      "  timestamp: 1658496700\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 268800\n",
      "  training_iteration: 96\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    96</td><td style=\"text-align: right;\">          1779.1</td><td style=\"text-align: right;\">268800</td><td style=\"text-align: right;\">  -0.226</td><td style=\"text-align: right;\">                 0.6</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            242.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 543200\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-31-58\n",
      "  done: false\n",
      "  episode_len_mean: 237.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.800000011920929\n",
      "  episode_reward_mean: -0.23399999842047692\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1109\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.3794534280896187\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.014069331941299298\n",
      "          policy_loss: -0.08046793467153857\n",
      "          total_loss: -0.06991159680616138\n",
      "          vf_explained_var: -0.145375058054924\n",
      "          vf_loss: 0.0028373856315565284\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.4616535831065405\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.012718203830445534\n",
      "          policy_loss: -0.0757764919525514\n",
      "          total_loss: -0.06313589844891491\n",
      "          vf_explained_var: 0.27216997742652893\n",
      "          vf_loss: 0.002701038269151468\n",
      "    num_agent_steps_sampled: 543200\n",
      "    num_agent_steps_trained: 543200\n",
      "    num_steps_sampled: 271600\n",
      "    num_steps_trained: 271600\n",
      "  iterations_since_restore: 97\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.820833333333334\n",
      "    gpu_util_percent0: 0.050416666666666665\n",
      "    ram_util_percent: 54.29999999999999\n",
      "    vram_util_percent0: 0.2173502604166667\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.4000000059604645\n",
      "    agent_1: 0.6000000089406967\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.16599999994039535\n",
      "    agent_1: -0.06799999848008156\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05171067645475216\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.890228884929265\n",
      "    mean_inference_ms: 1.774069770609447\n",
      "    mean_raw_obs_processing_ms: 0.15780471499987714\n",
      "  time_since_restore: 1797.6560850143433\n",
      "  time_this_iter_s: 18.55190134048462\n",
      "  time_total_s: 1797.6560850143433\n",
      "  timers:\n",
      "    learn_throughput: 654.823\n",
      "    learn_time_ms: 4275.964\n",
      "    load_throughput: 89589.064\n",
      "    load_time_ms: 31.254\n",
      "    sample_throughput: 196.938\n",
      "    sample_time_ms: 14217.702\n",
      "    update_time_ms: 3.277\n",
      "  timestamp: 1658496718\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 271600\n",
      "  training_iteration: 97\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    97</td><td style=\"text-align: right;\">         1797.66</td><td style=\"text-align: right;\">271600</td><td style=\"text-align: right;\">  -0.234</td><td style=\"text-align: right;\">                 0.8</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            237.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 548800\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-32-17\n",
      "  done: false\n",
      "  episode_len_mean: 237.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.9999999701976776\n",
      "  episode_reward_mean: -0.2559999987483025\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 1124\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.3869558565673374\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.00984765877435122\n",
      "          policy_loss: -0.040513687124378826\n",
      "          total_loss: -0.028971072579527874\n",
      "          vf_explained_var: -0.121971994638443\n",
      "          vf_loss: 0.014700382123399842\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.443803694276583\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.00903076690350638\n",
      "          policy_loss: -0.04871650367775666\n",
      "          total_loss: -0.036950876930772904\n",
      "          vf_explained_var: -0.17024452984333038\n",
      "          vf_loss: 0.010708518197610655\n",
      "    num_agent_steps_sampled: 548800\n",
      "    num_agent_steps_trained: 548800\n",
      "    num_steps_sampled: 274400\n",
      "    num_steps_trained: 274400\n",
      "  iterations_since_restore: 98\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.888\n",
      "    gpu_util_percent0: 0.0572\n",
      "    ram_util_percent: 54.38399999999999\n",
      "    vram_util_percent0: 0.21771875\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 1.9999999701976776\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.17599999994039536\n",
      "    agent_1: -0.0799999988079071\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05171070778711783\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8900892704484784\n",
      "    mean_inference_ms: 1.774178206524299\n",
      "    mean_raw_obs_processing_ms: 0.1578582062544937\n",
      "  time_since_restore: 1816.2912576198578\n",
      "  time_this_iter_s: 18.635172605514526\n",
      "  time_total_s: 1816.2912576198578\n",
      "  timers:\n",
      "    learn_throughput: 655.146\n",
      "    learn_time_ms: 4273.859\n",
      "    load_throughput: 89673.067\n",
      "    load_time_ms: 31.225\n",
      "    sample_throughput: 196.849\n",
      "    sample_time_ms: 14224.077\n",
      "    update_time_ms: 3.248\n",
      "  timestamp: 1658496737\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 274400\n",
      "  training_iteration: 98\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    98</td><td style=\"text-align: right;\">         1816.29</td><td style=\"text-align: right;\">274400</td><td style=\"text-align: right;\">  -0.256</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            237.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 554400\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-32-36\n",
      "  done: false\n",
      "  episode_len_mean: 241.54\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.9999999701976776\n",
      "  episode_reward_mean: -0.20199999913573266\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1134\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.416949279251553\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011958260850739007\n",
      "          policy_loss: -0.07145038468055039\n",
      "          total_loss: -0.06097702257933894\n",
      "          vf_explained_var: -0.14173582196235657\n",
      "          vf_loss: 0.007164836836073803\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.5196446563516344\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010016086093595283\n",
      "          policy_loss: -0.035078471489604066\n",
      "          total_loss: -0.024484179564751685\n",
      "          vf_explained_var: -0.08271647989749908\n",
      "          vf_loss: 0.004642027778594465\n",
      "    num_agent_steps_sampled: 554400\n",
      "    num_agent_steps_trained: 554400\n",
      "    num_steps_sampled: 277200\n",
      "    num_steps_trained: 277200\n",
      "  iterations_since_restore: 99\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.879166666666667\n",
      "    gpu_util_percent0: 0.050416666666666665\n",
      "    ram_util_percent: 54.4\n",
      "    vram_util_percent0: 0.21656901041666665\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 1.9999999701976776\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.14599999994039536\n",
      "    agent_1: -0.0559999991953373\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05171102703363086\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.890067626994817\n",
      "    mean_inference_ms: 1.7742564984144704\n",
      "    mean_raw_obs_processing_ms: 0.15788387416125385\n",
      "  time_since_restore: 1834.9726724624634\n",
      "  time_this_iter_s: 18.68141484260559\n",
      "  time_total_s: 1834.9726724624634\n",
      "  timers:\n",
      "    learn_throughput: 656.587\n",
      "    learn_time_ms: 4264.476\n",
      "    load_throughput: 89861.002\n",
      "    load_time_ms: 31.159\n",
      "    sample_throughput: 196.409\n",
      "    sample_time_ms: 14255.994\n",
      "    update_time_ms: 3.256\n",
      "  timestamp: 1658496756\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 277200\n",
      "  training_iteration: 99\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         1834.97</td><td style=\"text-align: right;\">277200</td><td style=\"text-align: right;\">  -0.202</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            241.54</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 560000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-32-54\n",
      "  done: false\n",
      "  episode_len_mean: 240.09\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.9999999701976776\n",
      "  episode_reward_mean: -0.19899999901652335\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 1147\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.408566645923115\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01557041725553542\n",
      "          policy_loss: -0.08425100455923755\n",
      "          total_loss: -0.0713337610762946\n",
      "          vf_explained_var: -0.11253510415554047\n",
      "          vf_loss: 0.006397725253745669\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.487467089579219\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01243551333483796\n",
      "          policy_loss: -0.06361192724380955\n",
      "          total_loss: -0.04993778281728044\n",
      "          vf_explained_var: 0.04166017845273018\n",
      "          vf_loss: 0.006489107889744143\n",
      "    num_agent_steps_sampled: 560000\n",
      "    num_agent_steps_trained: 560000\n",
      "    num_steps_sampled: 280000\n",
      "    num_steps_trained: 280000\n",
      "  iterations_since_restore: 100\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.836\n",
      "    gpu_util_percent0: 0.0552\n",
      "    ram_util_percent: 54.38399999999999\n",
      "    vram_util_percent0: 0.21748437499999998\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 1.9999999701976776\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.14599999994039536\n",
      "    agent_1: -0.052999999076128006\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051711425824908196\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.89004708023483\n",
      "    mean_inference_ms: 1.7743384246924663\n",
      "    mean_raw_obs_processing_ms: 0.15791645955638953\n",
      "  time_since_restore: 1853.7482378482819\n",
      "  time_this_iter_s: 18.77556538581848\n",
      "  time_total_s: 1853.7482378482819\n",
      "  timers:\n",
      "    learn_throughput: 653.448\n",
      "    learn_time_ms: 4284.962\n",
      "    load_throughput: 89926.99\n",
      "    load_time_ms: 31.136\n",
      "    sample_throughput: 196.02\n",
      "    sample_time_ms: 14284.221\n",
      "    update_time_ms: 3.287\n",
      "  timestamp: 1658496774\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 280000\n",
      "  training_iteration: 100\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         1853.75</td><td style=\"text-align: right;\">280000</td><td style=\"text-align: right;\">  -0.199</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            240.09</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 565600\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-33-13\n",
      "  done: false\n",
      "  episode_len_mean: 243.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.9999999701976776\n",
      "  episode_reward_mean: -0.17699999898672103\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1158\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.431636427130018\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.014393258596329889\n",
      "          policy_loss: -0.0767556075392609\n",
      "          total_loss: -0.0648414724315184\n",
      "          vf_explained_var: 0.2085898220539093\n",
      "          vf_loss: 0.006081870502384845\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.475234206943285\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010786390194066919\n",
      "          policy_loss: -0.07421633078064631\n",
      "          total_loss: -0.06225430526354189\n",
      "          vf_explained_var: 0.17809626460075378\n",
      "          vf_loss: 0.0062947571009100926\n",
      "    num_agent_steps_sampled: 565600\n",
      "    num_agent_steps_trained: 565600\n",
      "    num_steps_sampled: 282800\n",
      "    num_steps_trained: 282800\n",
      "  iterations_since_restore: 101\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.888\n",
      "    gpu_util_percent0: 0.0572\n",
      "    ram_util_percent: 54.37999999999999\n",
      "    vram_util_percent0: 0.2168515625\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 1.9999999701976776\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.13599999994039536\n",
      "    agent_1: -0.04099999904632568\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05171210035747028\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8900431261490964\n",
      "    mean_inference_ms: 1.7743778066338463\n",
      "    mean_raw_obs_processing_ms: 0.1579413298530102\n",
      "  time_since_restore: 1872.4014694690704\n",
      "  time_this_iter_s: 18.653231620788574\n",
      "  time_total_s: 1872.4014694690704\n",
      "  timers:\n",
      "    learn_throughput: 655.626\n",
      "    learn_time_ms: 4270.73\n",
      "    load_throughput: 90061.259\n",
      "    load_time_ms: 31.09\n",
      "    sample_throughput: 196.18\n",
      "    sample_time_ms: 14272.639\n",
      "    update_time_ms: 3.324\n",
      "  timestamp: 1658496793\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 282800\n",
      "  training_iteration: 101\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   101</td><td style=\"text-align: right;\">          1872.4</td><td style=\"text-align: right;\">282800</td><td style=\"text-align: right;\">  -0.177</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            243.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 571200\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-33-32\n",
      "  done: false\n",
      "  episode_len_mean: 229.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.9999999701976776\n",
      "  episode_reward_mean: -0.23699999898672103\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 1172\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.4114895399127687\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.014104067342379059\n",
      "          policy_loss: -0.05657313537986262\n",
      "          total_loss: -0.04352584458787793\n",
      "          vf_explained_var: 0.220147043466568\n",
      "          vf_loss: 0.009911892381881313\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.4967809326591945\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011842274458529674\n",
      "          policy_loss: -0.06267284473488828\n",
      "          total_loss: -0.04832393082740184\n",
      "          vf_explained_var: 0.1900937259197235\n",
      "          vf_loss: 0.010119920134197205\n",
      "    num_agent_steps_sampled: 571200\n",
      "    num_agent_steps_trained: 571200\n",
      "    num_steps_sampled: 285600\n",
      "    num_steps_trained: 285600\n",
      "  iterations_since_restore: 102\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.879166666666667\n",
      "    gpu_util_percent0: 0.04791666666666666\n",
      "    ram_util_percent: 54.4125\n",
      "    vram_util_percent0: 0.21734619140625\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 1.9999999701976776\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.16599999994039535\n",
      "    agent_1: -0.07099999904632569\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05171418378024058\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.89011995417072\n",
      "    mean_inference_ms: 1.7744976275475994\n",
      "    mean_raw_obs_processing_ms: 0.15801271015557533\n",
      "  time_since_restore: 1891.3136193752289\n",
      "  time_this_iter_s: 18.912149906158447\n",
      "  time_total_s: 1891.3136193752289\n",
      "  timers:\n",
      "    learn_throughput: 654.297\n",
      "    learn_time_ms: 4279.404\n",
      "    load_throughput: 90162.068\n",
      "    load_time_ms: 31.055\n",
      "    sample_throughput: 196.069\n",
      "    sample_time_ms: 14280.706\n",
      "    update_time_ms: 3.329\n",
      "  timestamp: 1658496812\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 285600\n",
      "  training_iteration: 102\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   102</td><td style=\"text-align: right;\">         1891.31</td><td style=\"text-align: right;\">285600</td><td style=\"text-align: right;\">  -0.237</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            229.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 576800\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-33-51\n",
      "  done: false\n",
      "  episode_len_mean: 215.2\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.9999999701976776\n",
      "  episode_reward_mean: -0.2479999988526106\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 1186\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.4144735655614307\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.014238120251055239\n",
      "          policy_loss: -0.062075453046764734\n",
      "          total_loss: -0.05040285721701193\n",
      "          vf_explained_var: 0.30338066816329956\n",
      "          vf_loss: 0.005704132580874665\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.4703430455355417\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.0128513602530659\n",
      "          policy_loss: -0.07072708819897516\n",
      "          total_loss: -0.056492800197225905\n",
      "          vf_explained_var: 0.3598460257053375\n",
      "          vf_loss: 0.006880707396270563\n",
      "    num_agent_steps_sampled: 576800\n",
      "    num_agent_steps_trained: 576800\n",
      "    num_steps_sampled: 288400\n",
      "    num_steps_trained: 288400\n",
      "  iterations_since_restore: 103\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.976\n",
      "    gpu_util_percent0: 0.05159999999999999\n",
      "    ram_util_percent: 54.56799999999999\n",
      "    vram_util_percent0: 0.21748437499999998\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 1.9999999701976776\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.17599999994039536\n",
      "    agent_1: -0.07199999891221523\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05171637619945115\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.890238298300109\n",
      "    mean_inference_ms: 1.77464974661881\n",
      "    mean_raw_obs_processing_ms: 0.15812475117068517\n",
      "  time_since_restore: 1910.241581439972\n",
      "  time_this_iter_s: 18.927962064743042\n",
      "  time_total_s: 1910.241581439972\n",
      "  timers:\n",
      "    learn_throughput: 652.786\n",
      "    learn_time_ms: 4289.306\n",
      "    load_throughput: 89426.363\n",
      "    load_time_ms: 31.311\n",
      "    sample_throughput: 195.511\n",
      "    sample_time_ms: 14321.415\n",
      "    update_time_ms: 3.328\n",
      "  timestamp: 1658496831\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 288400\n",
      "  training_iteration: 103\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   103</td><td style=\"text-align: right;\">         1910.24</td><td style=\"text-align: right;\">288400</td><td style=\"text-align: right;\">  -0.248</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             215.2</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 582400\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-34-10\n",
      "  done: false\n",
      "  episode_len_mean: 225.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.9999999701976776\n",
      "  episode_reward_mean: -0.24399999879300593\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 1195\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.3470461602721895\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01848549969732017\n",
      "          policy_loss: -0.0667352677565716\n",
      "          total_loss: -0.052660902569186874\n",
      "          vf_explained_var: 0.49504417181015015\n",
      "          vf_loss: 0.0033866864936758895\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.4453872328712825\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.014547069569613111\n",
      "          policy_loss: -0.09283214087987463\n",
      "          total_loss: -0.07777375603334458\n",
      "          vf_explained_var: 0.2849307060241699\n",
      "          vf_loss: 0.00436291781772438\n",
      "    num_agent_steps_sampled: 582400\n",
      "    num_agent_steps_trained: 582400\n",
      "    num_steps_sampled: 291200\n",
      "    num_steps_trained: 291200\n",
      "  iterations_since_restore: 104\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.94\n",
      "    gpu_util_percent0: 0.0604\n",
      "    ram_util_percent: 54.54\n",
      "    vram_util_percent0: 0.21698046875\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 1.9999999701976776\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.17599999994039536\n",
      "    agent_1: -0.06799999885261059\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05171717163578743\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8902975182431647\n",
      "    mean_inference_ms: 1.7747306720564688\n",
      "    mean_raw_obs_processing_ms: 0.1581666592912014\n",
      "  time_since_restore: 1928.8268582820892\n",
      "  time_this_iter_s: 18.58527684211731\n",
      "  time_total_s: 1928.8268582820892\n",
      "  timers:\n",
      "    learn_throughput: 651.105\n",
      "    learn_time_ms: 4300.38\n",
      "    load_throughput: 89283.796\n",
      "    load_time_ms: 31.361\n",
      "    sample_throughput: 195.368\n",
      "    sample_time_ms: 14331.933\n",
      "    update_time_ms: 3.31\n",
      "  timestamp: 1658496850\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 291200\n",
      "  training_iteration: 104\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   104</td><td style=\"text-align: right;\">         1928.83</td><td style=\"text-align: right;\">291200</td><td style=\"text-align: right;\">  -0.244</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            225.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 588000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-34-29\n",
      "  done: false\n",
      "  episode_len_mean: 226.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.9999999701976776\n",
      "  episode_reward_mean: -0.2909999988973141\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 1207\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.3366464592871212\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01579181331054484\n",
      "          policy_loss: -0.08104897944858316\n",
      "          total_loss: -0.06866605109271837\n",
      "          vf_explained_var: 0.21288159489631653\n",
      "          vf_loss: 0.004313038347650685\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.4492997590984618\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01471964479665425\n",
      "          policy_loss: -0.09153745130489759\n",
      "          total_loss: -0.07618627408703019\n",
      "          vf_explained_var: 0.3478504717350006\n",
      "          vf_loss: 0.004710733563721685\n",
      "    num_agent_steps_sampled: 588000\n",
      "    num_agent_steps_trained: 588000\n",
      "    num_steps_sampled: 294000\n",
      "    num_steps_trained: 294000\n",
      "  iterations_since_restore: 105\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.863999999999999\n",
      "    gpu_util_percent0: 0.0564\n",
      "    ram_util_percent: 54.452000000000005\n",
      "    vram_util_percent0: 0.2172890625\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 1.9999999701976776\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.19599999994039535\n",
      "    agent_1: -0.09499999895691871\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05171916825176203\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8903872498956527\n",
      "    mean_inference_ms: 1.7748863384050764\n",
      "    mean_raw_obs_processing_ms: 0.15823183094818166\n",
      "  time_since_restore: 1947.6859195232391\n",
      "  time_this_iter_s: 18.859061241149902\n",
      "  time_total_s: 1947.6859195232391\n",
      "  timers:\n",
      "    learn_throughput: 650.276\n",
      "    learn_time_ms: 4305.862\n",
      "    load_throughput: 89408.049\n",
      "    load_time_ms: 31.317\n",
      "    sample_throughput: 195.178\n",
      "    sample_time_ms: 14345.89\n",
      "    update_time_ms: 3.335\n",
      "  timestamp: 1658496869\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 294000\n",
      "  training_iteration: 105\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   105</td><td style=\"text-align: right;\">         1947.69</td><td style=\"text-align: right;\">294000</td><td style=\"text-align: right;\">  -0.291</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            226.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 593600\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-34-47\n",
      "  done: false\n",
      "  episode_len_mean: 236.94\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.9999999701976776\n",
      "  episode_reward_mean: -0.271999998614192\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1218\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.3926796913146973\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01805878235406014\n",
      "          policy_loss: -0.08910707745789773\n",
      "          total_loss: -0.07590822770708612\n",
      "          vf_explained_var: 0.159773588180542\n",
      "          vf_loss: 0.001855160726596036\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.4293862538678304\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.015849769278476318\n",
      "          policy_loss: -0.10932675151908759\n",
      "          total_loss: -0.09381265501031608\n",
      "          vf_explained_var: 0.3667210042476654\n",
      "          vf_loss: 0.0019260434077122703\n",
      "    num_agent_steps_sampled: 593600\n",
      "    num_agent_steps_trained: 593600\n",
      "    num_steps_sampled: 296800\n",
      "    num_steps_trained: 296800\n",
      "  iterations_since_restore: 106\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.8999999999999995\n",
      "    gpu_util_percent0: 0.050833333333333335\n",
      "    ram_util_percent: 54.40416666666666\n",
      "    vram_util_percent0: 0.21656901041666665\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 1.9999999701976776\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.19\n",
      "    agent_1: -0.08199999861419201\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05172159983315664\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8905176752265396\n",
      "    mean_inference_ms: 1.7750675366517283\n",
      "    mean_raw_obs_processing_ms: 0.15827041971225606\n",
      "  time_since_restore: 1966.286004781723\n",
      "  time_this_iter_s: 18.600085258483887\n",
      "  time_total_s: 1966.286004781723\n",
      "  timers:\n",
      "    learn_throughput: 651.602\n",
      "    learn_time_ms: 4297.099\n",
      "    load_throughput: 89272.461\n",
      "    load_time_ms: 31.365\n",
      "    sample_throughput: 195.338\n",
      "    sample_time_ms: 14334.157\n",
      "    update_time_ms: 3.329\n",
      "  timestamp: 1658496887\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 296800\n",
      "  training_iteration: 106\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   106</td><td style=\"text-align: right;\">         1966.29</td><td style=\"text-align: right;\">296800</td><td style=\"text-align: right;\">  -0.272</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            236.94</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 599200\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-35-06\n",
      "  done: false\n",
      "  episode_len_mean: 232.54\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.6000000163912773\n",
      "  episode_reward_mean: -0.32999999828636645\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 1231\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.3672306087045443\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.013673502892280648\n",
      "          policy_loss: -0.06652276069096497\n",
      "          total_loss: -0.0559222093186344\n",
      "          vf_explained_var: 0.004854233469814062\n",
      "          vf_loss: 0.003796558143048536\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.4388497656299952\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011597820316429848\n",
      "          policy_loss: -0.07695308375009734\n",
      "          total_loss: -0.0648750702451358\n",
      "          vf_explained_var: 0.2190190702676773\n",
      "          vf_loss: 0.0042663097836858855\n",
      "    num_agent_steps_sampled: 599200\n",
      "    num_agent_steps_trained: 599200\n",
      "    num_steps_sampled: 299600\n",
      "    num_steps_trained: 299600\n",
      "  iterations_since_restore: 107\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.824\n",
      "    gpu_util_percent0: 0.056799999999999996\n",
      "    ram_util_percent: 54.4\n",
      "    vram_util_percent0: 0.21748437499999998\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.6000000163912773\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.22\n",
      "    agent_1: -0.10999999828636646\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05172528160469028\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8906714294140823\n",
      "    mean_inference_ms: 1.7753164854808403\n",
      "    mean_raw_obs_processing_ms: 0.1583307102110736\n",
      "  time_since_restore: 1985.1234185695648\n",
      "  time_this_iter_s: 18.837413787841797\n",
      "  time_total_s: 1985.1234185695648\n",
      "  timers:\n",
      "    learn_throughput: 650.502\n",
      "    learn_time_ms: 4304.37\n",
      "    load_throughput: 95816.305\n",
      "    load_time_ms: 29.223\n",
      "    sample_throughput: 195.019\n",
      "    sample_time_ms: 14357.609\n",
      "    update_time_ms: 3.312\n",
      "  timestamp: 1658496906\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 299600\n",
      "  training_iteration: 107\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   107</td><td style=\"text-align: right;\">         1985.12</td><td style=\"text-align: right;\">299600</td><td style=\"text-align: right;\">   -0.33</td><td style=\"text-align: right;\">                 0.6</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            232.54</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 604800\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-35-25\n",
      "  done: false\n",
      "  episode_len_mean: 225.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.9000000208616257\n",
      "  episode_reward_mean: -0.31999999813735486\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 1245\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.3549762527857507\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01249686645890178\n",
      "          policy_loss: -0.05637297795025612\n",
      "          total_loss: -0.04600519827190077\n",
      "          vf_explained_var: 0.1325370967388153\n",
      "          vf_loss: 0.005636701696944545\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.409177642493021\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01029517547264088\n",
      "          policy_loss: -0.05475584920142345\n",
      "          total_loss: -0.043456803960171306\n",
      "          vf_explained_var: 0.0891118198633194\n",
      "          vf_loss: 0.0057260034506422626\n",
      "    num_agent_steps_sampled: 604800\n",
      "    num_agent_steps_trained: 604800\n",
      "    num_steps_sampled: 302400\n",
      "    num_steps_trained: 302400\n",
      "  iterations_since_restore: 108\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.879999999999999\n",
      "    gpu_util_percent0: 0.055999999999999994\n",
      "    ram_util_percent: 54.408\n",
      "    vram_util_percent0: 0.21748437499999998\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.30000001192092896\n",
      "    agent_1: 0.6000000089406967\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.21699999988079072\n",
      "    agent_1: -0.10299999825656414\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05172959616582931\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.890802582119617\n",
      "    mean_inference_ms: 1.7755638344942508\n",
      "    mean_raw_obs_processing_ms: 0.15841145331476617\n",
      "  time_since_restore: 2003.866329908371\n",
      "  time_this_iter_s: 18.742911338806152\n",
      "  time_total_s: 2003.866329908371\n",
      "  timers:\n",
      "    learn_throughput: 650.8\n",
      "    learn_time_ms: 4302.398\n",
      "    load_throughput: 95783.64\n",
      "    load_time_ms: 29.233\n",
      "    sample_throughput: 194.892\n",
      "    sample_time_ms: 14366.901\n",
      "    update_time_ms: 5.431\n",
      "  timestamp: 1658496925\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 302400\n",
      "  training_iteration: 108\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   108</td><td style=\"text-align: right;\">         2003.87</td><td style=\"text-align: right;\">302400</td><td style=\"text-align: right;\">   -0.32</td><td style=\"text-align: right;\">                 0.9</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            225.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 610400\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-35-44\n",
      "  done: false\n",
      "  episode_len_mean: 235.66\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.9000000208616257\n",
      "  episode_reward_mean: -0.2919999980181456\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 1254\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.3771157012808892\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011857275355415692\n",
      "          policy_loss: -0.057370220314450784\n",
      "          total_loss: -0.04836571691668637\n",
      "          vf_explained_var: 0.12291424721479416\n",
      "          vf_loss: 0.003140649570100539\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.4576187502770197\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011034925119556378\n",
      "          policy_loss: -0.055510642113967196\n",
      "          total_loss: -0.044218192848867535\n",
      "          vf_explained_var: 0.20207750797271729\n",
      "          vf_loss: 0.0036529473419315245\n",
      "    num_agent_steps_sampled: 610400\n",
      "    num_agent_steps_trained: 610400\n",
      "    num_steps_sampled: 305200\n",
      "    num_steps_trained: 305200\n",
      "  iterations_since_restore: 109\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.854166666666667\n",
      "    gpu_util_percent0: 0.048749999999999995\n",
      "    ram_util_percent: 54.4\n",
      "    vram_util_percent0: 0.21656901041666665\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.30000001192092896\n",
      "    agent_1: 0.6000000089406967\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.2069999998807907\n",
      "    agent_1: -0.08499999813735486\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05173228977664397\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8909222306005735\n",
      "    mean_inference_ms: 1.7757373314804406\n",
      "    mean_raw_obs_processing_ms: 0.15845137733619208\n",
      "  time_since_restore: 2022.5938844680786\n",
      "  time_this_iter_s: 18.72755455970764\n",
      "  time_total_s: 2022.5938844680786\n",
      "  timers:\n",
      "    learn_throughput: 650.844\n",
      "    learn_time_ms: 4302.107\n",
      "    load_throughput: 95746.0\n",
      "    load_time_ms: 29.244\n",
      "    sample_throughput: 194.827\n",
      "    sample_time_ms: 14371.704\n",
      "    update_time_ms: 5.406\n",
      "  timestamp: 1658496944\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 305200\n",
      "  training_iteration: 109\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   109</td><td style=\"text-align: right;\">         2022.59</td><td style=\"text-align: right;\">305200</td><td style=\"text-align: right;\">  -0.292</td><td style=\"text-align: right;\">                 0.9</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            235.66</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 616000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-36-02\n",
      "  done: false\n",
      "  episode_len_mean: 239.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.9000000208616257\n",
      "  episode_reward_mean: -0.2059999979287386\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1264\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.3710614634411677\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.014779161202800074\n",
      "          policy_loss: -0.06324497037632595\n",
      "          total_loss: -0.05217198446007387\n",
      "          vf_explained_var: 0.0771840438246727\n",
      "          vf_loss: 0.002782539389694908\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.402808370689551\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011379049375818795\n",
      "          policy_loss: -0.07103072037543393\n",
      "          total_loss: -0.059752449432077505\n",
      "          vf_explained_var: 0.2940417230129242\n",
      "          vf_loss: 0.0025650360101367583\n",
      "    num_agent_steps_sampled: 616000\n",
      "    num_agent_steps_trained: 616000\n",
      "    num_steps_sampled: 308000\n",
      "    num_steps_trained: 308000\n",
      "  iterations_since_restore: 110\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.92\n",
      "    gpu_util_percent0: 0.052000000000000005\n",
      "    ram_util_percent: 54.403999999999996\n",
      "    vram_util_percent0: 0.21751171875\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.30000001192092896\n",
      "    agent_1: 0.6000000089406967\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.1669999998807907\n",
      "    agent_1: -0.03899999804794788\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05173570737320029\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8910575571775494\n",
      "    mean_inference_ms: 1.775957937962193\n",
      "    mean_raw_obs_processing_ms: 0.1584802881544996\n",
      "  time_since_restore: 2041.3415954113007\n",
      "  time_this_iter_s: 18.747710943222046\n",
      "  time_total_s: 2041.3415954113007\n",
      "  timers:\n",
      "    learn_throughput: 652.736\n",
      "    learn_time_ms: 4289.637\n",
      "    load_throughput: 95731.015\n",
      "    load_time_ms: 29.249\n",
      "    sample_throughput: 194.696\n",
      "    sample_time_ms: 14381.376\n",
      "    update_time_ms: 5.408\n",
      "  timestamp: 1658496962\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 308000\n",
      "  training_iteration: 110\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   110</td><td style=\"text-align: right;\">         2041.34</td><td style=\"text-align: right;\">308000</td><td style=\"text-align: right;\">  -0.206</td><td style=\"text-align: right;\">                 0.9</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            239.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 621600\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-36-21\n",
      "  done: false\n",
      "  episode_len_mean: 246.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.9000000208616257\n",
      "  episode_reward_mean: -0.0729999977350235\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1275\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.3892261435588202\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.013949530675160042\n",
      "          policy_loss: -0.07193681122355754\n",
      "          total_loss: -0.061497380154010205\n",
      "          vf_explained_var: 0.04023231193423271\n",
      "          vf_loss: 0.002771757090764974\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.4292215825546357\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01355156479098107\n",
      "          policy_loss: -0.06955923391797114\n",
      "          total_loss: -0.05599951453684563\n",
      "          vf_explained_var: 0.08848116546869278\n",
      "          vf_loss: 0.002907368405690899\n",
      "    num_agent_steps_sampled: 621600\n",
      "    num_agent_steps_trained: 621600\n",
      "    num_steps_sampled: 310800\n",
      "    num_steps_trained: 310800\n",
      "  iterations_since_restore: 111\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.912000000000001\n",
      "    gpu_util_percent0: 0.0564\n",
      "    ram_util_percent: 54.43600000000001\n",
      "    vram_util_percent0: 0.216953125\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.30000001192092896\n",
      "    agent_1: 0.6000000089406967\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.10699999988079072\n",
      "    agent_1: 0.03400000214576721\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051740188222591785\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8912397239551466\n",
      "    mean_inference_ms: 1.7762081992121548\n",
      "    mean_raw_obs_processing_ms: 0.15849817762373397\n",
      "  time_since_restore: 2060.259166240692\n",
      "  time_this_iter_s: 18.91757082939148\n",
      "  time_total_s: 2060.259166240692\n",
      "  timers:\n",
      "    learn_throughput: 652.113\n",
      "    learn_time_ms: 4293.736\n",
      "    load_throughput: 95762.942\n",
      "    load_time_ms: 29.239\n",
      "    sample_throughput: 194.352\n",
      "    sample_time_ms: 14406.833\n",
      "    update_time_ms: 5.37\n",
      "  timestamp: 1658496981\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 310800\n",
      "  training_iteration: 111\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   111</td><td style=\"text-align: right;\">         2060.26</td><td style=\"text-align: right;\">310800</td><td style=\"text-align: right;\">  -0.073</td><td style=\"text-align: right;\">                 0.9</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            246.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 627200\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-36-40\n",
      "  done: false\n",
      "  episode_len_mean: 248.61\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.9000000208616257\n",
      "  episode_reward_mean: -0.12699999764561654\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 1287\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.3527992850258235\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011521890531974858\n",
      "          policy_loss: -0.04808681116749843\n",
      "          total_loss: -0.038454906294299734\n",
      "          vf_explained_var: -0.3415692448616028\n",
      "          vf_loss: 0.005620864593202853\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.4172444953804924\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.009916493020116409\n",
      "          policy_loss: -0.05075501264426358\n",
      "          total_loss: -0.039589777611018645\n",
      "          vf_explained_var: 0.03816612437367439\n",
      "          vf_loss: 0.006434604800320119\n",
      "    num_agent_steps_sampled: 627200\n",
      "    num_agent_steps_trained: 627200\n",
      "    num_steps_sampled: 313600\n",
      "    num_steps_trained: 313600\n",
      "  iterations_since_restore: 112\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.870833333333334\n",
      "    gpu_util_percent0: 0.050416666666666665\n",
      "    ram_util_percent: 54.4\n",
      "    vram_util_percent0: 0.2173502604166667\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.30000001192092896\n",
      "    agent_1: 0.6000000089406967\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.1369999998807907\n",
      "    agent_1: 0.01000000223517418\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05174543522948083\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8914688702480054\n",
      "    mean_inference_ms: 1.7764746763629007\n",
      "    mean_raw_obs_processing_ms: 0.15850997087594426\n",
      "  time_since_restore: 2079.2147855758667\n",
      "  time_this_iter_s: 18.95561933517456\n",
      "  time_total_s: 2079.2147855758667\n",
      "  timers:\n",
      "    learn_throughput: 651.029\n",
      "    learn_time_ms: 4300.883\n",
      "    load_throughput: 95777.234\n",
      "    load_time_ms: 29.235\n",
      "    sample_throughput: 194.389\n",
      "    sample_time_ms: 14404.127\n",
      "    update_time_ms: 5.364\n",
      "  timestamp: 1658497000\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 313600\n",
      "  training_iteration: 112\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   112</td><td style=\"text-align: right;\">         2079.21</td><td style=\"text-align: right;\">313600</td><td style=\"text-align: right;\">  -0.127</td><td style=\"text-align: right;\">                 0.9</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            248.61</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 632800\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-36-59\n",
      "  done: false\n",
      "  episode_len_mean: 249.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.9000000208616257\n",
      "  episode_reward_mean: -0.1199999975413084\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1297\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.3952928746030446\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011894223225938937\n",
      "          policy_loss: -0.07078397481581349\n",
      "          total_loss: -0.06165253163808735\n",
      "          vf_explained_var: -0.14247989654541016\n",
      "          vf_loss: 0.00344549776266268\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.4323908871128443\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.012538454237601995\n",
      "          policy_loss: -0.07841063898403686\n",
      "          total_loss: -0.06582194959130339\n",
      "          vf_explained_var: 0.21366150677204132\n",
      "          vf_loss: 0.0030312545508682626\n",
      "    num_agent_steps_sampled: 632800\n",
      "    num_agent_steps_trained: 632800\n",
      "    num_steps_sampled: 316400\n",
      "    num_steps_trained: 316400\n",
      "  iterations_since_restore: 113\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.848000000000001\n",
      "    gpu_util_percent0: 0.057999999999999996\n",
      "    ram_util_percent: 54.4\n",
      "    vram_util_percent0: 0.21748437499999998\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.30000001192092896\n",
      "    agent_1: 0.6000000089406967\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.1369999998807907\n",
      "    agent_1: 0.017000002339482307\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05174976642872229\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8916549411334382\n",
      "    mean_inference_ms: 1.7767096245759026\n",
      "    mean_raw_obs_processing_ms: 0.15852506796780655\n",
      "  time_since_restore: 2097.91517496109\n",
      "  time_this_iter_s: 18.70038938522339\n",
      "  time_total_s: 2097.91517496109\n",
      "  timers:\n",
      "    learn_throughput: 650.463\n",
      "    learn_time_ms: 4304.629\n",
      "    load_throughput: 96443.19\n",
      "    load_time_ms: 29.033\n",
      "    sample_throughput: 194.74\n",
      "    sample_time_ms: 14378.115\n",
      "    update_time_ms: 5.355\n",
      "  timestamp: 1658497019\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 316400\n",
      "  training_iteration: 113\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   113</td><td style=\"text-align: right;\">         2097.92</td><td style=\"text-align: right;\">316400</td><td style=\"text-align: right;\">   -0.12</td><td style=\"text-align: right;\">                 0.9</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            249.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 638400\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-37-18\n",
      "  done: false\n",
      "  episode_len_mean: 250.21\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.9000000208616257\n",
      "  episode_reward_mean: -0.10799999736249447\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1308\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.395163594966843\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.013899192331022728\n",
      "          policy_loss: -0.0589199209851878\n",
      "          total_loss: -0.048282659626455005\n",
      "          vf_explained_var: 0.025042464956641197\n",
      "          vf_loss: 0.003451318122166586\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.4640428445168903\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.013866913679634954\n",
      "          policy_loss: -0.08345416090672925\n",
      "          total_loss: -0.06894914272786645\n",
      "          vf_explained_var: 0.2731059491634369\n",
      "          vf_loss: 0.004747006147335057\n",
      "    num_agent_steps_sampled: 638400\n",
      "    num_agent_steps_trained: 638400\n",
      "    num_steps_sampled: 319200\n",
      "    num_steps_trained: 319200\n",
      "  iterations_since_restore: 114\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.896000000000002\n",
      "    gpu_util_percent0: 0.0508\n",
      "    ram_util_percent: 54.4\n",
      "    vram_util_percent0: 0.21687890625\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.30000001192092896\n",
      "    agent_1: 0.6000000089406967\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.13599999986588954\n",
      "    agent_1: 0.028000002503395082\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051754588286784545\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.891895671621757\n",
      "    mean_inference_ms: 1.776955873004132\n",
      "    mean_raw_obs_processing_ms: 0.15853609883049533\n",
      "  time_since_restore: 2116.8852207660675\n",
      "  time_this_iter_s: 18.970045804977417\n",
      "  time_total_s: 2116.8852207660675\n",
      "  timers:\n",
      "    learn_throughput: 649.985\n",
      "    learn_time_ms: 4307.794\n",
      "    load_throughput: 96049.595\n",
      "    load_time_ms: 29.152\n",
      "    sample_throughput: 194.263\n",
      "    sample_time_ms: 14413.444\n",
      "    update_time_ms: 5.361\n",
      "  timestamp: 1658497038\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 319200\n",
      "  training_iteration: 114\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   114</td><td style=\"text-align: right;\">         2116.89</td><td style=\"text-align: right;\">319200</td><td style=\"text-align: right;\">  -0.108</td><td style=\"text-align: right;\">                 0.9</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            250.21</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 644000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-37-37\n",
      "  done: false\n",
      "  episode_len_mean: 259.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.9000000208616257\n",
      "  episode_reward_mean: -0.10599999733269215\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 1316\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.397865594142959\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.017915403890000153\n",
      "          policy_loss: -0.0966500643380208\n",
      "          total_loss: -0.08367602548796207\n",
      "          vf_explained_var: -0.002700045006349683\n",
      "          vf_loss: 0.001526532309093281\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.4329873770475388\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.015584323323879627\n",
      "          policy_loss: -0.11668327230810453\n",
      "          total_loss: -0.10149220250319299\n",
      "          vf_explained_var: 0.3005565106868744\n",
      "          vf_loss: 0.0017659451829049864\n",
      "    num_agent_steps_sampled: 644000\n",
      "    num_agent_steps_trained: 644000\n",
      "    num_steps_sampled: 322000\n",
      "    num_steps_trained: 322000\n",
      "  iterations_since_restore: 115\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.824000000000001\n",
      "    gpu_util_percent0: 0.056799999999999996\n",
      "    ram_util_percent: 54.403999999999996\n",
      "    vram_util_percent0: 0.21748437499999998\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.30000001192092896\n",
      "    agent_1: 0.6000000089406967\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.13599999986588954\n",
      "    agent_1: 0.030000002533197404\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051758449443093074\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.892074662396575\n",
      "    mean_inference_ms: 1.7771366275332132\n",
      "    mean_raw_obs_processing_ms: 0.1585312875397784\n",
      "  time_since_restore: 2135.6240468025208\n",
      "  time_this_iter_s: 18.738826036453247\n",
      "  time_total_s: 2135.6240468025208\n",
      "  timers:\n",
      "    learn_throughput: 650.478\n",
      "    learn_time_ms: 4304.53\n",
      "    load_throughput: 96283.55\n",
      "    load_time_ms: 29.081\n",
      "    sample_throughput: 194.38\n",
      "    sample_time_ms: 14404.805\n",
      "    update_time_ms: 5.354\n",
      "  timestamp: 1658497057\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 322000\n",
      "  training_iteration: 115\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   115</td><td style=\"text-align: right;\">         2135.62</td><td style=\"text-align: right;\">322000</td><td style=\"text-align: right;\">  -0.106</td><td style=\"text-align: right;\">                 0.9</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            259.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 649600\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-37-56\n",
      "  done: false\n",
      "  episode_len_mean: 258.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.9000000208616257\n",
      "  episode_reward_mean: -0.12599999696016312\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 1329\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.3933109733320417\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.009675390053584601\n",
      "          policy_loss: -0.11101404745533641\n",
      "          total_loss: -0.10336638673020172\n",
      "          vf_explained_var: 0.09745807945728302\n",
      "          vf_loss: 0.003957958268182708\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.419428202367964\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.00878912978802991\n",
      "          policy_loss: -0.11943596000007044\n",
      "          total_loss: -0.11006974240410186\n",
      "          vf_explained_var: 0.1969970464706421\n",
      "          vf_loss: 0.0045198501365814\n",
      "    num_agent_steps_sampled: 649600\n",
      "    num_agent_steps_trained: 649600\n",
      "    num_steps_sampled: 324800\n",
      "    num_steps_trained: 324800\n",
      "  iterations_since_restore: 116\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.849999999999999\n",
      "    gpu_util_percent0: 0.050416666666666665\n",
      "    ram_util_percent: 54.4\n",
      "    vram_util_percent0: 0.21656901041666665\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.30000001192092896\n",
      "    agent_1: 0.6000000089406967\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.1549999998509884\n",
      "    agent_1: 0.02900000289082527\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051765033321557505\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.892365593985062\n",
      "    mean_inference_ms: 1.7774269982626703\n",
      "    mean_raw_obs_processing_ms: 0.15852502719629535\n",
      "  time_since_restore: 2154.443672657013\n",
      "  time_this_iter_s: 18.819625854492188\n",
      "  time_total_s: 2154.443672657013\n",
      "  timers:\n",
      "    learn_throughput: 648.822\n",
      "    learn_time_ms: 4315.514\n",
      "    load_throughput: 96393.636\n",
      "    load_time_ms: 29.048\n",
      "    sample_throughput: 194.231\n",
      "    sample_time_ms: 14415.838\n",
      "    update_time_ms: 5.337\n",
      "  timestamp: 1658497076\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 324800\n",
      "  training_iteration: 116\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   116</td><td style=\"text-align: right;\">         2154.44</td><td style=\"text-align: right;\">324800</td><td style=\"text-align: right;\">  -0.126</td><td style=\"text-align: right;\">                 0.9</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             258.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 655200\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-38-14\n",
      "  done: false\n",
      "  episode_len_mean: 268.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.7000000178813934\n",
      "  episode_reward_mean: -0.12399999685585499\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 1337\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.3452968608055795\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.014505601313295823\n",
      "          policy_loss: -0.059446418479746216\n",
      "          total_loss: -0.04825636869252646\n",
      "          vf_explained_var: -0.15041597187519073\n",
      "          vf_loss: 0.0036718125187083692\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.3956174488578523\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.012227818378090389\n",
      "          policy_loss: -0.06285723373843038\n",
      "          total_loss: -0.05022474379819912\n",
      "          vf_explained_var: 0.061534155160188675\n",
      "          vf_loss: 0.003999384551856825\n",
      "    num_agent_steps_sampled: 655200\n",
      "    num_agent_steps_trained: 655200\n",
      "    num_steps_sampled: 327600\n",
      "    num_steps_trained: 327600\n",
      "  iterations_since_restore: 117\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.9\n",
      "    gpu_util_percent0: 0.0644\n",
      "    ram_util_percent: 54.42400000000001\n",
      "    vram_util_percent0: 0.21780078125\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.30000001192092896\n",
      "    agent_1: 0.6000000089406967\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.1549999998509884\n",
      "    agent_1: 0.0310000029951334\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05176929223405091\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.892552026432651\n",
      "    mean_inference_ms: 1.7776084370443532\n",
      "    mean_raw_obs_processing_ms: 0.15849657099242695\n",
      "  time_since_restore: 2173.10094165802\n",
      "  time_this_iter_s: 18.65726900100708\n",
      "  time_total_s: 2173.10094165802\n",
      "  timers:\n",
      "    learn_throughput: 649.636\n",
      "    learn_time_ms: 4310.106\n",
      "    load_throughput: 96457.21\n",
      "    load_time_ms: 29.028\n",
      "    sample_throughput: 194.399\n",
      "    sample_time_ms: 14403.389\n",
      "    update_time_ms: 5.359\n",
      "  timestamp: 1658497094\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 327600\n",
      "  training_iteration: 117\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   117</td><td style=\"text-align: right;\">          2173.1</td><td style=\"text-align: right;\">327600</td><td style=\"text-align: right;\">  -0.124</td><td style=\"text-align: right;\">                 0.7</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            268.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 660800\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-38-33\n",
      "  done: false\n",
      "  episode_len_mean: 274.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.7000000178813934\n",
      "  episode_reward_mean: -0.10699999690055848\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 1346\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.376360277334849\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.016322478552258\n",
      "          policy_loss: -0.08657118973150361\n",
      "          total_loss: -0.07465070205286056\n",
      "          vf_explained_var: 0.18062520027160645\n",
      "          vf_loss: 0.0019039195650250242\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.426130468646685\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.015423679466134047\n",
      "          policy_loss: -0.0968230102076805\n",
      "          total_loss: -0.08171371371177624\n",
      "          vf_explained_var: 0.15150418877601624\n",
      "          vf_loss: 0.001982952505776276\n",
      "    num_agent_steps_sampled: 660800\n",
      "    num_agent_steps_trained: 660800\n",
      "    num_steps_sampled: 330400\n",
      "    num_steps_trained: 330400\n",
      "  iterations_since_restore: 118\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.507999999999999\n",
      "    gpu_util_percent0: 0.0552\n",
      "    ram_util_percent: 54.552\n",
      "    vram_util_percent0: 0.2173046875\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.30000001192092896\n",
      "    agent_1: 0.6000000089406967\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.14499999985098838\n",
      "    agent_1: 0.03800000295042991\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051774755776894915\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8927903734367693\n",
      "    mean_inference_ms: 1.777834310644588\n",
      "    mean_raw_obs_processing_ms: 0.1584526091740509\n",
      "  time_since_restore: 2192.027302980423\n",
      "  time_this_iter_s: 18.926361322402954\n",
      "  time_total_s: 2192.027302980423\n",
      "  timers:\n",
      "    learn_throughput: 647.939\n",
      "    learn_time_ms: 4321.394\n",
      "    load_throughput: 95690.766\n",
      "    load_time_ms: 29.261\n",
      "    sample_throughput: 194.261\n",
      "    sample_time_ms: 14413.591\n",
      "    update_time_ms: 3.268\n",
      "  timestamp: 1658497113\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 330400\n",
      "  training_iteration: 118\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   118</td><td style=\"text-align: right;\">         2192.03</td><td style=\"text-align: right;\">330400</td><td style=\"text-align: right;\">  -0.107</td><td style=\"text-align: right;\">                 0.7</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            274.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 666400\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-38-52\n",
      "  done: false\n",
      "  episode_len_mean: 279.02\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: -0.08799999624490738\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 1355\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.383316609831083\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.007976803978066454\n",
      "          policy_loss: 0.05615660146802492\n",
      "          total_loss: 0.06254674987325243\n",
      "          vf_explained_var: 0.06291036307811737\n",
      "          vf_loss: 0.003992991508398734\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.466713010555222\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.008273997362279765\n",
      "          policy_loss: 0.036617698413985114\n",
      "          total_loss: 0.04528963908801199\n",
      "          vf_explained_var: -0.14271323382854462\n",
      "          vf_loss: 0.004064591925054542\n",
      "    num_agent_steps_sampled: 666400\n",
      "    num_agent_steps_trained: 666400\n",
      "    num_steps_sampled: 333200\n",
      "    num_steps_trained: 333200\n",
      "  iterations_since_restore: 119\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.0\n",
      "    gpu_util_percent0: 0.04624999999999999\n",
      "    ram_util_percent: 54.94166666666666\n",
      "    vram_util_percent0: 0.21656901041666665\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.1349999998509884\n",
      "    agent_1: 0.04700000360608101\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051780319971875134\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.893036323367629\n",
      "    mean_inference_ms: 1.7780500053712176\n",
      "    mean_raw_obs_processing_ms: 0.15840898765152847\n",
      "  time_since_restore: 2210.840917110443\n",
      "  time_this_iter_s: 18.81361413002014\n",
      "  time_total_s: 2210.840917110443\n",
      "  timers:\n",
      "    learn_throughput: 644.837\n",
      "    learn_time_ms: 4342.183\n",
      "    load_throughput: 95727.348\n",
      "    load_time_ms: 29.25\n",
      "    sample_throughput: 194.425\n",
      "    sample_time_ms: 14401.424\n",
      "    update_time_ms: 3.288\n",
      "  timestamp: 1658497132\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 333200\n",
      "  training_iteration: 119\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   119</td><td style=\"text-align: right;\">         2210.84</td><td style=\"text-align: right;\">333200</td><td style=\"text-align: right;\">  -0.088</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            279.02</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 672000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-39-11\n",
      "  done: false\n",
      "  episode_len_mean: 278.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: -0.0429999965429306\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 1364\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.385060211732274\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.009915764245837547\n",
      "          policy_loss: -0.06361543077721199\n",
      "          total_loss: -0.052697909909814676\n",
      "          vf_explained_var: -0.2817467749118805\n",
      "          vf_loss: 0.012767890736410794\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.4120866464717046\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011395065605688995\n",
      "          policy_loss: -0.06188119605243022\n",
      "          total_loss: -0.04902643750041794\n",
      "          vf_explained_var: -0.02468382939696312\n",
      "          vf_loss: 0.007030679461292623\n",
      "    num_agent_steps_sampled: 672000\n",
      "    num_agent_steps_trained: 672000\n",
      "    num_steps_sampled: 336000\n",
      "    num_steps_trained: 336000\n",
      "  iterations_since_restore: 120\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.928\n",
      "    gpu_util_percent0: 0.0588\n",
      "    ram_util_percent: 54.684\n",
      "    vram_util_percent0: 0.21766015625000001\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.11499999985098838\n",
      "    agent_1: 0.07200000330805778\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0517856243963815\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.893280419700588\n",
      "    mean_inference_ms: 1.7782406001825075\n",
      "    mean_raw_obs_processing_ms: 0.15835892152949207\n",
      "  time_since_restore: 2229.4036004543304\n",
      "  time_this_iter_s: 18.56268334388733\n",
      "  time_total_s: 2229.4036004543304\n",
      "  timers:\n",
      "    learn_throughput: 643.986\n",
      "    learn_time_ms: 4347.919\n",
      "    load_throughput: 95552.26\n",
      "    load_time_ms: 29.303\n",
      "    sample_throughput: 194.752\n",
      "    sample_time_ms: 14377.267\n",
      "    update_time_ms: 3.262\n",
      "  timestamp: 1658497151\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 336000\n",
      "  training_iteration: 120\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   120</td><td style=\"text-align: right;\">          2229.4</td><td style=\"text-align: right;\">336000</td><td style=\"text-align: right;\">  -0.043</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            278.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 677600\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-39-29\n",
      "  done: false\n",
      "  episode_len_mean: 284.14\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: -0.06499999657273292\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1374\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.3386393927392506\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.016379529040111016\n",
      "          policy_loss: -0.07201137548586953\n",
      "          total_loss: -0.05941744090926347\n",
      "          vf_explained_var: 0.3199727237224579\n",
      "          vf_loss: 0.003659454176505235\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.4636170204196657\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01171758872336281\n",
      "          policy_loss: -0.0841713270745545\n",
      "          total_loss: -0.07205480467625118\n",
      "          vf_explained_var: 0.25485479831695557\n",
      "          vf_loss: 0.004063743207985854\n",
      "    num_agent_steps_sampled: 677600\n",
      "    num_agent_steps_trained: 677600\n",
      "    num_steps_sampled: 338800\n",
      "    num_steps_trained: 338800\n",
      "  iterations_since_restore: 121\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.820833333333333\n",
      "    gpu_util_percent0: 0.050833333333333335\n",
      "    ram_util_percent: 54.5\n",
      "    vram_util_percent0: 0.216552734375\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.12499999985098839\n",
      "    agent_1: 0.060000003278255463\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051790870701054724\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.893525517082678\n",
      "    mean_inference_ms: 1.7784259880994233\n",
      "    mean_raw_obs_processing_ms: 0.15829865708617663\n",
      "  time_since_restore: 2248.0415568351746\n",
      "  time_this_iter_s: 18.637956380844116\n",
      "  time_total_s: 2248.0415568351746\n",
      "  timers:\n",
      "    learn_throughput: 645.824\n",
      "    learn_time_ms: 4335.545\n",
      "    load_throughput: 95591.381\n",
      "    load_time_ms: 29.291\n",
      "    sample_throughput: 194.964\n",
      "    sample_time_ms: 14361.59\n",
      "    update_time_ms: 3.247\n",
      "  timestamp: 1658497169\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 338800\n",
      "  training_iteration: 121\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   121</td><td style=\"text-align: right;\">         2248.04</td><td style=\"text-align: right;\">338800</td><td style=\"text-align: right;\">  -0.065</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            284.14</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 683200\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-39-48\n",
      "  done: false\n",
      "  episode_len_mean: 295.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: -0.06399999655783177\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 1381\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.3561066037842204\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.017368977381844185\n",
      "          policy_loss: -0.10136556506179095\n",
      "          total_loss: -0.08838636632676103\n",
      "          vf_explained_var: 0.20588736236095428\n",
      "          vf_loss: 0.0026615947920055035\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.4058276365200677\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01434466427840668\n",
      "          policy_loss: -0.07579526201533597\n",
      "          total_loss: -0.06136599713188064\n",
      "          vf_explained_var: -0.08469048887491226\n",
      "          vf_loss: 0.003097814581494839\n",
      "    num_agent_steps_sampled: 683200\n",
      "    num_agent_steps_trained: 683200\n",
      "    num_steps_sampled: 341600\n",
      "    num_steps_trained: 341600\n",
      "  iterations_since_restore: 122\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.843999999999999\n",
      "    gpu_util_percent0: 0.0532\n",
      "    ram_util_percent: 54.5\n",
      "    vram_util_percent0: 0.21748437499999998\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.12499999985098839\n",
      "    agent_1: 0.06100000329315662\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051794739403530556\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8936943689373082\n",
      "    mean_inference_ms: 1.7785572572909905\n",
      "    mean_raw_obs_processing_ms: 0.15823862185322\n",
      "  time_since_restore: 2266.778539657593\n",
      "  time_this_iter_s: 18.736982822418213\n",
      "  time_total_s: 2266.778539657593\n",
      "  timers:\n",
      "    learn_throughput: 648.123\n",
      "    learn_time_ms: 4320.168\n",
      "    load_throughput: 95772.86\n",
      "    load_time_ms: 29.236\n",
      "    sample_throughput: 195.052\n",
      "    sample_time_ms: 14355.116\n",
      "    update_time_ms: 3.237\n",
      "  timestamp: 1658497188\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 341600\n",
      "  training_iteration: 122\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   122</td><td style=\"text-align: right;\">         2266.78</td><td style=\"text-align: right;\">341600</td><td style=\"text-align: right;\">  -0.064</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            295.37</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 688800\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-40-07\n",
      "  done: false\n",
      "  episode_len_mean: 301.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: -0.04499999649822712\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1391\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.3915507736660184\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011033279384699245\n",
      "          policy_loss: -0.09997140595805831\n",
      "          total_loss: -0.08873365014802576\n",
      "          vf_explained_var: -0.25022006034851074\n",
      "          vf_loss: 0.011297147833656276\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.4648335455428985\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.00821708371162704\n",
      "          policy_loss: -0.09278653627595868\n",
      "          total_loss: -0.08181420016375168\n",
      "          vf_explained_var: -0.21491940319538116\n",
      "          vf_loss: 0.010791693602922828\n",
      "    num_agent_steps_sampled: 688800\n",
      "    num_agent_steps_trained: 688800\n",
      "    num_steps_sampled: 344400\n",
      "    num_steps_trained: 344400\n",
      "  iterations_since_restore: 123\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.94\n",
      "    gpu_util_percent0: 0.056799999999999996\n",
      "    ram_util_percent: 54.448\n",
      "    vram_util_percent0: 0.21770312500000003\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.11499999985098838\n",
      "    agent_1: 0.07000000335276127\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05180057819360353\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8939408642777162\n",
      "    mean_inference_ms: 1.778742210455529\n",
      "    mean_raw_obs_processing_ms: 0.1581492370949457\n",
      "  time_since_restore: 2285.6234741210938\n",
      "  time_this_iter_s: 18.844934463500977\n",
      "  time_total_s: 2285.6234741210938\n",
      "  timers:\n",
      "    learn_throughput: 648.839\n",
      "    learn_time_ms: 4315.403\n",
      "    load_throughput: 95852.67\n",
      "    load_time_ms: 29.211\n",
      "    sample_throughput: 194.793\n",
      "    sample_time_ms: 14374.265\n",
      "    update_time_ms: 3.234\n",
      "  timestamp: 1658497207\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 344400\n",
      "  training_iteration: 123\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   123</td><td style=\"text-align: right;\">         2285.62</td><td style=\"text-align: right;\">344400</td><td style=\"text-align: right;\">  -0.045</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            301.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 694400\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-40-26\n",
      "  done: false\n",
      "  episode_len_mean: 295.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: -0.00499999649822712\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 1400\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.336895955815202\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.015675794053590795\n",
      "          policy_loss: -0.09234610815976685\n",
      "          total_loss: -0.08013755050039133\n",
      "          vf_explained_var: 0.09832748025655746\n",
      "          vf_loss: 0.004063960652101308\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.4474156385376338\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.015396291121826547\n",
      "          policy_loss: -0.08601739733213824\n",
      "          total_loss: -0.07055002144937005\n",
      "          vf_explained_var: 0.20657041668891907\n",
      "          vf_loss: 0.0031086067508676067\n",
      "    num_agent_steps_sampled: 694400\n",
      "    num_agent_steps_trained: 694400\n",
      "    num_steps_sampled: 347200\n",
      "    num_steps_trained: 347200\n",
      "  iterations_since_restore: 124\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.2\n",
      "    gpu_util_percent0: 0.04958333333333333\n",
      "    ram_util_percent: 54.44166666666667\n",
      "    vram_util_percent0: 0.21656087239583333\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.09399999983608723\n",
      "    agent_1: 0.08900000333786011\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05180558812170134\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.894219850201199\n",
      "    mean_inference_ms: 1.778884572047591\n",
      "    mean_raw_obs_processing_ms: 0.1580622693396012\n",
      "  time_since_restore: 2304.4065868854523\n",
      "  time_this_iter_s: 18.78311276435852\n",
      "  time_total_s: 2304.4065868854523\n",
      "  timers:\n",
      "    learn_throughput: 650.503\n",
      "    learn_time_ms: 4304.363\n",
      "    load_throughput: 96245.911\n",
      "    load_time_ms: 29.092\n",
      "    sample_throughput: 194.897\n",
      "    sample_time_ms: 14366.578\n",
      "    update_time_ms: 3.258\n",
      "  timestamp: 1658497226\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 347200\n",
      "  training_iteration: 124\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   124</td><td style=\"text-align: right;\">         2304.41</td><td style=\"text-align: right;\">347200</td><td style=\"text-align: right;\">  -0.005</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             295.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 700000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-40-45\n",
      "  done: false\n",
      "  episode_len_mean: 293.66\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.07100000314414501\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1411\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.4363227734963098\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011123282351653205\n",
      "          policy_loss: -0.06270683740426432\n",
      "          total_loss: -0.05121380823246519\n",
      "          vf_explained_var: -0.1919466257095337\n",
      "          vf_loss: 0.011886336753594236\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.4613485435644784\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.012651434336020067\n",
      "          policy_loss: -0.06402222598998855\n",
      "          total_loss: -0.050018683170492295\n",
      "          vf_explained_var: 0.13570216298103333\n",
      "          vf_loss: 0.006782061627182841\n",
      "    num_agent_steps_sampled: 700000\n",
      "    num_agent_steps_trained: 700000\n",
      "    num_steps_sampled: 350000\n",
      "    num_steps_trained: 350000\n",
      "  iterations_since_restore: 125\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.856\n",
      "    gpu_util_percent0: 0.054799999999999995\n",
      "    ram_util_percent: 54.5\n",
      "    vram_util_percent0: 0.21748437499999998\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.054999999850988385\n",
      "    agent_1: 0.1260000029951334\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05181225465346439\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.89456598983272\n",
      "    mean_inference_ms: 1.779089365994173\n",
      "    mean_raw_obs_processing_ms: 0.15796313588557176\n",
      "  time_since_restore: 2323.316371679306\n",
      "  time_this_iter_s: 18.90978479385376\n",
      "  time_total_s: 2323.316371679306\n",
      "  timers:\n",
      "    learn_throughput: 651.743\n",
      "    learn_time_ms: 4296.175\n",
      "    load_throughput: 95833.428\n",
      "    load_time_ms: 29.217\n",
      "    sample_throughput: 194.558\n",
      "    sample_time_ms: 14391.601\n",
      "    update_time_ms: 3.286\n",
      "  timestamp: 1658497245\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 350000\n",
      "  training_iteration: 125\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   125</td><td style=\"text-align: right;\">         2323.32</td><td style=\"text-align: right;\">350000</td><td style=\"text-align: right;\">   0.071</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            293.66</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 705600\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-41-04\n",
      "  done: false\n",
      "  episode_len_mean: 294.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.06800000302493572\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1422\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.3954920825504122\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.014529282399579756\n",
      "          policy_loss: -0.09761054162351814\n",
      "          total_loss: -0.08661128005232535\n",
      "          vf_explained_var: -0.41766998171806335\n",
      "          vf_loss: 0.003136070307673002\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.437430113554001\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.012298621001597409\n",
      "          policy_loss: -0.08207692636642605\n",
      "          total_loss: -0.06953085774910592\n",
      "          vf_explained_var: 0.20842288434505463\n",
      "          vf_loss: 0.0036002141888665\n",
      "    num_agent_steps_sampled: 705600\n",
      "    num_agent_steps_trained: 705600\n",
      "    num_steps_sampled: 352800\n",
      "    num_steps_trained: 352800\n",
      "  iterations_since_restore: 126\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.872000000000001\n",
      "    gpu_util_percent0: 0.05399999999999999\n",
      "    ram_util_percent: 54.456\n",
      "    vram_util_percent0: 0.21670312500000002\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.054999999850988385\n",
      "    agent_1: 0.12300000287592411\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05181876604926961\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8949281591217018\n",
      "    mean_inference_ms: 1.7792923075681748\n",
      "    mean_raw_obs_processing_ms: 0.15787182898090876\n",
      "  time_since_restore: 2342.1569871902466\n",
      "  time_this_iter_s: 18.84061551094055\n",
      "  time_total_s: 2342.1569871902466\n",
      "  timers:\n",
      "    learn_throughput: 652.844\n",
      "    learn_time_ms: 4288.925\n",
      "    load_throughput: 95476.132\n",
      "    load_time_ms: 29.327\n",
      "    sample_throughput: 194.435\n",
      "    sample_time_ms: 14400.727\n",
      "    update_time_ms: 3.282\n",
      "  timestamp: 1658497264\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 352800\n",
      "  training_iteration: 126\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   126</td><td style=\"text-align: right;\">         2342.16</td><td style=\"text-align: right;\">352800</td><td style=\"text-align: right;\">   0.068</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            294.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 711200\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-41-23\n",
      "  done: false\n",
      "  episode_len_mean: 297.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.10200000286102295\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1432\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.4095355683848974\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.012630979226735447\n",
      "          policy_loss: -0.07007884971972089\n",
      "          total_loss: -0.06050372293395326\n",
      "          vf_explained_var: -0.26310962438583374\n",
      "          vf_loss: 0.0031515909353661948\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.461636260861442\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01224649271225349\n",
      "          policy_loss: -0.06760122682698282\n",
      "          total_loss: -0.054994677676621084\n",
      "          vf_explained_var: 0.17043383419513702\n",
      "          vf_loss: 0.00395042030175405\n",
      "    num_agent_steps_sampled: 711200\n",
      "    num_agent_steps_trained: 711200\n",
      "    num_steps_sampled: 355600\n",
      "    num_steps_trained: 355600\n",
      "  iterations_since_restore: 127\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.92\n",
      "    gpu_util_percent0: 0.0472\n",
      "    ram_util_percent: 54.44\n",
      "    vram_util_percent0: 0.21769921874999998\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.038999999985098836\n",
      "    agent_1: 0.1410000028461218\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05182493295049376\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8952806384879533\n",
      "    mean_inference_ms: 1.7794830254158234\n",
      "    mean_raw_obs_processing_ms: 0.15778453037611115\n",
      "  time_since_restore: 2361.123466491699\n",
      "  time_this_iter_s: 18.966479301452637\n",
      "  time_total_s: 2361.123466491699\n",
      "  timers:\n",
      "    learn_throughput: 650.532\n",
      "    learn_time_ms: 4304.172\n",
      "    load_throughput: 95629.833\n",
      "    load_time_ms: 29.28\n",
      "    sample_throughput: 194.224\n",
      "    sample_time_ms: 14416.337\n",
      "    update_time_ms: 3.279\n",
      "  timestamp: 1658497283\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 355600\n",
      "  training_iteration: 127\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   127</td><td style=\"text-align: right;\">         2361.12</td><td style=\"text-align: right;\">355600</td><td style=\"text-align: right;\">   0.102</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             297.7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 716800\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-41-42\n",
      "  done: false\n",
      "  episode_len_mean: 292.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.10800000287592411\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1442\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.387622527423359\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.014534741098175267\n",
      "          policy_loss: -0.0779217665901039\n",
      "          total_loss: -0.06705160255313829\n",
      "          vf_explained_var: -0.4418891668319702\n",
      "          vf_loss: 0.0027465104267321294\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.4603272130091987\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.012722194734466936\n",
      "          policy_loss: -0.07931100592014402\n",
      "          total_loss: -0.06644192578165703\n",
      "          vf_explained_var: 0.08433762192726135\n",
      "          vf_loss: 0.0033403212088333454\n",
      "    num_agent_steps_sampled: 716800\n",
      "    num_agent_steps_trained: 716800\n",
      "    num_steps_sampled: 358400\n",
      "    num_steps_trained: 358400\n",
      "  iterations_since_restore: 128\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.8839999999999995\n",
      "    gpu_util_percent0: 0.0532\n",
      "    ram_util_percent: 54.492\n",
      "    vram_util_percent0: 0.21748437499999998\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.038999999985098836\n",
      "    agent_1: 0.14700000286102294\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05183072825669445\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.895685484384206\n",
      "    mean_inference_ms: 1.7796758441302132\n",
      "    mean_raw_obs_processing_ms: 0.1577094393933052\n",
      "  time_since_restore: 2380.0833530426025\n",
      "  time_this_iter_s: 18.95988655090332\n",
      "  time_total_s: 2380.0833530426025\n",
      "  timers:\n",
      "    learn_throughput: 651.22\n",
      "    learn_time_ms: 4299.624\n",
      "    load_throughput: 96144.74\n",
      "    load_time_ms: 29.123\n",
      "    sample_throughput: 194.115\n",
      "    sample_time_ms: 14424.456\n",
      "    update_time_ms: 3.252\n",
      "  timestamp: 1658497302\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 358400\n",
      "  training_iteration: 128\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   128</td><td style=\"text-align: right;\">         2380.08</td><td style=\"text-align: right;\">358400</td><td style=\"text-align: right;\">   0.108</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            292.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 722400\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-42-00\n",
      "  done: false\n",
      "  episode_len_mean: 290.2\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.08500000275671482\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1452\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.342744893616154\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.013072052204004151\n",
      "          policy_loss: -0.061103141365276226\n",
      "          total_loss: -0.05106011538486027\n",
      "          vf_explained_var: -0.20652911067008972\n",
      "          vf_loss: 0.003463647903810765\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.416581561168035\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011080640665162688\n",
      "          policy_loss: -0.05651156003088025\n",
      "          total_loss: -0.04487611807139945\n",
      "          vf_explained_var: 0.024913018569350243\n",
      "          vf_loss: 0.004452847917578072\n",
      "    num_agent_steps_sampled: 722400\n",
      "    num_agent_steps_trained: 722400\n",
      "    num_steps_sampled: 361200\n",
      "    num_steps_trained: 361200\n",
      "  iterations_since_restore: 129\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.888\n",
      "    gpu_util_percent0: 0.056799999999999996\n",
      "    ram_util_percent: 54.483999999999995\n",
      "    vram_util_percent0: 0.21670312500000002\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.04899999998509884\n",
      "    agent_1: 0.13400000274181367\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05183643993731502\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.896054458015192\n",
      "    mean_inference_ms: 1.779852580380297\n",
      "    mean_raw_obs_processing_ms: 0.15764126280659427\n",
      "  time_since_restore: 2398.85791015625\n",
      "  time_this_iter_s: 18.77455711364746\n",
      "  time_total_s: 2398.85791015625\n",
      "  timers:\n",
      "    learn_throughput: 652.135\n",
      "    learn_time_ms: 4293.587\n",
      "    load_throughput: 96210.903\n",
      "    load_time_ms: 29.103\n",
      "    sample_throughput: 194.085\n",
      "    sample_time_ms: 14426.688\n",
      "    update_time_ms: 3.236\n",
      "  timestamp: 1658497320\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 361200\n",
      "  training_iteration: 129\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   129</td><td style=\"text-align: right;\">         2398.86</td><td style=\"text-align: right;\">361200</td><td style=\"text-align: right;\">   0.085</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             290.2</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 728000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-42-19\n",
      "  done: false\n",
      "  episode_len_mean: 270.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 0.029000002443790435\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 1468\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.3071543428869474\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.012880383003288462\n",
      "          policy_loss: -0.04850877357893339\n",
      "          total_loss: -0.03458944429864156\n",
      "          vf_explained_var: -0.36808180809020996\n",
      "          vf_loss: 0.014897302780030128\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.333626552706673\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01049650700672917\n",
      "          policy_loss: -0.06071608889483503\n",
      "          total_loss: -0.04800527097159786\n",
      "          vf_explained_var: 0.022554971277713776\n",
      "          vf_loss: 0.009091714519204938\n",
      "    num_agent_steps_sampled: 728000\n",
      "    num_agent_steps_trained: 728000\n",
      "    num_steps_sampled: 364000\n",
      "    num_steps_trained: 364000\n",
      "  iterations_since_restore: 130\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.966666666666666\n",
      "    gpu_util_percent0: 0.04791666666666666\n",
      "    ram_util_percent: 54.56666666666666\n",
      "    vram_util_percent0: 0.2177083333333333\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.07799999997019767\n",
      "    agent_1: 0.10700000241398812\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05184396179963852\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8965697260288668\n",
      "    mean_inference_ms: 1.7800784329495525\n",
      "    mean_raw_obs_processing_ms: 0.1575930806051247\n",
      "  time_since_restore: 2417.494954586029\n",
      "  time_this_iter_s: 18.637044429779053\n",
      "  time_total_s: 2417.494954586029\n",
      "  timers:\n",
      "    learn_throughput: 652.051\n",
      "    learn_time_ms: 4294.141\n",
      "    load_throughput: 96091.168\n",
      "    load_time_ms: 29.139\n",
      "    sample_throughput: 193.994\n",
      "    sample_time_ms: 14433.42\n",
      "    update_time_ms: 3.264\n",
      "  timestamp: 1658497339\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 364000\n",
      "  training_iteration: 130\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   130</td><td style=\"text-align: right;\">         2417.49</td><td style=\"text-align: right;\">364000</td><td style=\"text-align: right;\">   0.029</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            270.04</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 733600\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-42-38\n",
      "  done: false\n",
      "  episode_len_mean: 253.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 0.03400000251829624\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1479\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.3438267026628767\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.014354241151632165\n",
      "          policy_loss: -0.06751622858324222\n",
      "          total_loss: -0.05571666483662002\n",
      "          vf_explained_var: -0.10784922540187836\n",
      "          vf_loss: 0.005734090616204499\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.3808639631384896\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.012517976882688695\n",
      "          policy_loss: -0.06713778185396568\n",
      "          total_loss: -0.053839342662762475\n",
      "          vf_explained_var: 0.2585335969924927\n",
      "          vf_loss: 0.005054628233448879\n",
      "    num_agent_steps_sampled: 733600\n",
      "    num_agent_steps_trained: 733600\n",
      "    num_steps_sampled: 366800\n",
      "    num_steps_trained: 366800\n",
      "  iterations_since_restore: 131\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.8919999999999995\n",
      "    gpu_util_percent0: 0.0552\n",
      "    ram_util_percent: 54.468\n",
      "    vram_util_percent0: 0.21670312500000002\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.07799999997019767\n",
      "    agent_1: 0.11200000248849391\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05184854345250723\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8968927357258227\n",
      "    mean_inference_ms: 1.7802325583823408\n",
      "    mean_raw_obs_processing_ms: 0.15757632036443808\n",
      "  time_since_restore: 2436.209494829178\n",
      "  time_this_iter_s: 18.714540243148804\n",
      "  time_total_s: 2436.209494829178\n",
      "  timers:\n",
      "    learn_throughput: 650.577\n",
      "    learn_time_ms: 4303.873\n",
      "    load_throughput: 96153.399\n",
      "    load_time_ms: 29.12\n",
      "    sample_throughput: 194.023\n",
      "    sample_time_ms: 14431.313\n",
      "    update_time_ms: 3.292\n",
      "  timestamp: 1658497358\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 366800\n",
      "  training_iteration: 131\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   131</td><td style=\"text-align: right;\">         2436.21</td><td style=\"text-align: right;\">366800</td><td style=\"text-align: right;\">   0.034</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             253.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 739200\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-42-57\n",
      "  done: false\n",
      "  episode_len_mean: 247.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.999999985098839\n",
      "  episode_reward_mean: 0.004000002667307854\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 1492\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.3136699824106124\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.014343368082418248\n",
      "          policy_loss: -0.0779890329653253\n",
      "          total_loss: -0.06583036698326136\n",
      "          vf_explained_var: -0.0684371367096901\n",
      "          vf_loss: 0.006746689416883912\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.3345424820269858\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011457360626011517\n",
      "          policy_loss: -0.07111754221218039\n",
      "          total_loss: -0.058158916547351204\n",
      "          vf_explained_var: 0.2508324384689331\n",
      "          vf_loss: 0.007057287965705784\n",
      "    num_agent_steps_sampled: 739200\n",
      "    num_agent_steps_trained: 739200\n",
      "    num_steps_sampled: 369600\n",
      "    num_steps_trained: 369600\n",
      "  iterations_since_restore: 132\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.904\n",
      "    gpu_util_percent0: 0.056799999999999996\n",
      "    ram_util_percent: 54.5\n",
      "    vram_util_percent0: 0.21748437499999998\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 1.9999999850988388\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.09799999997019768\n",
      "    agent_1: 0.10200000263750553\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0518527378206511\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8972469222464263\n",
      "    mean_inference_ms: 1.7803774662185845\n",
      "    mean_raw_obs_processing_ms: 0.15758942788053407\n",
      "  time_since_restore: 2454.960703611374\n",
      "  time_this_iter_s: 18.751208782196045\n",
      "  time_total_s: 2454.960703611374\n",
      "  timers:\n",
      "    learn_throughput: 650.742\n",
      "    learn_time_ms: 4302.782\n",
      "    load_throughput: 96077.804\n",
      "    load_time_ms: 29.143\n",
      "    sample_throughput: 193.989\n",
      "    sample_time_ms: 14433.779\n",
      "    update_time_ms: 3.28\n",
      "  timestamp: 1658497377\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 369600\n",
      "  training_iteration: 132\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   132</td><td style=\"text-align: right;\">         2454.96</td><td style=\"text-align: right;\">369600</td><td style=\"text-align: right;\">   0.004</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            247.39</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 744800\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-43-15\n",
      "  done: false\n",
      "  episode_len_mean: 239.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.999999985098839\n",
      "  episode_reward_mean: -0.04699999690055847\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 1505\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.3314175399995984\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.014083407699903915\n",
      "          policy_loss: -0.06983302058263992\n",
      "          total_loss: -0.05896716584850635\n",
      "          vf_explained_var: -0.05734900012612343\n",
      "          vf_loss: 0.0036337866041711614\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.384956200917562\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011874411668221702\n",
      "          policy_loss: -0.07437584705517761\n",
      "          total_loss: -0.062041065776654816\n",
      "          vf_explained_var: 0.10151824355125427\n",
      "          vf_loss: 0.004145738993386095\n",
      "    num_agent_steps_sampled: 744800\n",
      "    num_agent_steps_trained: 744800\n",
      "    num_steps_sampled: 372400\n",
      "    num_steps_trained: 372400\n",
      "  iterations_since_restore: 133\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.8875\n",
      "    gpu_util_percent0: 0.059166666666666666\n",
      "    ram_util_percent: 54.5\n",
      "    vram_util_percent0: 0.2175048828125\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 1.9999999850988388\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.12899999998509884\n",
      "    agent_1: 0.08200000308454036\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05185553933640475\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8974229355187138\n",
      "    mean_inference_ms: 1.7804579786022532\n",
      "    mean_raw_obs_processing_ms: 0.15762632093821338\n",
      "  time_since_restore: 2473.358868122101\n",
      "  time_this_iter_s: 18.39816451072693\n",
      "  time_total_s: 2473.358868122101\n",
      "  timers:\n",
      "    learn_throughput: 650.958\n",
      "    learn_time_ms: 4301.352\n",
      "    load_throughput: 96203.81\n",
      "    load_time_ms: 29.105\n",
      "    sample_throughput: 194.572\n",
      "    sample_time_ms: 14390.568\n",
      "    update_time_ms: 3.276\n",
      "  timestamp: 1658497395\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 372400\n",
      "  training_iteration: 133\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   133</td><td style=\"text-align: right;\">         2473.36</td><td style=\"text-align: right;\">372400</td><td style=\"text-align: right;\">  -0.047</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            239.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 750400\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-43-33\n",
      "  done: false\n",
      "  episode_len_mean: 245.13\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.999999985098839\n",
      "  episode_reward_mean: -0.0809999968111515\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 1512\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.354141520247573\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011145204868431327\n",
      "          policy_loss: -0.04739157174606676\n",
      "          total_loss: -0.038756382332177874\n",
      "          vf_explained_var: -0.1405554562807083\n",
      "          vf_loss: 0.0035836572936074973\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.3704545199871063\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.012913376733860378\n",
      "          policy_loss: -0.05859719725863431\n",
      "          total_loss: -0.04522228745294602\n",
      "          vf_explained_var: -0.02573401667177677\n",
      "          vf_loss: 0.00413183720965684\n",
      "    num_agent_steps_sampled: 750400\n",
      "    num_agent_steps_trained: 750400\n",
      "    num_steps_sampled: 375200\n",
      "    num_steps_trained: 375200\n",
      "  iterations_since_restore: 134\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.970833333333334\n",
      "    gpu_util_percent0: 0.050416666666666665\n",
      "    ram_util_percent: 54.52916666666667\n",
      "    vram_util_percent0: 0.21656901041666665\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 1.9999999850988388\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.14699999995529653\n",
      "    agent_1: 0.06600000314414502\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05185609198623853\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8974935047165404\n",
      "    mean_inference_ms: 1.7804655052685365\n",
      "    mean_raw_obs_processing_ms: 0.15763108243526033\n",
      "  time_since_restore: 2491.7742063999176\n",
      "  time_this_iter_s: 18.415338277816772\n",
      "  time_total_s: 2491.7742063999176\n",
      "  timers:\n",
      "    learn_throughput: 652.373\n",
      "    learn_time_ms: 4292.025\n",
      "    load_throughput: 95750.372\n",
      "    load_time_ms: 29.243\n",
      "    sample_throughput: 194.945\n",
      "    sample_time_ms: 14363.059\n",
      "    update_time_ms: 3.241\n",
      "  timestamp: 1658497413\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 375200\n",
      "  training_iteration: 134\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   134</td><td style=\"text-align: right;\">         2491.77</td><td style=\"text-align: right;\">375200</td><td style=\"text-align: right;\">  -0.081</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            245.13</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 756000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-43-52\n",
      "  done: false\n",
      "  episode_len_mean: 244.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.999999985098839\n",
      "  episode_reward_mean: -0.09499999672174454\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 1524\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.3496137389114926\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.014624291910715723\n",
      "          policy_loss: -0.0869382015780069\n",
      "          total_loss: -0.07577829861769542\n",
      "          vf_explained_var: 0.04721546173095703\n",
      "          vf_loss: 0.00333676903704015\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.407335971792539\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.012234701605058013\n",
      "          policy_loss: -0.09814735332370869\n",
      "          total_loss: -0.08531976481269037\n",
      "          vf_explained_var: 0.07647975534200668\n",
      "          vf_loss: 0.0045506012345521455\n",
      "    num_agent_steps_sampled: 756000\n",
      "    num_agent_steps_trained: 756000\n",
      "    num_steps_sampled: 378000\n",
      "    num_steps_trained: 378000\n",
      "  iterations_since_restore: 135\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.854166666666667\n",
      "    gpu_util_percent0: 0.050416666666666665\n",
      "    ram_util_percent: 54.69166666666667\n",
      "    vram_util_percent0: 0.21753743489583333\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 1.9999999850988388\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.1569999999552965\n",
      "    agent_1: 0.06200000323355198\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051855204669652134\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.89752756775404\n",
      "    mean_inference_ms: 1.7804285909016033\n",
      "    mean_raw_obs_processing_ms: 0.15764279109130108\n",
      "  time_since_restore: 2510.1527881622314\n",
      "  time_this_iter_s: 18.378581762313843\n",
      "  time_total_s: 2510.1527881622314\n",
      "  timers:\n",
      "    learn_throughput: 653.55\n",
      "    learn_time_ms: 4284.296\n",
      "    load_throughput: 95771.767\n",
      "    load_time_ms: 29.236\n",
      "    sample_throughput: 195.56\n",
      "    sample_time_ms: 14317.878\n",
      "    update_time_ms: 3.214\n",
      "  timestamp: 1658497432\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 378000\n",
      "  training_iteration: 135\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   135</td><td style=\"text-align: right;\">         2510.15</td><td style=\"text-align: right;\">378000</td><td style=\"text-align: right;\">  -0.095</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             244.7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 761600\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-44-10\n",
      "  done: false\n",
      "  episode_len_mean: 246.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.999999985098839\n",
      "  episode_reward_mean: -0.11999999679625034\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 1533\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.375009358638809\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.016101241833060293\n",
      "          policy_loss: -0.07571159361430632\n",
      "          total_loss: -0.06341856164259038\n",
      "          vf_explained_var: -0.18637506663799286\n",
      "          vf_loss: 0.0034394759972894377\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.4074718923795793\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.012833007605980511\n",
      "          policy_loss: -0.05984497010684295\n",
      "          total_loss: -0.046619442647040286\n",
      "          vf_explained_var: 0.22266119718551636\n",
      "          vf_loss: 0.003978783480479684\n",
      "    num_agent_steps_sampled: 761600\n",
      "    num_agent_steps_trained: 761600\n",
      "    num_steps_sampled: 380800\n",
      "    num_steps_trained: 380800\n",
      "  iterations_since_restore: 136\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.0\n",
      "    gpu_util_percent0: 0.05125\n",
      "    ram_util_percent: 54.616666666666674\n",
      "    vram_util_percent0: 0.21706136067708334\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 1.9999999850988388\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.16699999995529652\n",
      "    agent_1: 0.04700000315904617\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0518532198882927\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8974706287168477\n",
      "    mean_inference_ms: 1.7803578546236043\n",
      "    mean_raw_obs_processing_ms: 0.15764575144825008\n",
      "  time_since_restore: 2528.317188024521\n",
      "  time_this_iter_s: 18.16439986228943\n",
      "  time_total_s: 2528.317188024521\n",
      "  timers:\n",
      "    learn_throughput: 652.485\n",
      "    learn_time_ms: 4291.283\n",
      "    load_throughput: 95800.673\n",
      "    load_time_ms: 29.227\n",
      "    sample_throughput: 196.584\n",
      "    sample_time_ms: 14243.289\n",
      "    update_time_ms: 3.221\n",
      "  timestamp: 1658497450\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 380800\n",
      "  training_iteration: 136\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   136</td><td style=\"text-align: right;\">         2528.32</td><td style=\"text-align: right;\">380800</td><td style=\"text-align: right;\">   -0.12</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             246.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 767200\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-44-28\n",
      "  done: false\n",
      "  episode_len_mean: 248.13\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.999999985098839\n",
      "  episode_reward_mean: -0.12199999682605267\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1544\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.343845376656169\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.014777088575247468\n",
      "          policy_loss: -0.07040323095060601\n",
      "          total_loss: -0.05859299132723771\n",
      "          vf_explained_var: 0.030823588371276855\n",
      "          vf_loss: 0.004859275748631695\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.346749804559208\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011203730247214242\n",
      "          policy_loss: -0.06958002782491912\n",
      "          total_loss: -0.05738245961956203\n",
      "          vf_explained_var: 0.05696321651339531\n",
      "          vf_loss: 0.005623252905674495\n",
      "    num_agent_steps_sampled: 767200\n",
      "    num_agent_steps_trained: 767200\n",
      "    num_steps_sampled: 383600\n",
      "    num_steps_trained: 383600\n",
      "  iterations_since_restore: 137\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.8500000000000005\n",
      "    gpu_util_percent0: 0.055833333333333325\n",
      "    ram_util_percent: 54.5\n",
      "    vram_util_percent0: 0.21752115885416665\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 1.9999999850988388\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.16699999995529652\n",
      "    agent_1: 0.04500000312924385\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05184939558205169\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.897295455559485\n",
      "    mean_inference_ms: 1.780223174898613\n",
      "    mean_raw_obs_processing_ms: 0.157653073488957\n",
      "  time_since_restore: 2546.5134196281433\n",
      "  time_this_iter_s: 18.196231603622437\n",
      "  time_total_s: 2546.5134196281433\n",
      "  timers:\n",
      "    learn_throughput: 656.042\n",
      "    learn_time_ms: 4268.021\n",
      "    load_throughput: 95815.289\n",
      "    load_time_ms: 29.223\n",
      "    sample_throughput: 197.327\n",
      "    sample_time_ms: 14189.612\n",
      "    update_time_ms: 3.199\n",
      "  timestamp: 1658497468\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 383600\n",
      "  training_iteration: 137\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   137</td><td style=\"text-align: right;\">         2546.51</td><td style=\"text-align: right;\">383600</td><td style=\"text-align: right;\">  -0.122</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            248.13</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 772800\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-44-47\n",
      "  done: false\n",
      "  episode_len_mean: 254.09\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.999999985098839\n",
      "  episode_reward_mean: -0.11899999670684337\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 1551\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.380124128290585\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.012926280951568236\n",
      "          policy_loss: -0.07557897574034064\n",
      "          total_loss: -0.06556327006477486\n",
      "          vf_explained_var: -0.2878701686859131\n",
      "          vf_loss: 0.003742127653735917\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.3938293620234443\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.0112595013445051\n",
      "          policy_loss: -0.05523716862877509\n",
      "          total_loss: -0.04316654135549297\n",
      "          vf_explained_var: -0.017473328858613968\n",
      "          vf_loss: 0.005157557627431483\n",
      "    num_agent_steps_sampled: 772800\n",
      "    num_agent_steps_trained: 772800\n",
      "    num_steps_sampled: 386400\n",
      "    num_steps_trained: 386400\n",
      "  iterations_since_restore: 138\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.884000000000001\n",
      "    gpu_util_percent0: 0.056799999999999996\n",
      "    ram_util_percent: 54.5\n",
      "    vram_util_percent0: 0.21748437499999998\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 1.9999999850988388\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.16699999995529652\n",
      "    agent_1: 0.04800000324845314\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051846228161453886\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.897179330090105\n",
      "    mean_inference_ms: 1.7801186259066022\n",
      "    mean_raw_obs_processing_ms: 0.15764661213615397\n",
      "  time_since_restore: 2565.0551402568817\n",
      "  time_this_iter_s: 18.541720628738403\n",
      "  time_total_s: 2565.0551402568817\n",
      "  timers:\n",
      "    learn_throughput: 656.419\n",
      "    learn_time_ms: 4265.567\n",
      "    load_throughput: 95988.753\n",
      "    load_time_ms: 29.17\n",
      "    sample_throughput: 197.876\n",
      "    sample_time_ms: 14150.266\n",
      "    update_time_ms: 3.229\n",
      "  timestamp: 1658497487\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 386400\n",
      "  training_iteration: 138\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   138</td><td style=\"text-align: right;\">         2565.06</td><td style=\"text-align: right;\">386400</td><td style=\"text-align: right;\">  -0.119</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            254.09</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 778400\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-45-05\n",
      "  done: false\n",
      "  episode_len_mean: 266.66\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.9000000134110451\n",
      "  episode_reward_mean: -0.1949999964982271\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1562\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.3558270519688014\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.013464874636047466\n",
      "          policy_loss: -0.06957036877117519\n",
      "          total_loss: -0.05848384854072633\n",
      "          vf_explained_var: 0.07067008316516876\n",
      "          vf_loss: 0.005616973189847029\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.386628383327098\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010937041048775097\n",
      "          policy_loss: -0.07896735510439612\n",
      "          total_loss: -0.0665651899596144\n",
      "          vf_explained_var: 0.15238025784492493\n",
      "          vf_loss: 0.00701595455480163\n",
      "    num_agent_steps_sampled: 778400\n",
      "    num_agent_steps_trained: 778400\n",
      "    num_steps_sampled: 389200\n",
      "    num_steps_trained: 389200\n",
      "  iterations_since_restore: 139\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.808333333333333\n",
      "    gpu_util_percent0: 0.0525\n",
      "    ram_util_percent: 54.5\n",
      "    vram_util_percent0: 0.21656901041666665\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.9000000134110451\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.2079999999701977\n",
      "    agent_1: 0.013000003471970558\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051841020664984555\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8970068832924034\n",
      "    mean_inference_ms: 1.7799492865213267\n",
      "    mean_raw_obs_processing_ms: 0.1576120822375266\n",
      "  time_since_restore: 2583.486969232559\n",
      "  time_this_iter_s: 18.43182897567749\n",
      "  time_total_s: 2583.486969232559\n",
      "  timers:\n",
      "    learn_throughput: 658.69\n",
      "    learn_time_ms: 4250.863\n",
      "    load_throughput: 95933.161\n",
      "    load_time_ms: 29.187\n",
      "    sample_throughput: 198.151\n",
      "    sample_time_ms: 14130.659\n",
      "    update_time_ms: 3.253\n",
      "  timestamp: 1658497505\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 389200\n",
      "  training_iteration: 139\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   139</td><td style=\"text-align: right;\">         2583.49</td><td style=\"text-align: right;\">389200</td><td style=\"text-align: right;\">  -0.195</td><td style=\"text-align: right;\">                 0.9</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            266.66</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 784000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-45-24\n",
      "  done: false\n",
      "  episode_len_mean: 268.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.9000000134110451\n",
      "  episode_reward_mean: -0.17399999648332595\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 1574\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.329377132512274\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.016122190760000203\n",
      "          policy_loss: -0.0802752869910494\n",
      "          total_loss: -0.06824906429669465\n",
      "          vf_explained_var: 0.15050499141216278\n",
      "          vf_loss: 0.0025787844613746743\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.361593840732461\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.014179274756876007\n",
      "          policy_loss: -0.06944578612733278\n",
      "          total_loss: -0.05446840713210847\n",
      "          vf_explained_var: 0.15033204853534698\n",
      "          vf_loss: 0.0050821174592697175\n",
      "    num_agent_steps_sampled: 784000\n",
      "    num_agent_steps_trained: 784000\n",
      "    num_steps_sampled: 392000\n",
      "    num_steps_trained: 392000\n",
      "  iterations_since_restore: 140\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.866666666666667\n",
      "    gpu_util_percent0: 0.050416666666666665\n",
      "    ram_util_percent: 54.49583333333334\n",
      "    vram_util_percent0: 0.21753743489583333\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.9000000134110451\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.19799999997019768\n",
      "    agent_1: 0.02400000348687172\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05183474566840018\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8968067544763296\n",
      "    mean_inference_ms: 1.779723171198935\n",
      "    mean_raw_obs_processing_ms: 0.1575765673436279\n",
      "  time_since_restore: 2601.878061771393\n",
      "  time_this_iter_s: 18.391092538833618\n",
      "  time_total_s: 2601.878061771393\n",
      "  timers:\n",
      "    learn_throughput: 661.205\n",
      "    learn_time_ms: 4234.691\n",
      "    load_throughput: 95974.32\n",
      "    load_time_ms: 29.174\n",
      "    sample_throughput: 198.271\n",
      "    sample_time_ms: 14122.054\n",
      "    update_time_ms: 3.212\n",
      "  timestamp: 1658497524\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 392000\n",
      "  training_iteration: 140\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   140</td><td style=\"text-align: right;\">         2601.88</td><td style=\"text-align: right;\">392000</td><td style=\"text-align: right;\">  -0.174</td><td style=\"text-align: right;\">                 0.9</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             268.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 789600\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-45-42\n",
      "  done: false\n",
      "  episode_len_mean: 269.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.9000000134110451\n",
      "  episode_reward_mean: -0.12899999640882015\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1584\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.372585254056113\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.014555092655993281\n",
      "          policy_loss: -0.07075777286220164\n",
      "          total_loss: -0.06030102733833095\n",
      "          vf_explained_var: 0.24055840075016022\n",
      "          vf_loss: 0.0015049117290843804\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.3769373577975093\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01501748302305769\n",
      "          policy_loss: -0.06254811447399247\n",
      "          total_loss: -0.04786837257588992\n",
      "          vf_explained_var: 0.09551937133073807\n",
      "          vf_loss: 0.0018578743531959066\n",
      "    num_agent_steps_sampled: 789600\n",
      "    num_agent_steps_trained: 789600\n",
      "    num_steps_sampled: 394800\n",
      "    num_steps_trained: 394800\n",
      "  iterations_since_restore: 141\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.858333333333333\n",
      "    gpu_util_percent0: 0.04958333333333333\n",
      "    ram_util_percent: 54.5\n",
      "    vram_util_percent0: 0.21675618489583334\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.9000000134110451\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.17799999997019766\n",
      "    agent_1: 0.04900000356137753\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05182888129869406\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.896634007528097\n",
      "    mean_inference_ms: 1.7795034608394662\n",
      "    mean_raw_obs_processing_ms: 0.1575363010471081\n",
      "  time_since_restore: 2620.2671477794647\n",
      "  time_this_iter_s: 18.3890860080719\n",
      "  time_total_s: 2620.2671477794647\n",
      "  timers:\n",
      "    learn_throughput: 664.036\n",
      "    learn_time_ms: 4216.641\n",
      "    load_throughput: 96076.154\n",
      "    load_time_ms: 29.144\n",
      "    sample_throughput: 198.472\n",
      "    sample_time_ms: 14107.766\n",
      "    update_time_ms: 3.174\n",
      "  timestamp: 1658497542\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 394800\n",
      "  training_iteration: 141\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   141</td><td style=\"text-align: right;\">         2620.27</td><td style=\"text-align: right;\">394800</td><td style=\"text-align: right;\">  -0.129</td><td style=\"text-align: right;\">                 0.9</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            269.37</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 795200\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-46-01\n",
      "  done: false\n",
      "  episode_len_mean: 273.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.9000000134110451\n",
      "  episode_reward_mean: -0.12999999642372131\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1594\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.307034630505812\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011724793189757002\n",
      "          policy_loss: -0.06099133123435812\n",
      "          total_loss: -0.051972448490449186\n",
      "          vf_explained_var: 0.10599362105131149\n",
      "          vf_loss: 0.003382136578043823\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.376988712520826\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011616782640517163\n",
      "          policy_loss: -0.06537909518582248\n",
      "          total_loss: -0.05331286510391649\n",
      "          vf_explained_var: -0.13027571141719818\n",
      "          vf_loss: 0.0041050991817644985\n",
      "    num_agent_steps_sampled: 795200\n",
      "    num_agent_steps_trained: 795200\n",
      "    num_steps_sampled: 397600\n",
      "    num_steps_trained: 397600\n",
      "  iterations_since_restore: 142\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.895833333333333\n",
      "    gpu_util_percent0: 0.05291666666666667\n",
      "    ram_util_percent: 54.5\n",
      "    vram_util_percent0: 0.2173095703125\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 0.9000000134110451\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.17799999997019766\n",
      "    agent_1: 0.048000003546476364\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0518225585683884\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8964456751439163\n",
      "    mean_inference_ms: 1.7792782187814793\n",
      "    mean_raw_obs_processing_ms: 0.15748389544134306\n",
      "  time_since_restore: 2638.628291606903\n",
      "  time_this_iter_s: 18.361143827438354\n",
      "  time_total_s: 2638.628291606903\n",
      "  timers:\n",
      "    learn_throughput: 666.347\n",
      "    learn_time_ms: 4202.017\n",
      "    load_throughput: 95877.319\n",
      "    load_time_ms: 29.204\n",
      "    sample_throughput: 198.816\n",
      "    sample_time_ms: 14083.352\n",
      "    update_time_ms: 3.203\n",
      "  timestamp: 1658497561\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 397600\n",
      "  training_iteration: 142\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   142</td><td style=\"text-align: right;\">         2638.63</td><td style=\"text-align: right;\">397600</td><td style=\"text-align: right;\">   -0.13</td><td style=\"text-align: right;\">                 0.9</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            273.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 800800\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-46-19\n",
      "  done: false\n",
      "  episode_len_mean: 281.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000044703484\n",
      "  episode_reward_mean: -0.16199999615550043\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1605\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.3562001387278237\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010108555993410026\n",
      "          policy_loss: -0.05314267033429046\n",
      "          total_loss: -0.041593157017468046\n",
      "          vf_explained_var: -0.36703065037727356\n",
      "          vf_loss: 0.014124978510759926\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.345907067968732\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.009727463616101509\n",
      "          policy_loss: -0.05695442187633099\n",
      "          total_loss: -0.04463746019783208\n",
      "          vf_explained_var: -0.005768366623669863\n",
      "          vf_loss: 0.010177327183295606\n",
      "    num_agent_steps_sampled: 800800\n",
      "    num_agent_steps_trained: 800800\n",
      "    num_steps_sampled: 400400\n",
      "    num_steps_trained: 400400\n",
      "  iterations_since_restore: 143\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.983333333333333\n",
      "    gpu_util_percent0: 0.04791666666666667\n",
      "    ram_util_percent: 54.54583333333333\n",
      "    vram_util_percent0: 0.2177164713541667\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000447034836\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.18799999997019767\n",
      "    agent_1: 0.026000003814697265\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05181561896703591\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8962616071817466\n",
      "    mean_inference_ms: 1.7790368308916045\n",
      "    mean_raw_obs_processing_ms: 0.1574176609589954\n",
      "  time_since_restore: 2657.004679441452\n",
      "  time_this_iter_s: 18.37638783454895\n",
      "  time_total_s: 2657.004679441452\n",
      "  timers:\n",
      "    learn_throughput: 666.891\n",
      "    learn_time_ms: 4198.587\n",
      "    load_throughput: 95805.127\n",
      "    load_time_ms: 29.226\n",
      "    sample_throughput: 198.798\n",
      "    sample_time_ms: 14084.619\n",
      "    update_time_ms: 3.203\n",
      "  timestamp: 1658497579\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 400400\n",
      "  training_iteration: 143\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   143</td><td style=\"text-align: right;\">            2657</td><td style=\"text-align: right;\">400400</td><td style=\"text-align: right;\">  -0.162</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            281.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 806400\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-46-37\n",
      "  done: false\n",
      "  episode_len_mean: 265.14\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000044703484\n",
      "  episode_reward_mean: -0.1769999960064888\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 1619\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.3533532434985753\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01386746249432157\n",
      "          policy_loss: -0.07414412411994167\n",
      "          total_loss: -0.062477018418056626\n",
      "          vf_explained_var: -0.28378021717071533\n",
      "          vf_loss: 0.006409466324753123\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.3171144943861735\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011487941349545958\n",
      "          policy_loss: -0.07579762957365824\n",
      "          total_loss: -0.06265007595253078\n",
      "          vf_explained_var: 0.18671409785747528\n",
      "          vf_loss: 0.007488625018595485\n",
      "    num_agent_steps_sampled: 806400\n",
      "    num_agent_steps_trained: 806400\n",
      "    num_steps_sampled: 403200\n",
      "    num_steps_trained: 403200\n",
      "  iterations_since_restore: 144\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.920833333333334\n",
      "    gpu_util_percent0: 0.05333333333333334\n",
      "    ram_util_percent: 54.5\n",
      "    vram_util_percent0: 0.21675618489583334\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000447034836\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.2\n",
      "    agent_1: 0.0230000039935112\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051807291047704956\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8960551904969374\n",
      "    mean_inference_ms: 1.7787510263248425\n",
      "    mean_raw_obs_processing_ms: 0.15737633220334046\n",
      "  time_since_restore: 2675.365950345993\n",
      "  time_this_iter_s: 18.361270904541016\n",
      "  time_total_s: 2675.365950345993\n",
      "  timers:\n",
      "    learn_throughput: 667.165\n",
      "    learn_time_ms: 4196.861\n",
      "    load_throughput: 96270.29\n",
      "    load_time_ms: 29.085\n",
      "    sample_throughput: 198.847\n",
      "    sample_time_ms: 14081.19\n",
      "    update_time_ms: 3.21\n",
      "  timestamp: 1658497597\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 403200\n",
      "  training_iteration: 144\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   144</td><td style=\"text-align: right;\">         2675.37</td><td style=\"text-align: right;\">403200</td><td style=\"text-align: right;\">  -0.177</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            265.14</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 812000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-46-56\n",
      "  done: false\n",
      "  episode_len_mean: 262.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000044703484\n",
      "  episode_reward_mean: -0.11799999549984933\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1630\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.227258692894663\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.016351677712165905\n",
      "          policy_loss: -0.10129119115853905\n",
      "          total_loss: -0.08808469339523331\n",
      "          vf_explained_var: -0.009257611818611622\n",
      "          vf_loss: 0.005335513934759157\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.3059668409682454\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.013829848528371816\n",
      "          policy_loss: -0.08500812303377427\n",
      "          total_loss: -0.07051299988441835\n",
      "          vf_explained_var: 0.41936060786247253\n",
      "          vf_loss: 0.004636890986321738\n",
      "    num_agent_steps_sampled: 812000\n",
      "    num_agent_steps_trained: 812000\n",
      "    num_steps_sampled: 406000\n",
      "    num_steps_trained: 406000\n",
      "  iterations_since_restore: 145\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.8599999999999985\n",
      "    gpu_util_percent0: 0.057999999999999996\n",
      "    ram_util_percent: 54.5\n",
      "    vram_util_percent0: 0.21748437499999998\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000447034836\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.17399999976158143\n",
      "    agent_1: 0.0560000042617321\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051802087735162594\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8959473951700807\n",
      "    mean_inference_ms: 1.778585886042748\n",
      "    mean_raw_obs_processing_ms: 0.15734935544915574\n",
      "  time_since_restore: 2694.0822143554688\n",
      "  time_this_iter_s: 18.716264009475708\n",
      "  time_total_s: 2694.0822143554688\n",
      "  timers:\n",
      "    learn_throughput: 666.719\n",
      "    learn_time_ms: 4199.671\n",
      "    load_throughput: 89311.227\n",
      "    load_time_ms: 31.351\n",
      "    sample_throughput: 198.446\n",
      "    sample_time_ms: 14109.607\n",
      "    update_time_ms: 3.195\n",
      "  timestamp: 1658497616\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 406000\n",
      "  training_iteration: 145\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   145</td><td style=\"text-align: right;\">         2694.08</td><td style=\"text-align: right;\">406000</td><td style=\"text-align: right;\">  -0.118</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            262.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 817600\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-47-15\n",
      "  done: false\n",
      "  episode_len_mean: 261.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000044703484\n",
      "  episode_reward_mean: -0.09199999541044235\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1641\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.2475088380631947\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.018680368322142055\n",
      "          policy_loss: -0.10490285456416175\n",
      "          total_loss: -0.09078345354092085\n",
      "          vf_explained_var: 0.3301009237766266\n",
      "          vf_loss: 0.0029798510471058157\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.309719716863973\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.013859278242084777\n",
      "          policy_loss: -0.09181803269200914\n",
      "          total_loss: -0.07771703029755458\n",
      "          vf_explained_var: 0.2588645815849304\n",
      "          vf_loss: 0.0034322450555053017\n",
      "    num_agent_steps_sampled: 817600\n",
      "    num_agent_steps_trained: 817600\n",
      "    num_steps_sampled: 408800\n",
      "    num_steps_trained: 408800\n",
      "  iterations_since_restore: 146\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.9\n",
      "    gpu_util_percent0: 0.0588\n",
      "    ram_util_percent: 54.523999999999994\n",
      "    vram_util_percent0: 0.21685156249999998\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000447034836\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.16399999976158142\n",
      "    agent_1: 0.07200000435113907\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051798520631929745\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.895885078437168\n",
      "    mean_inference_ms: 1.778489815089999\n",
      "    mean_raw_obs_processing_ms: 0.15733068317228016\n",
      "  time_since_restore: 2712.8231549263\n",
      "  time_this_iter_s: 18.7409405708313\n",
      "  time_total_s: 2712.8231549263\n",
      "  timers:\n",
      "    learn_throughput: 667.485\n",
      "    learn_time_ms: 4194.848\n",
      "    load_throughput: 89609.503\n",
      "    load_time_ms: 31.247\n",
      "    sample_throughput: 197.57\n",
      "    sample_time_ms: 14172.193\n",
      "    update_time_ms: 3.227\n",
      "  timestamp: 1658497635\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 408800\n",
      "  training_iteration: 146\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   146</td><td style=\"text-align: right;\">         2712.82</td><td style=\"text-align: right;\">408800</td><td style=\"text-align: right;\">  -0.092</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            261.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 823200\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-47-34\n",
      "  done: false\n",
      "  episode_len_mean: 239.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000044703484\n",
      "  episode_reward_mean: -0.012999995201826096\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 1656\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.3240839803502675\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01375515023770316\n",
      "          policy_loss: -0.0746189248722138\n",
      "          total_loss: -0.06411821302147776\n",
      "          vf_explained_var: -0.07936656475067139\n",
      "          vf_loss: 0.0032855160430439617\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.3227325833979107\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.012936842035559185\n",
      "          policy_loss: -0.08413671726555635\n",
      "          total_loss: -0.07041444765491178\n",
      "          vf_explained_var: 0.308156818151474\n",
      "          vf_loss: 0.004999790908013459\n",
      "    num_agent_steps_sampled: 823200\n",
      "    num_agent_steps_trained: 823200\n",
      "    num_steps_sampled: 411600\n",
      "    num_steps_trained: 411600\n",
      "  iterations_since_restore: 147\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.870833333333334\n",
      "    gpu_util_percent0: 0.050833333333333335\n",
      "    ram_util_percent: 54.59166666666667\n",
      "    vram_util_percent0: 0.2173502604166667\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000447034836\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.13399999976158142\n",
      "    agent_1: 0.12100000455975532\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05179583562775645\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8958073164780864\n",
      "    mean_inference_ms: 1.7784227785201407\n",
      "    mean_raw_obs_processing_ms: 0.15735742740797115\n",
      "  time_since_restore: 2731.486307144165\n",
      "  time_this_iter_s: 18.66315221786499\n",
      "  time_total_s: 2731.486307144165\n",
      "  timers:\n",
      "    learn_throughput: 666.566\n",
      "    learn_time_ms: 4200.632\n",
      "    load_throughput: 89419.349\n",
      "    load_time_ms: 31.313\n",
      "    sample_throughput: 197.002\n",
      "    sample_time_ms: 14213.088\n",
      "    update_time_ms: 3.237\n",
      "  timestamp: 1658497654\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 411600\n",
      "  training_iteration: 147\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   147</td><td style=\"text-align: right;\">         2731.49</td><td style=\"text-align: right;\">411600</td><td style=\"text-align: right;\">  -0.013</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            239.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 828800\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-47-52\n",
      "  done: false\n",
      "  episode_len_mean: 242.65\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000044703484\n",
      "  episode_reward_mean: -0.02799999512732029\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1666\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.2913501791301227\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.012721865779793454\n",
      "          policy_loss: -0.07713714989880655\n",
      "          total_loss: -0.06751368302795006\n",
      "          vf_explained_var: -0.19655227661132812\n",
      "          vf_loss: 0.002954678123863968\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.377348853009088\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.012319642702956993\n",
      "          policy_loss: -0.09878310057961028\n",
      "          total_loss: -0.08607864181976765\n",
      "          vf_explained_var: 0.01601925864815712\n",
      "          vf_loss: 0.003921025297174873\n",
      "    num_agent_steps_sampled: 828800\n",
      "    num_agent_steps_trained: 828800\n",
      "    num_steps_sampled: 414400\n",
      "    num_steps_trained: 414400\n",
      "  iterations_since_restore: 148\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.876\n",
      "    gpu_util_percent0: 0.0584\n",
      "    ram_util_percent: 54.56799999999999\n",
      "    vram_util_percent0: 0.21748437499999998\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000447034836\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.14399999976158143\n",
      "    agent_1: 0.11600000463426113\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05179544829239896\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.895789080836577\n",
      "    mean_inference_ms: 1.7784337527202296\n",
      "    mean_raw_obs_processing_ms: 0.15737450363896133\n",
      "  time_since_restore: 2750.267248392105\n",
      "  time_this_iter_s: 18.780941247940063\n",
      "  time_total_s: 2750.267248392105\n",
      "  timers:\n",
      "    learn_throughput: 668.021\n",
      "    learn_time_ms: 4191.487\n",
      "    load_throughput: 89512.449\n",
      "    load_time_ms: 31.281\n",
      "    sample_throughput: 196.542\n",
      "    sample_time_ms: 14246.294\n",
      "    update_time_ms: 3.24\n",
      "  timestamp: 1658497672\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 414400\n",
      "  training_iteration: 148\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   148</td><td style=\"text-align: right;\">         2750.27</td><td style=\"text-align: right;\">414400</td><td style=\"text-align: right;\">  -0.028</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            242.65</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 834400\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-48-11\n",
      "  done: false\n",
      "  episode_len_mean: 251.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000044703484\n",
      "  episode_reward_mean: -0.03199999518692494\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 1672\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.326724945789292\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.016074087049900956\n",
      "          policy_loss: -0.05695411810925829\n",
      "          total_loss: -0.04516265956668316\n",
      "          vf_explained_var: -0.0002389010915067047\n",
      "          vf_loss: 0.0020084591106491905\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.3549781753903343\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.015139027246133157\n",
      "          policy_loss: -0.04686359530898612\n",
      "          total_loss: -0.03207568729305335\n",
      "          vf_explained_var: 0.28071480989456177\n",
      "          vf_loss: 0.0017936063685781499\n",
      "    num_agent_steps_sampled: 834400\n",
      "    num_agent_steps_trained: 834400\n",
      "    num_steps_sampled: 417200\n",
      "    num_steps_trained: 417200\n",
      "  iterations_since_restore: 149\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.979166666666667\n",
      "    gpu_util_percent0: 0.04958333333333333\n",
      "    ram_util_percent: 54.604166666666664\n",
      "    vram_util_percent0: 0.21689860026041666\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000447034836\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.14399999976158143\n",
      "    agent_1: 0.11200000457465649\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05179574039242029\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8957948917488405\n",
      "    mean_inference_ms: 1.7784627538482034\n",
      "    mean_raw_obs_processing_ms: 0.15737119238008493\n",
      "  time_since_restore: 2768.958574295044\n",
      "  time_this_iter_s: 18.691325902938843\n",
      "  time_total_s: 2768.958574295044\n",
      "  timers:\n",
      "    learn_throughput: 668.45\n",
      "    learn_time_ms: 4188.794\n",
      "    load_throughput: 89283.049\n",
      "    load_time_ms: 31.361\n",
      "    sample_throughput: 196.147\n",
      "    sample_time_ms: 14274.998\n",
      "    update_time_ms: 3.201\n",
      "  timestamp: 1658497691\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 417200\n",
      "  training_iteration: 149\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   149</td><td style=\"text-align: right;\">         2768.96</td><td style=\"text-align: right;\">417200</td><td style=\"text-align: right;\">  -0.032</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            251.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 840000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-48-30\n",
      "  done: false\n",
      "  episode_len_mean: 247.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000044703484\n",
      "  episode_reward_mean: -0.09199999511241913\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 1684\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.3326001344692138\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01127673149278626\n",
      "          policy_loss: -0.057094778444609674\n",
      "          total_loss: -0.04785793354849808\n",
      "          vf_explained_var: -0.1740422248840332\n",
      "          vf_loss: 0.00499400353570915\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.3465129867905663\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011266043560316373\n",
      "          policy_loss: -0.0677434958471611\n",
      "          total_loss: -0.05529402401346791\n",
      "          vf_explained_var: -0.08491069823503494\n",
      "          vf_loss: 0.006164182214672023\n",
      "    num_agent_steps_sampled: 840000\n",
      "    num_agent_steps_trained: 840000\n",
      "    num_steps_sampled: 420000\n",
      "    num_steps_trained: 420000\n",
      "  iterations_since_restore: 150\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.924000000000001\n",
      "    gpu_util_percent0: 0.0544\n",
      "    ram_util_percent: 54.624\n",
      "    vram_util_percent0: 0.21748437499999998\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000447034836\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.17399999976158143\n",
      "    agent_1: 0.0820000046491623\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05179782405057413\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.895836026032197\n",
      "    mean_inference_ms: 1.778569029386757\n",
      "    mean_raw_obs_processing_ms: 0.15737692511506426\n",
      "  time_since_restore: 2787.785473585129\n",
      "  time_this_iter_s: 18.82689929008484\n",
      "  time_total_s: 2787.785473585129\n",
      "  timers:\n",
      "    learn_throughput: 665.338\n",
      "    learn_time_ms: 4208.387\n",
      "    load_throughput: 89588.244\n",
      "    load_time_ms: 31.254\n",
      "    sample_throughput: 195.813\n",
      "    sample_time_ms: 14299.373\n",
      "    update_time_ms: 3.234\n",
      "  timestamp: 1658497710\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 420000\n",
      "  training_iteration: 150\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   150</td><td style=\"text-align: right;\">         2787.79</td><td style=\"text-align: right;\">420000</td><td style=\"text-align: right;\">  -0.092</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            247.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 845600\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-48-48\n",
      "  done: false\n",
      "  episode_len_mean: 253.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000044703484\n",
      "  episode_reward_mean: -0.04299999497830868\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 1693\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.407241042880785\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.017190636698442757\n",
      "          policy_loss: -0.10902963590792968\n",
      "          total_loss: -0.09657857593889016\n",
      "          vf_explained_var: 0.15702486038208008\n",
      "          vf_loss: 0.0015964653577958373\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.3799401372671127\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.013374706673350405\n",
      "          policy_loss: -0.11082973045332827\n",
      "          total_loss: -0.09769147184366982\n",
      "          vf_explained_var: 0.3885337710380554\n",
      "          vf_loss: 0.002150591237209266\n",
      "    num_agent_steps_sampled: 845600\n",
      "    num_agent_steps_trained: 845600\n",
      "    num_steps_sampled: 422800\n",
      "    num_steps_trained: 422800\n",
      "  iterations_since_restore: 151\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.9\n",
      "    gpu_util_percent0: 0.061200000000000004\n",
      "    ram_util_percent: 54.775999999999996\n",
      "    vram_util_percent0: 0.21670312500000002\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000447034836\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.1539999997615814\n",
      "    agent_1: 0.11100000478327274\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05180034404665426\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.895887796546181\n",
      "    mean_inference_ms: 1.7786813207436483\n",
      "    mean_raw_obs_processing_ms: 0.15738112480132851\n",
      "  time_since_restore: 2806.2875361442566\n",
      "  time_this_iter_s: 18.502062559127808\n",
      "  time_total_s: 2806.2875361442566\n",
      "  timers:\n",
      "    learn_throughput: 664.527\n",
      "    learn_time_ms: 4213.526\n",
      "    load_throughput: 89634.877\n",
      "    load_time_ms: 31.238\n",
      "    sample_throughput: 195.775\n",
      "    sample_time_ms: 14302.101\n",
      "    update_time_ms: 5.408\n",
      "  timestamp: 1658497728\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 422800\n",
      "  training_iteration: 151\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   151</td><td style=\"text-align: right;\">         2806.29</td><td style=\"text-align: right;\">422800</td><td style=\"text-align: right;\">  -0.043</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            253.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 851200\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-49-07\n",
      "  done: false\n",
      "  episode_len_mean: 258.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: -0.011999994665384293\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1703\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.4188882935614813\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010458516076682451\n",
      "          policy_loss: -0.04689029642066175\n",
      "          total_loss: -0.03471283754463782\n",
      "          vf_explained_var: -0.35110175609588623\n",
      "          vf_loss: 0.015242726310437623\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.38741269530285\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010063015451393108\n",
      "          policy_loss: -0.06300603599326375\n",
      "          total_loss: -0.0513342686643314\n",
      "          vf_explained_var: 0.2158432900905609\n",
      "          vf_loss: 0.007426899575356012\n",
      "    num_agent_steps_sampled: 851200\n",
      "    num_agent_steps_trained: 851200\n",
      "    num_steps_sampled: 425600\n",
      "    num_steps_trained: 425600\n",
      "  iterations_since_restore: 152\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.883333333333333\n",
      "    gpu_util_percent0: 0.050833333333333335\n",
      "    ram_util_percent: 54.65416666666667\n",
      "    vram_util_percent0: 0.21721191406250004\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.14399999976158143\n",
      "    agent_1: 0.13200000509619714\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05180365500929903\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8959532371718355\n",
      "    mean_inference_ms: 1.778848435147644\n",
      "    mean_raw_obs_processing_ms: 0.15738367833431172\n",
      "  time_since_restore: 2824.8928792476654\n",
      "  time_this_iter_s: 18.605343103408813\n",
      "  time_total_s: 2824.8928792476654\n",
      "  timers:\n",
      "    learn_throughput: 663.605\n",
      "    learn_time_ms: 4219.38\n",
      "    load_throughput: 89728.563\n",
      "    load_time_ms: 31.205\n",
      "    sample_throughput: 195.52\n",
      "    sample_time_ms: 14320.771\n",
      "    update_time_ms: 5.395\n",
      "  timestamp: 1658497747\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 425600\n",
      "  training_iteration: 152\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   152</td><td style=\"text-align: right;\">         2824.89</td><td style=\"text-align: right;\">425600</td><td style=\"text-align: right;\">  -0.012</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            258.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 856800\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-49-26\n",
      "  done: false\n",
      "  episode_len_mean: 259.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.01000000536441803\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1714\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.340760947692962\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011738705620528943\n",
      "          policy_loss: -0.0610078355689655\n",
      "          total_loss: -0.05065202396958955\n",
      "          vf_explained_var: 0.009184173308312893\n",
      "          vf_loss: 0.007208887321948645\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.3455508102973304\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011556799381021403\n",
      "          policy_loss: -0.0653082660189847\n",
      "          total_loss: -0.05184317417054748\n",
      "          vf_explained_var: 0.4093526303768158\n",
      "          vf_loss: 0.008232286272340432\n",
      "    num_agent_steps_sampled: 856800\n",
      "    num_agent_steps_trained: 856800\n",
      "    num_steps_sampled: 428400\n",
      "    num_steps_trained: 428400\n",
      "  iterations_since_restore: 153\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.872000000000001\n",
      "    gpu_util_percent0: 0.056799999999999996\n",
      "    ram_util_percent: 54.512\n",
      "    vram_util_percent0: 0.2174765625\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.13399999976158142\n",
      "    agent_1: 0.14400000512599945\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05180820433244495\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8960391241087815\n",
      "    mean_inference_ms: 1.7790721235582163\n",
      "    mean_raw_obs_processing_ms: 0.1573764478955361\n",
      "  time_since_restore: 2843.626933336258\n",
      "  time_this_iter_s: 18.73405408859253\n",
      "  time_total_s: 2843.626933336258\n",
      "  timers:\n",
      "    learn_throughput: 664.623\n",
      "    learn_time_ms: 4212.913\n",
      "    load_throughput: 89734.185\n",
      "    load_time_ms: 31.203\n",
      "    sample_throughput: 194.946\n",
      "    sample_time_ms: 14362.922\n",
      "    update_time_ms: 5.43\n",
      "  timestamp: 1658497766\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 428400\n",
      "  training_iteration: 153\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   153</td><td style=\"text-align: right;\">         2843.63</td><td style=\"text-align: right;\">428400</td><td style=\"text-align: right;\">    0.01</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            259.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 862400\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-49-45\n",
      "  done: false\n",
      "  episode_len_mean: 260.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: -0.024999995082616806\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 1726\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.335094464676721\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.012688347693712056\n",
      "          policy_loss: -0.08169212698010683\n",
      "          total_loss: -0.07155293416387092\n",
      "          vf_explained_var: -0.042207587510347366\n",
      "          vf_loss: 0.004550599671433918\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.3232651647357714\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011983164667445947\n",
      "          policy_loss: -0.09592088146781039\n",
      "          total_loss: -0.0828427528720654\n",
      "          vf_explained_var: 0.13758644461631775\n",
      "          vf_loss: 0.0058840537016554405\n",
      "    num_agent_steps_sampled: 862400\n",
      "    num_agent_steps_trained: 862400\n",
      "    num_steps_sampled: 431200\n",
      "    num_steps_trained: 431200\n",
      "  iterations_since_restore: 154\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.8625\n",
      "    gpu_util_percent0: 0.050416666666666665\n",
      "    ram_util_percent: 54.5\n",
      "    vram_util_percent0: 0.21656901041666665\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.15\n",
      "    agent_1: 0.1250000049173832\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05181354382958691\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8961639131674897\n",
      "    mean_inference_ms: 1.7793198378163693\n",
      "    mean_raw_obs_processing_ms: 0.15737135198245739\n",
      "  time_since_restore: 2862.4199295043945\n",
      "  time_this_iter_s: 18.792996168136597\n",
      "  time_total_s: 2862.4199295043945\n",
      "  timers:\n",
      "    learn_throughput: 663.455\n",
      "    learn_time_ms: 4220.334\n",
      "    load_throughput: 89781.108\n",
      "    load_time_ms: 31.187\n",
      "    sample_throughput: 194.462\n",
      "    sample_time_ms: 14398.694\n",
      "    update_time_ms: 5.445\n",
      "  timestamp: 1658497785\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 431200\n",
      "  training_iteration: 154\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   154</td><td style=\"text-align: right;\">         2862.42</td><td style=\"text-align: right;\">431200</td><td style=\"text-align: right;\">  -0.025</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            260.97</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 868000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-50-03\n",
      "  done: false\n",
      "  episode_len_mean: 262.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: -0.019999995008111\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1737\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.3766110142072043\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.014055809944394466\n",
      "          policy_loss: -0.07023062294857005\n",
      "          total_loss: -0.06013176831921945\n",
      "          vf_explained_var: -0.2370021492242813\n",
      "          vf_loss: 0.001556998930070785\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.3409885196458724\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01222799034886781\n",
      "          policy_loss: -0.07478289776606419\n",
      "          total_loss: -0.06259142553157983\n",
      "          vf_explained_var: 0.1309642344713211\n",
      "          vf_loss: 0.0026750896980350567\n",
      "    num_agent_steps_sampled: 868000\n",
      "    num_agent_steps_trained: 868000\n",
      "    num_steps_sampled: 434000\n",
      "    num_steps_trained: 434000\n",
      "  iterations_since_restore: 155\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.944\n",
      "    gpu_util_percent0: 0.061200000000000004\n",
      "    ram_util_percent: 54.544\n",
      "    vram_util_percent0: 0.21756640625000004\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.15\n",
      "    agent_1: 0.130000004991889\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051818060152909344\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8962841159978243\n",
      "    mean_inference_ms: 1.7795322023058702\n",
      "    mean_raw_obs_processing_ms: 0.15736620373104673\n",
      "  time_since_restore: 2881.170166015625\n",
      "  time_this_iter_s: 18.75023651123047\n",
      "  time_total_s: 2881.170166015625\n",
      "  timers:\n",
      "    learn_throughput: 662.545\n",
      "    learn_time_ms: 4226.131\n",
      "    load_throughput: 97026.52\n",
      "    load_time_ms: 28.858\n",
      "    sample_throughput: 194.461\n",
      "    sample_time_ms: 14398.746\n",
      "    update_time_ms: 5.469\n",
      "  timestamp: 1658497803\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 434000\n",
      "  training_iteration: 155\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   155</td><td style=\"text-align: right;\">         2881.17</td><td style=\"text-align: right;\">434000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            262.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 873600\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-50-22\n",
      "  done: false\n",
      "  episode_len_mean: 266.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000067055225\n",
      "  episode_reward_mean: 0.007000005543231964\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1748\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.3560660027322315\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010443954897550515\n",
      "          policy_loss: -0.04700021655142856\n",
      "          total_loss: -0.03697732993470838\n",
      "          vf_explained_var: -0.016334345564246178\n",
      "          vf_loss: 0.009048725268387767\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.3046414646364393\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011486755501542233\n",
      "          policy_loss: -0.07177312753317112\n",
      "          total_loss: -0.05935726032337351\n",
      "          vf_explained_var: 0.40229859948158264\n",
      "          vf_loss: 0.005388486747224739\n",
      "    num_agent_steps_sampled: 873600\n",
      "    num_agent_steps_trained: 873600\n",
      "    num_steps_sampled: 436800\n",
      "    num_steps_trained: 436800\n",
      "  iterations_since_restore: 156\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.156000000000001\n",
      "    gpu_util_percent0: 0.0556\n",
      "    ram_util_percent: 54.604\n",
      "    vram_util_percent0: 0.2169140625\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000670552254\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.13\n",
      "    agent_1: 0.13700000554323197\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05182229672059017\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8964482667515967\n",
      "    mean_inference_ms: 1.779743454350848\n",
      "    mean_raw_obs_processing_ms: 0.15734865284637448\n",
      "  time_since_restore: 2899.9924499988556\n",
      "  time_this_iter_s: 18.82228398323059\n",
      "  time_total_s: 2899.9924499988556\n",
      "  timers:\n",
      "    learn_throughput: 661.195\n",
      "    learn_time_ms: 4234.757\n",
      "    load_throughput: 96685.277\n",
      "    load_time_ms: 28.96\n",
      "    sample_throughput: 194.47\n",
      "    sample_time_ms: 14398.119\n",
      "    update_time_ms: 5.456\n",
      "  timestamp: 1658497822\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 436800\n",
      "  training_iteration: 156\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   156</td><td style=\"text-align: right;\">         2899.99</td><td style=\"text-align: right;\">436800</td><td style=\"text-align: right;\">0.00700001</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            266.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 879200\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-50-41\n",
      "  done: false\n",
      "  episode_len_mean: 257.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000067055225\n",
      "  episode_reward_mean: 0.08900000616908073\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 1763\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.3331019402969453\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010114425242064554\n",
      "          policy_loss: -0.04937325067328943\n",
      "          total_loss: -0.03777761484074983\n",
      "          vf_explained_var: -0.45578765869140625\n",
      "          vf_loss: 0.014216658365555867\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.2894849120861007\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010904261190020494\n",
      "          policy_loss: -0.07407699340997385\n",
      "          total_loss: -0.06175786721308084\n",
      "          vf_explained_var: 0.1402866542339325\n",
      "          vf_loss: 0.006757153402889214\n",
      "    num_agent_steps_sampled: 879200\n",
      "    num_agent_steps_trained: 879200\n",
      "    num_steps_sampled: 439600\n",
      "    num_steps_trained: 439600\n",
      "  iterations_since_restore: 157\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.8500000000000005\n",
      "    gpu_util_percent0: 0.049166666666666664\n",
      "    ram_util_percent: 54.6\n",
      "    vram_util_percent0: 0.21715494791666667\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000670552254\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.09\n",
      "    agent_1: 0.17900000616908074\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051826471543009955\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.896652694150865\n",
      "    mean_inference_ms: 1.7799596838807548\n",
      "    mean_raw_obs_processing_ms: 0.15734034290297835\n",
      "  time_since_restore: 2918.6168496608734\n",
      "  time_this_iter_s: 18.624399662017822\n",
      "  time_total_s: 2918.6168496608734\n",
      "  timers:\n",
      "    learn_throughput: 658.913\n",
      "    learn_time_ms: 4249.422\n",
      "    load_throughput: 96726.845\n",
      "    load_time_ms: 28.947\n",
      "    sample_throughput: 194.721\n",
      "    sample_time_ms: 14379.559\n",
      "    update_time_ms: 5.453\n",
      "  timestamp: 1658497841\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 439600\n",
      "  training_iteration: 157\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   157</td><td style=\"text-align: right;\">         2918.62</td><td style=\"text-align: right;\">439600</td><td style=\"text-align: right;\">   0.089</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            257.39</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 884800\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-50-59\n",
      "  done: false\n",
      "  episode_len_mean: 239.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000067055225\n",
      "  episode_reward_mean: 0.13300000600516795\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 1777\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.262329774243491\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.013298396392352614\n",
      "          policy_loss: -0.06911812707576678\n",
      "          total_loss: -0.05612232394298103\n",
      "          vf_explained_var: -0.14823664724826813\n",
      "          vf_loss: 0.011312763395835645\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.247049688228539\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010475467058404039\n",
      "          policy_loss: -0.06000633583711793\n",
      "          total_loss: -0.04678301859946389\n",
      "          vf_explained_var: 0.18682178854942322\n",
      "          vf_loss: 0.010512002148711222\n",
      "    num_agent_steps_sampled: 884800\n",
      "    num_agent_steps_trained: 884800\n",
      "    num_steps_sampled: 442400\n",
      "    num_steps_trained: 442400\n",
      "  iterations_since_restore: 158\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.908333333333334\n",
      "    gpu_util_percent0: 0.04958333333333333\n",
      "    ram_util_percent: 54.6\n",
      "    vram_util_percent0: 0.21753743489583333\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000670552254\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.06599999994039535\n",
      "    agent_1: 0.19900000594556333\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05182803272074559\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.896759996931538\n",
      "    mean_inference_ms: 1.7800813615822948\n",
      "    mean_raw_obs_processing_ms: 0.1573691912624052\n",
      "  time_since_restore: 2937.046996116638\n",
      "  time_this_iter_s: 18.43014645576477\n",
      "  time_total_s: 2937.046996116638\n",
      "  timers:\n",
      "    learn_throughput: 657.977\n",
      "    learn_time_ms: 4255.47\n",
      "    load_throughput: 96705.34\n",
      "    load_time_ms: 28.954\n",
      "    sample_throughput: 195.284\n",
      "    sample_time_ms: 14338.113\n",
      "    update_time_ms: 5.456\n",
      "  timestamp: 1658497859\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 442400\n",
      "  training_iteration: 158\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   158</td><td style=\"text-align: right;\">         2937.05</td><td style=\"text-align: right;\">442400</td><td style=\"text-align: right;\">   0.133</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            239.37</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 890400\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-51-18\n",
      "  done: false\n",
      "  episode_len_mean: 244.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000067055225\n",
      "  episode_reward_mean: 0.17300000593066214\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1787\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.75\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.2928015419415066\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.020390607082858697\n",
      "          policy_loss: -0.09833756265585353\n",
      "          total_loss: -0.08333998587719786\n",
      "          vf_explained_var: 0.09112092852592468\n",
      "          vf_loss: 0.0018789137013213587\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.2855411395430565\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.015376204543587827\n",
      "          policy_loss: -0.07873873925945234\n",
      "          total_loss: -0.06361977430545569\n",
      "          vf_explained_var: 0.3864465355873108\n",
      "          vf_loss: 0.0019791734823575964\n",
      "    num_agent_steps_sampled: 890400\n",
      "    num_agent_steps_trained: 890400\n",
      "    num_steps_sampled: 445200\n",
      "    num_steps_trained: 445200\n",
      "  iterations_since_restore: 159\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.871999999999999\n",
      "    gpu_util_percent0: 0.0652\n",
      "    ram_util_percent: 54.6\n",
      "    vram_util_percent0: 0.21670312500000002\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000670552254\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.044999999925494194\n",
      "    agent_1: 0.21800000585615634\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05182774404211479\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8967844739438124\n",
      "    mean_inference_ms: 1.7801157712005482\n",
      "    mean_raw_obs_processing_ms: 0.15738746349652316\n",
      "  time_since_restore: 2955.470237016678\n",
      "  time_this_iter_s: 18.423240900039673\n",
      "  time_total_s: 2955.470237016678\n",
      "  timers:\n",
      "    learn_throughput: 655.53\n",
      "    learn_time_ms: 4271.355\n",
      "    load_throughput: 97000.635\n",
      "    load_time_ms: 28.866\n",
      "    sample_throughput: 195.878\n",
      "    sample_time_ms: 14294.592\n",
      "    update_time_ms: 5.702\n",
      "  timestamp: 1658497878\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 445200\n",
      "  training_iteration: 159\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   159</td><td style=\"text-align: right;\">         2955.47</td><td style=\"text-align: right;\">445200</td><td style=\"text-align: right;\">   0.173</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            244.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 896000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-51-36\n",
      "  done: false\n",
      "  episode_len_mean: 229.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000067055225\n",
      "  episode_reward_mean: 0.21600000612437725\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 1800\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.3139682362476983\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01399530496085585\n",
      "          policy_loss: -0.10595240350951263\n",
      "          total_loss: -0.09064150681106617\n",
      "          vf_explained_var: -0.02017752267420292\n",
      "          vf_loss: 0.001508843200489147\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.2290189188151133\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.014300029127887985\n",
      "          policy_loss: -0.08823136381536079\n",
      "          total_loss: -0.07410075072073682\n",
      "          vf_explained_var: 0.1339113414287567\n",
      "          vf_loss: 0.002162761579321731\n",
      "    num_agent_steps_sampled: 896000\n",
      "    num_agent_steps_trained: 896000\n",
      "    num_steps_sampled: 448000\n",
      "    num_steps_trained: 448000\n",
      "  iterations_since_restore: 160\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.8625\n",
      "    gpu_util_percent0: 0.05416666666666666\n",
      "    ram_util_percent: 54.6\n",
      "    vram_util_percent0: 0.2173502604166667\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000670552254\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.024999999925494194\n",
      "    agent_1: 0.24100000604987146\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05182636386555608\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8967937706420988\n",
      "    mean_inference_ms: 1.7800926366959464\n",
      "    mean_raw_obs_processing_ms: 0.15742862657923112\n",
      "  time_since_restore: 2974.0162541866302\n",
      "  time_this_iter_s: 18.546017169952393\n",
      "  time_total_s: 2974.0162541866302\n",
      "  timers:\n",
      "    learn_throughput: 655.23\n",
      "    learn_time_ms: 4273.306\n",
      "    load_throughput: 96594.461\n",
      "    load_time_ms: 28.987\n",
      "    sample_throughput: 196.294\n",
      "    sample_time_ms: 14264.348\n",
      "    update_time_ms: 5.718\n",
      "  timestamp: 1658497896\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 448000\n",
      "  training_iteration: 160\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   160</td><td style=\"text-align: right;\">         2974.02</td><td style=\"text-align: right;\">448000</td><td style=\"text-align: right;\">   0.216</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            229.97</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 901600\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-51-55\n",
      "  done: false\n",
      "  episode_len_mean: 228.21\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000067055225\n",
      "  episode_reward_mean: 0.2120000058412552\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 1813\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.3482868607555116\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.008579278034222955\n",
      "          policy_loss: -0.11795339045654275\n",
      "          total_loss: -0.10845756899687417\n",
      "          vf_explained_var: -0.22241275012493134\n",
      "          vf_loss: 0.002343049185583368\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.2753366352546784\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.009156649014220001\n",
      "          policy_loss: -0.12388062763438072\n",
      "          total_loss: -0.11456240383822228\n",
      "          vf_explained_var: 0.1224537119269371\n",
      "          vf_loss: 0.0031626250620320207\n",
      "    num_agent_steps_sampled: 901600\n",
      "    num_agent_steps_trained: 901600\n",
      "    num_steps_sampled: 450800\n",
      "    num_steps_trained: 450800\n",
      "  iterations_since_restore: 161\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.875\n",
      "    gpu_util_percent0: 0.048749999999999995\n",
      "    ram_util_percent: 54.583333333333336\n",
      "    vram_util_percent0: 0.21675618489583334\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000670552254\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.03299999989569187\n",
      "    agent_1: 0.24500000573694705\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051823929797816956\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8967524225845436\n",
      "    mean_inference_ms: 1.78000798903308\n",
      "    mean_raw_obs_processing_ms: 0.15748010782445335\n",
      "  time_since_restore: 2992.4915335178375\n",
      "  time_this_iter_s: 18.475279331207275\n",
      "  time_total_s: 2992.4915335178375\n",
      "  timers:\n",
      "    learn_throughput: 653.011\n",
      "    learn_time_ms: 4287.827\n",
      "    load_throughput: 89850.415\n",
      "    load_time_ms: 31.163\n",
      "    sample_throughput: 196.514\n",
      "    sample_time_ms: 14248.373\n",
      "    update_time_ms: 3.557\n",
      "  timestamp: 1658497915\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 450800\n",
      "  training_iteration: 161\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   161</td><td style=\"text-align: right;\">         2992.49</td><td style=\"text-align: right;\">450800</td><td style=\"text-align: right;\">   0.212</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            228.21</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 907200\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-52-14\n",
      "  done: false\n",
      "  episode_len_mean: 221.65\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000067055225\n",
      "  episode_reward_mean: 0.2510000064969063\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 1828\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.330701216700531\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.008052995086561892\n",
      "          policy_loss: -0.11589486217347994\n",
      "          total_loss: -0.10595724486657196\n",
      "          vf_explained_var: -0.48974037170410156\n",
      "          vf_loss: 0.005273504990652457\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.219969575603803\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.008338497330092346\n",
      "          policy_loss: -0.1162067935580299\n",
      "          total_loss: -0.10703646387195304\n",
      "          vf_explained_var: 0.17093995213508606\n",
      "          vf_loss: 0.0050102548149793405\n",
      "    num_agent_steps_sampled: 907200\n",
      "    num_agent_steps_trained: 907200\n",
      "    num_steps_sampled: 453600\n",
      "    num_steps_trained: 453600\n",
      "  iterations_since_restore: 162\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.94\n",
      "    gpu_util_percent0: 0.0576\n",
      "    ram_util_percent: 54.647999999999996\n",
      "    vram_util_percent0: 0.21749999999999997\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000670552254\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.015999999418854712\n",
      "    agent_1: 0.267000005915761\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05181959187985323\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8966425715344686\n",
      "    mean_inference_ms: 1.7798495408807964\n",
      "    mean_raw_obs_processing_ms: 0.1575534175033513\n",
      "  time_since_restore: 3011.144392967224\n",
      "  time_this_iter_s: 18.652859449386597\n",
      "  time_total_s: 3011.144392967224\n",
      "  timers:\n",
      "    learn_throughput: 650.953\n",
      "    learn_time_ms: 4301.386\n",
      "    load_throughput: 89921.826\n",
      "    load_time_ms: 31.138\n",
      "    sample_throughput: 196.637\n",
      "    sample_time_ms: 14239.442\n",
      "    update_time_ms: 3.553\n",
      "  timestamp: 1658497934\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 453600\n",
      "  training_iteration: 162\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   162</td><td style=\"text-align: right;\">         3011.14</td><td style=\"text-align: right;\">453600</td><td style=\"text-align: right;\">   0.251</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            221.65</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 912800\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-52-32\n",
      "  done: false\n",
      "  episode_len_mean: 225.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000067055225\n",
      "  episode_reward_mean: 0.23100000649690627\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 1836\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.3144190339815047\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.016255260431601748\n",
      "          policy_loss: -0.07849044698234918\n",
      "          total_loss: -0.060277667236819286\n",
      "          vf_explained_var: -0.1225258857011795\n",
      "          vf_loss: 0.002535437387580584\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.324911133164451\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.015966359369647035\n",
      "          policy_loss: -0.08870779777117561\n",
      "          total_loss: -0.07274504329793022\n",
      "          vf_explained_var: -0.024735337123274803\n",
      "          vf_loss: 0.002749947056361721\n",
      "    num_agent_steps_sampled: 912800\n",
      "    num_agent_steps_trained: 912800\n",
      "    num_steps_sampled: 456400\n",
      "    num_steps_trained: 456400\n",
      "  iterations_since_restore: 163\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.904166666666666\n",
      "    gpu_util_percent0: 0.05875\n",
      "    ram_util_percent: 54.61250000000001\n",
      "    vram_util_percent0: 0.2173502604166667\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000670552254\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.025999999418854714\n",
      "    agent_1: 0.257000005915761\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051816753557399495\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8965484109622603\n",
      "    mean_inference_ms: 1.7797347355660367\n",
      "    mean_raw_obs_processing_ms: 0.15758108316630914\n",
      "  time_since_restore: 3029.5315222740173\n",
      "  time_this_iter_s: 18.387129306793213\n",
      "  time_total_s: 3029.5315222740173\n",
      "  timers:\n",
      "    learn_throughput: 648.956\n",
      "    learn_time_ms: 4314.622\n",
      "    load_throughput: 89617.709\n",
      "    load_time_ms: 31.244\n",
      "    sample_throughput: 197.299\n",
      "    sample_time_ms: 14191.629\n",
      "    update_time_ms: 3.512\n",
      "  timestamp: 1658497952\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 456400\n",
      "  training_iteration: 163\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   163</td><td style=\"text-align: right;\">         3029.53</td><td style=\"text-align: right;\">456400</td><td style=\"text-align: right;\">   0.231</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            225.97</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 918400\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-52-51\n",
      "  done: false\n",
      "  episode_len_mean: 226.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.18200000613927841\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 1848\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.360599806975751\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010760426444611589\n",
      "          policy_loss: -0.06138980426892106\n",
      "          total_loss: -0.049508234841812826\n",
      "          vf_explained_var: -0.17131344974040985\n",
      "          vf_loss: 0.0021634325778774966\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.2713927743690356\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.012383886514007422\n",
      "          policy_loss: -0.0837474799991902\n",
      "          total_loss: -0.07119347422987976\n",
      "          vf_explained_var: 0.20729906857013702\n",
      "          vf_loss: 0.0031823346900550645\n",
      "    num_agent_steps_sampled: 918400\n",
      "    num_agent_steps_trained: 918400\n",
      "    num_steps_sampled: 459200\n",
      "    num_steps_trained: 459200\n",
      "  iterations_since_restore: 164\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.900000000000001\n",
      "    gpu_util_percent0: 0.049166666666666664\n",
      "    ram_util_percent: 54.69166666666667\n",
      "    vram_util_percent0: 0.21656901041666665\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.053999999389052394\n",
      "    agent_1: 0.2360000055283308\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05181130265741337\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.896344315905724\n",
      "    mean_inference_ms: 1.7794950634363642\n",
      "    mean_raw_obs_processing_ms: 0.1576240085971207\n",
      "  time_since_restore: 3048.010790348053\n",
      "  time_this_iter_s: 18.479268074035645\n",
      "  time_total_s: 3048.010790348053\n",
      "  timers:\n",
      "    learn_throughput: 646.764\n",
      "    learn_time_ms: 4329.243\n",
      "    load_throughput: 89490.827\n",
      "    load_time_ms: 31.288\n",
      "    sample_throughput: 197.941\n",
      "    sample_time_ms: 14145.657\n",
      "    update_time_ms: 3.515\n",
      "  timestamp: 1658497971\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 459200\n",
      "  training_iteration: 164\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   164</td><td style=\"text-align: right;\">         3048.01</td><td style=\"text-align: right;\">459200</td><td style=\"text-align: right;\">   0.182</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             226.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 924000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-53-09\n",
      "  done: false\n",
      "  episode_len_mean: 225.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.999999985098839\n",
      "  episode_reward_mean: 0.08700000531971455\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 1860\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.2993311055359387\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.008935214611302159\n",
      "          policy_loss: -0.05913838273694897\n",
      "          total_loss: -0.048745235231416745\n",
      "          vf_explained_var: -0.19475755095481873\n",
      "          vf_loss: 0.0037034016671181496\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.2312879218232062\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.012410044758169451\n",
      "          policy_loss: -0.06065981225373478\n",
      "          total_loss: -0.04717516871009832\n",
      "          vf_explained_var: -0.0913810059428215\n",
      "          vf_loss: 0.005716713927291667\n",
      "    num_agent_steps_sampled: 924000\n",
      "    num_agent_steps_trained: 924000\n",
      "    num_steps_sampled: 462000\n",
      "    num_steps_trained: 462000\n",
      "  iterations_since_restore: 165\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.8839999999999995\n",
      "    gpu_util_percent0: 0.0556\n",
      "    ram_util_percent: 54.62\n",
      "    vram_util_percent0: 0.21748437499999998\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 1.9999999850988388\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.09399999938905239\n",
      "    agent_1: 0.18100000470876693\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05180586970167028\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8961213915534234\n",
      "    mean_inference_ms: 1.7792554423259126\n",
      "    mean_raw_obs_processing_ms: 0.15765150777210382\n",
      "  time_since_restore: 3066.627482175827\n",
      "  time_this_iter_s: 18.616691827774048\n",
      "  time_total_s: 3066.627482175827\n",
      "  timers:\n",
      "    learn_throughput: 645.149\n",
      "    learn_time_ms: 4340.08\n",
      "    load_throughput: 89331.947\n",
      "    load_time_ms: 31.344\n",
      "    sample_throughput: 198.281\n",
      "    sample_time_ms: 14121.362\n",
      "    update_time_ms: 3.534\n",
      "  timestamp: 1658497989\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 462000\n",
      "  training_iteration: 165\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   165</td><td style=\"text-align: right;\">         3066.63</td><td style=\"text-align: right;\">462000</td><td style=\"text-align: right;\">   0.087</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            225.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 929600\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-53-28\n",
      "  done: false\n",
      "  episode_len_mean: 233.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.5000000596046448\n",
      "  episode_reward_mean: 0.013000005558133125\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 1872\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.3834172082798823\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.009775370744109406\n",
      "          policy_loss: -0.057328562379725986\n",
      "          total_loss: -0.045478715893945525\n",
      "          vf_explained_var: -0.04203557223081589\n",
      "          vf_loss: 0.005263463909494974\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.396402798947834\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.009525874449247114\n",
      "          policy_loss: -0.0529106332178344\n",
      "          total_loss: -0.042129128728956425\n",
      "          vf_explained_var: -0.12046205252408981\n",
      "          vf_loss: 0.006429521869059889\n",
      "    num_agent_steps_sampled: 929600\n",
      "    num_agent_steps_trained: 929600\n",
      "    num_steps_sampled: 464800\n",
      "    num_steps_trained: 464800\n",
      "  iterations_since_restore: 166\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.866666666666667\n",
      "    gpu_util_percent0: 0.059166666666666666\n",
      "    ram_util_percent: 54.6\n",
      "    vram_util_percent0: 0.21656901041666665\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.7000000476837158\n",
      "    agent_1: 0.8000000193715096\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.1359999994188547\n",
      "    agent_1: 0.14900000497698784\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05180021959912546\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8958450360105017\n",
      "    mean_inference_ms: 1.7790050192833637\n",
      "    mean_raw_obs_processing_ms: 0.1576725574032977\n",
      "  time_since_restore: 3084.8917334079742\n",
      "  time_this_iter_s: 18.264251232147217\n",
      "  time_total_s: 3084.8917334079742\n",
      "  timers:\n",
      "    learn_throughput: 645.205\n",
      "    learn_time_ms: 4339.708\n",
      "    load_throughput: 89367.975\n",
      "    load_time_ms: 31.331\n",
      "    sample_throughput: 199.062\n",
      "    sample_time_ms: 14065.961\n",
      "    update_time_ms: 3.543\n",
      "  timestamp: 1658498008\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 464800\n",
      "  training_iteration: 166\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   166</td><td style=\"text-align: right;\">         3084.89</td><td style=\"text-align: right;\">464800</td><td style=\"text-align: right;\">   0.013</td><td style=\"text-align: right;\">                 1.5</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            233.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 935200\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-53-46\n",
      "  done: false\n",
      "  episode_len_mean: 240.49\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.5000000596046448\n",
      "  episode_reward_mean: 0.03700000561773777\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1882\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.2960016667133285\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010091886838409352\n",
      "          policy_loss: -0.08378765335822079\n",
      "          total_loss: -0.07256747165073259\n",
      "          vf_explained_var: 0.13017091155052185\n",
      "          vf_loss: 0.0023457070221324797\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.373692899942398\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.013136610804477715\n",
      "          policy_loss: -0.08124752655671078\n",
      "          total_loss: -0.06805124332285709\n",
      "          vf_explained_var: -0.010280606336891651\n",
      "          vf_loss: 0.0029885065599782614\n",
      "    num_agent_steps_sampled: 935200\n",
      "    num_agent_steps_trained: 935200\n",
      "    num_steps_sampled: 467600\n",
      "    num_steps_trained: 467600\n",
      "  iterations_since_restore: 167\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.983333333333334\n",
      "    gpu_util_percent0: 0.059166666666666666\n",
      "    ram_util_percent: 54.887499999999996\n",
      "    vram_util_percent0: 0.21715494791666667\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.7000000476837158\n",
      "    agent_1: 0.8000000193715096\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.12799999944865703\n",
      "    agent_1: 0.1650000050663948\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05179568756550632\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.895623106353938\n",
      "    mean_inference_ms: 1.7788000324374786\n",
      "    mean_raw_obs_processing_ms: 0.15768569799108975\n",
      "  time_since_restore: 3103.451904773712\n",
      "  time_this_iter_s: 18.560171365737915\n",
      "  time_total_s: 3103.451904773712\n",
      "  timers:\n",
      "    learn_throughput: 645.197\n",
      "    learn_time_ms: 4339.762\n",
      "    load_throughput: 89461.581\n",
      "    load_time_ms: 31.298\n",
      "    sample_throughput: 199.156\n",
      "    sample_time_ms: 14059.315\n",
      "    update_time_ms: 3.546\n",
      "  timestamp: 1658498026\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 467600\n",
      "  training_iteration: 167\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   167</td><td style=\"text-align: right;\">         3103.45</td><td style=\"text-align: right;\">467600</td><td style=\"text-align: right;\">   0.037</td><td style=\"text-align: right;\">                 1.5</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            240.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 940800\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-54-05\n",
      "  done: false\n",
      "  episode_len_mean: 239.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.5000000596046448\n",
      "  episode_reward_mean: 0.021000005677342413\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1892\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.2407265844799222\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.012941904930181441\n",
      "          policy_loss: -0.07042336030571761\n",
      "          total_loss: -0.05596943598196785\n",
      "          vf_explained_var: -0.1863097846508026\n",
      "          vf_loss: 0.002358507820082152\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.2785183638334274\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01239762058622173\n",
      "          policy_loss: -0.08368555244868774\n",
      "          total_loss: -0.07106210394912134\n",
      "          vf_explained_var: 0.2665455639362335\n",
      "          vf_loss: 0.0033498183112377383\n",
      "    num_agent_steps_sampled: 940800\n",
      "    num_agent_steps_trained: 940800\n",
      "    num_steps_sampled: 470400\n",
      "    num_steps_trained: 470400\n",
      "  iterations_since_restore: 168\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.954166666666666\n",
      "    gpu_util_percent0: 0.050833333333333335\n",
      "    ram_util_percent: 54.78333333333333\n",
      "    vram_util_percent0: 0.2176513671875\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.7000000476837158\n",
      "    agent_1: 0.8000000193715096\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.1389999994635582\n",
      "    agent_1: 0.16000000514090063\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05179114039499158\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8954078324832113\n",
      "    mean_inference_ms: 1.7786067129694139\n",
      "    mean_raw_obs_processing_ms: 0.15769298613956587\n",
      "  time_since_restore: 3121.953488588333\n",
      "  time_this_iter_s: 18.50158381462097\n",
      "  time_total_s: 3121.953488588333\n",
      "  timers:\n",
      "    learn_throughput: 644.913\n",
      "    learn_time_ms: 4341.671\n",
      "    load_throughput: 89189.274\n",
      "    load_time_ms: 31.394\n",
      "    sample_throughput: 199.084\n",
      "    sample_time_ms: 14064.4\n",
      "    update_time_ms: 3.521\n",
      "  timestamp: 1658498045\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 470400\n",
      "  training_iteration: 168\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   168</td><td style=\"text-align: right;\">         3121.95</td><td style=\"text-align: right;\">470400</td><td style=\"text-align: right;\">   0.021</td><td style=\"text-align: right;\">                 1.5</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            239.47</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 946400\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-54-23\n",
      "  done: false\n",
      "  episode_len_mean: 250.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.5000000596046448\n",
      "  episode_reward_mean: -0.04799999453127384\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 1901\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.336986401251384\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010754007642538279\n",
      "          policy_loss: -0.06367075515888809\n",
      "          total_loss: -0.05081307018796603\n",
      "          vf_explained_var: -0.02723013237118721\n",
      "          vf_loss: 0.004942489048538673\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.3093950783922557\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.012348194242526206\n",
      "          policy_loss: -0.06819028002999923\n",
      "          total_loss: -0.054779919769793024\n",
      "          vf_explained_var: 0.33503180742263794\n",
      "          vf_loss: 0.005773952058526837\n",
      "    num_agent_steps_sampled: 946400\n",
      "    num_agent_steps_trained: 946400\n",
      "    num_steps_sampled: 473200\n",
      "    num_steps_trained: 473200\n",
      "  iterations_since_restore: 169\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.88\n",
      "    gpu_util_percent0: 0.0544\n",
      "    ram_util_percent: 54.70000000000001\n",
      "    vram_util_percent0: 0.21669921875\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.7000000476837158\n",
      "    agent_1: 0.8000000193715096\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.1659999994188547\n",
      "    agent_1: 0.11800000488758088\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05178671162125726\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8952701399178964\n",
      "    mean_inference_ms: 1.7784298572961905\n",
      "    mean_raw_obs_processing_ms: 0.15768789254354285\n",
      "  time_since_restore: 3140.644331216812\n",
      "  time_this_iter_s: 18.690842628479004\n",
      "  time_total_s: 3140.644331216812\n",
      "  timers:\n",
      "    learn_throughput: 646.308\n",
      "    learn_time_ms: 4332.303\n",
      "    load_throughput: 89066.439\n",
      "    load_time_ms: 31.437\n",
      "    sample_throughput: 198.563\n",
      "    sample_time_ms: 14101.299\n",
      "    update_time_ms: 3.282\n",
      "  timestamp: 1658498063\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 473200\n",
      "  training_iteration: 169\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   169</td><td style=\"text-align: right;\">         3140.64</td><td style=\"text-align: right;\">473200</td><td style=\"text-align: right;\">  -0.048</td><td style=\"text-align: right;\">                 1.5</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            250.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 952000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-54-42\n",
      "  done: false\n",
      "  episode_len_mean: 258.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.5000000596046448\n",
      "  episode_reward_mean: -0.029999994710087775\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 1910\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.314986965131192\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.014318648783091758\n",
      "          policy_loss: -0.06772027415449668\n",
      "          total_loss: -0.05209516620241283\n",
      "          vf_explained_var: 0.03068511188030243\n",
      "          vf_loss: 0.0013685985950793284\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.276073599855105\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.014926918039775042\n",
      "          policy_loss: -0.08005927478613255\n",
      "          total_loss: -0.0653390263337531\n",
      "          vf_explained_var: 0.16789714992046356\n",
      "          vf_loss: 0.002112289223684153\n",
      "    num_agent_steps_sampled: 952000\n",
      "    num_agent_steps_trained: 952000\n",
      "    num_steps_sampled: 476000\n",
      "    num_steps_trained: 476000\n",
      "  iterations_since_restore: 170\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.904166666666668\n",
      "    gpu_util_percent0: 0.049999999999999996\n",
      "    ram_util_percent: 54.645833333333336\n",
      "    vram_util_percent0: 0.2173502604166667\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.7000000476837158\n",
      "    agent_1: 0.8000000193715096\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.15799999944865703\n",
      "    agent_1: 0.12800000473856926\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05178238081594068\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8951630075151042\n",
      "    mean_inference_ms: 1.7782485165804314\n",
      "    mean_raw_obs_processing_ms: 0.157669575911856\n",
      "  time_since_restore: 3159.1631965637207\n",
      "  time_this_iter_s: 18.51886534690857\n",
      "  time_total_s: 3159.1631965637207\n",
      "  timers:\n",
      "    learn_throughput: 649.073\n",
      "    learn_time_ms: 4313.847\n",
      "    load_throughput: 89415.264\n",
      "    load_time_ms: 31.315\n",
      "    sample_throughput: 198.341\n",
      "    sample_time_ms: 14117.082\n",
      "    update_time_ms: 3.262\n",
      "  timestamp: 1658498082\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 476000\n",
      "  training_iteration: 170\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   170</td><td style=\"text-align: right;\">         3159.16</td><td style=\"text-align: right;\">476000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                 1.5</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             258.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 957600\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-55-00\n",
      "  done: false\n",
      "  episode_len_mean: 266.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.4000000357627869\n",
      "  episode_reward_mean: -0.006999994888901711\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1921\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.356451155174346\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011184060832387549\n",
      "          policy_loss: -0.07048375315311464\n",
      "          total_loss: -0.058212758226242535\n",
      "          vf_explained_var: -0.09028800576925278\n",
      "          vf_loss: 0.0019096720660573261\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.2626340591481755\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.014542629146441052\n",
      "          policy_loss: -0.08579503140000659\n",
      "          total_loss: -0.07166550654645627\n",
      "          vf_explained_var: 0.42087578773498535\n",
      "          vf_loss: 0.0015070286818150808\n",
      "    num_agent_steps_sampled: 957600\n",
      "    num_agent_steps_trained: 957600\n",
      "    num_steps_sampled: 478800\n",
      "    num_steps_trained: 478800\n",
      "  iterations_since_restore: 171\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.891666666666667\n",
      "    gpu_util_percent0: 0.050416666666666665\n",
      "    ram_util_percent: 54.6\n",
      "    vram_util_percent0: 0.21675618489583334\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.6000000238418579\n",
      "    agent_1: 0.9000000134110451\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.1489999996870756\n",
      "    agent_1: 0.1420000047981739\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051776946513865926\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.895002543205494\n",
      "    mean_inference_ms: 1.7780115062151713\n",
      "    mean_raw_obs_processing_ms: 0.15763440296873596\n",
      "  time_since_restore: 3177.286078929901\n",
      "  time_this_iter_s: 18.12288236618042\n",
      "  time_total_s: 3177.286078929901\n",
      "  timers:\n",
      "    learn_throughput: 651.444\n",
      "    learn_time_ms: 4298.142\n",
      "    load_throughput: 95881.311\n",
      "    load_time_ms: 29.203\n",
      "    sample_throughput: 198.588\n",
      "    sample_time_ms: 14099.573\n",
      "    update_time_ms: 3.252\n",
      "  timestamp: 1658498100\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 478800\n",
      "  training_iteration: 171\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   171</td><td style=\"text-align: right;\">         3177.29</td><td style=\"text-align: right;\">478800</td><td style=\"text-align: right;\">-0.00699999</td><td style=\"text-align: right;\">                 1.4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             266.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 963200\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-55-18\n",
      "  done: false\n",
      "  episode_len_mean: 273.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.4000000357627869\n",
      "  episode_reward_mean: 0.04000000521540642\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 1930\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.3374573779957637\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010133773283140066\n",
      "          policy_loss: -0.053157853716714144\n",
      "          total_loss: -0.0416231762771661\n",
      "          vf_explained_var: -0.11474292725324631\n",
      "          vf_loss: 0.0031581873630611447\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.276032882077353\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.009317561939041192\n",
      "          policy_loss: -0.04499109754947096\n",
      "          total_loss: -0.035511225062475116\n",
      "          vf_explained_var: 0.06117386370897293\n",
      "          vf_loss: 0.003165553393435922\n",
      "    num_agent_steps_sampled: 963200\n",
      "    num_agent_steps_trained: 963200\n",
      "    num_steps_sampled: 481600\n",
      "    num_steps_trained: 481600\n",
      "  iterations_since_restore: 172\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.8500000000000005\n",
      "    gpu_util_percent0: 0.050416666666666665\n",
      "    ram_util_percent: 54.61250000000001\n",
      "    vram_util_percent0: 0.2173095703125\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.6000000238418579\n",
      "    agent_1: 0.9000000134110451\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.12899999968707562\n",
      "    agent_1: 0.16900000490248204\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05177223780766976\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8948696791963835\n",
      "    mean_inference_ms: 1.777813616548963\n",
      "    mean_raw_obs_processing_ms: 0.15759722533903434\n",
      "  time_since_restore: 3195.5883526802063\n",
      "  time_this_iter_s: 18.302273750305176\n",
      "  time_total_s: 3195.5883526802063\n",
      "  timers:\n",
      "    learn_throughput: 654.583\n",
      "    learn_time_ms: 4277.535\n",
      "    load_throughput: 95868.71\n",
      "    load_time_ms: 29.207\n",
      "    sample_throughput: 198.789\n",
      "    sample_time_ms: 14085.287\n",
      "    update_time_ms: 3.274\n",
      "  timestamp: 1658498118\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 481600\n",
      "  training_iteration: 172\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   172</td><td style=\"text-align: right;\">         3195.59</td><td style=\"text-align: right;\">481600</td><td style=\"text-align: right;\">    0.04</td><td style=\"text-align: right;\">                 1.4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            273.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 968800\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-55-37\n",
      "  done: false\n",
      "  episode_len_mean: 267.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.4000000357627869\n",
      "  episode_reward_mean: 0.024000005125999452\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1941\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.3500500860668363\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.00848296180569376\n",
      "          policy_loss: -0.05482151925874253\n",
      "          total_loss: -0.04430767790767104\n",
      "          vf_explained_var: -0.3730628490447998\n",
      "          vf_loss: 0.005560569515550708\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.2571336385749636\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010079283104566517\n",
      "          policy_loss: -0.06752975358367189\n",
      "          total_loss: -0.056194510224269745\n",
      "          vf_explained_var: -0.11624312400817871\n",
      "          vf_loss: 0.006265126969949398\n",
      "    num_agent_steps_sampled: 968800\n",
      "    num_agent_steps_trained: 968800\n",
      "    num_steps_sampled: 484400\n",
      "    num_steps_trained: 484400\n",
      "  iterations_since_restore: 173\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.8839999999999995\n",
      "    gpu_util_percent0: 0.0572\n",
      "    ram_util_percent: 54.684000000000005\n",
      "    vram_util_percent0: 0.21748437499999998\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.6000000238418579\n",
      "    agent_1: 1.0000000149011612\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.1389999996870756\n",
      "    agent_1: 0.16300000481307506\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05176691438898602\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8947161910105605\n",
      "    mean_inference_ms: 1.7775981414741417\n",
      "    mean_raw_obs_processing_ms: 0.15756120114976865\n",
      "  time_since_restore: 3214.075151205063\n",
      "  time_this_iter_s: 18.486798524856567\n",
      "  time_total_s: 3214.075151205063\n",
      "  timers:\n",
      "    learn_throughput: 655.839\n",
      "    learn_time_ms: 4269.343\n",
      "    load_throughput: 96121.684\n",
      "    load_time_ms: 29.13\n",
      "    sample_throughput: 198.575\n",
      "    sample_time_ms: 14100.498\n",
      "    update_time_ms: 3.266\n",
      "  timestamp: 1658498137\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 484400\n",
      "  training_iteration: 173\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   173</td><td style=\"text-align: right;\">         3214.08</td><td style=\"text-align: right;\">484400</td><td style=\"text-align: right;\">   0.024</td><td style=\"text-align: right;\">                 1.4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            267.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 974400\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-55-55\n",
      "  done: false\n",
      "  episode_len_mean: 267.61\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.4000000581145287\n",
      "  episode_reward_mean: 0.042000005692243575\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 1955\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.3079315037244843\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01105675113144743\n",
      "          policy_loss: -0.05888239789982479\n",
      "          total_loss: -0.04651129560858992\n",
      "          vf_explained_var: -0.42278817296028137\n",
      "          vf_loss: 0.002546705026361451\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.2105853309233985\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.012277513585645449\n",
      "          policy_loss: -0.057066262086404355\n",
      "          total_loss: -0.04460499988704755\n",
      "          vf_explained_var: 0.34043335914611816\n",
      "          vf_loss: 0.0031490482276803093\n",
      "    num_agent_steps_sampled: 974400\n",
      "    num_agent_steps_trained: 974400\n",
      "    num_steps_sampled: 487200\n",
      "    num_steps_trained: 487200\n",
      "  iterations_since_restore: 174\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.891666666666667\n",
      "    gpu_util_percent0: 0.05541666666666667\n",
      "    ram_util_percent: 54.70000000000001\n",
      "    vram_util_percent0: 0.21656901041666665\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.7000000476837158\n",
      "    agent_1: 1.0000000149011612\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.13399999924004077\n",
      "    agent_1: 0.17600000493228435\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05176048572466997\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.894544794487577\n",
      "    mean_inference_ms: 1.7773471655906612\n",
      "    mean_raw_obs_processing_ms: 0.15752680333324612\n",
      "  time_since_restore: 3232.581695318222\n",
      "  time_this_iter_s: 18.50654411315918\n",
      "  time_total_s: 3232.581695318222\n",
      "  timers:\n",
      "    learn_throughput: 658.031\n",
      "    learn_time_ms: 4255.118\n",
      "    load_throughput: 96139.939\n",
      "    load_time_ms: 29.124\n",
      "    sample_throughput: 198.337\n",
      "    sample_time_ms: 14117.418\n",
      "    update_time_ms: 3.256\n",
      "  timestamp: 1658498155\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 487200\n",
      "  training_iteration: 174\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   174</td><td style=\"text-align: right;\">         3232.58</td><td style=\"text-align: right;\">487200</td><td style=\"text-align: right;\">   0.042</td><td style=\"text-align: right;\">                 1.4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            267.61</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 980000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-56-14\n",
      "  done: false\n",
      "  episode_len_mean: 265.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.4000000581145287\n",
      "  episode_reward_mean: 0.08300000570714473\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1966\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.3370957601638067\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.008672343983485709\n",
      "          policy_loss: -0.054938683982375856\n",
      "          total_loss: -0.044276502821525184\n",
      "          vf_explained_var: -0.37830373644828796\n",
      "          vf_loss: 0.005360450288034848\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.2614821284299804\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011443884075118527\n",
      "          policy_loss: -0.0538426667825271\n",
      "          total_loss: -0.040900195145739074\n",
      "          vf_explained_var: -0.049528978765010834\n",
      "          vf_loss: 0.00696290649596319\n",
      "    num_agent_steps_sampled: 980000\n",
      "    num_agent_steps_trained: 980000\n",
      "    num_steps_sampled: 490000\n",
      "    num_steps_trained: 490000\n",
      "  iterations_since_restore: 175\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.966666666666666\n",
      "    gpu_util_percent0: 0.049999999999999996\n",
      "    ram_util_percent: 54.725\n",
      "    vram_util_percent0: 0.21839192708333335\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.7000000476837158\n",
      "    agent_1: 1.0000000149011612\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.11399999924004078\n",
      "    agent_1: 0.1970000049471855\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05175571668232932\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.894438030523687\n",
      "    mean_inference_ms: 1.777162765822757\n",
      "    mean_raw_obs_processing_ms: 0.1574966911369093\n",
      "  time_since_restore: 3250.957930803299\n",
      "  time_this_iter_s: 18.376235485076904\n",
      "  time_total_s: 3250.957930803299\n",
      "  timers:\n",
      "    learn_throughput: 661.141\n",
      "    learn_time_ms: 4235.105\n",
      "    load_throughput: 96172.533\n",
      "    load_time_ms: 29.114\n",
      "    sample_throughput: 198.39\n",
      "    sample_time_ms: 14113.596\n",
      "    update_time_ms: 3.198\n",
      "  timestamp: 1658498174\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 490000\n",
      "  training_iteration: 175\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   175</td><td style=\"text-align: right;\">         3250.96</td><td style=\"text-align: right;\">490000</td><td style=\"text-align: right;\">   0.083</td><td style=\"text-align: right;\">                 1.4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            265.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 985600\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-56-32\n",
      "  done: false\n",
      "  episode_len_mean: 266.28\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.4000000581145287\n",
      "  episode_reward_mean: 0.014000005573034286\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1976\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.3357927540228482\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010600726824570039\n",
      "          policy_loss: -0.06841243338948559\n",
      "          total_loss: -0.0559342468179585\n",
      "          vf_explained_var: -0.4441145956516266\n",
      "          vf_loss: 0.004349994694123217\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.2978192433005287\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.012351108322801766\n",
      "          policy_loss: -0.07618081016698852\n",
      "          total_loss: -0.06281356789016475\n",
      "          vf_explained_var: -0.010887427255511284\n",
      "          vf_loss: 0.005628801530076002\n",
      "    num_agent_steps_sampled: 985600\n",
      "    num_agent_steps_trained: 985600\n",
      "    num_steps_sampled: 492800\n",
      "    num_steps_trained: 492800\n",
      "  iterations_since_restore: 176\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.875\n",
      "    gpu_util_percent0: 0.05125\n",
      "    ram_util_percent: 54.70000000000001\n",
      "    vram_util_percent0: 0.2173421223958333\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.7000000476837158\n",
      "    agent_1: 1.0000000149011612\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.14399999924004078\n",
      "    agent_1: 0.15800000481307508\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051751274436949116\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8943635771841083\n",
      "    mean_inference_ms: 1.7769999562475944\n",
      "    mean_raw_obs_processing_ms: 0.1574660247829999\n",
      "  time_since_restore: 3269.339120388031\n",
      "  time_this_iter_s: 18.381189584732056\n",
      "  time_total_s: 3269.339120388031\n",
      "  timers:\n",
      "    learn_throughput: 663.658\n",
      "    learn_time_ms: 4219.041\n",
      "    load_throughput: 96530.547\n",
      "    load_time_ms: 29.006\n",
      "    sample_throughput: 197.996\n",
      "    sample_time_ms: 14141.688\n",
      "    update_time_ms: 3.178\n",
      "  timestamp: 1658498192\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 492800\n",
      "  training_iteration: 176\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   176</td><td style=\"text-align: right;\">         3269.34</td><td style=\"text-align: right;\">492800</td><td style=\"text-align: right;\">   0.014</td><td style=\"text-align: right;\">                 1.4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            266.28</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 991200\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-56-51\n",
      "  done: false\n",
      "  episode_len_mean: 270.15\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.4000000581145287\n",
      "  episode_reward_mean: 0.03500000558793545\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1986\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.3496156192961193\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.009918656728843284\n",
      "          policy_loss: -0.07657225571469033\n",
      "          total_loss: -0.06543411342898339\n",
      "          vf_explained_var: -0.21271319687366486\n",
      "          vf_loss: 0.0027314930555240565\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.2217330655881335\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.012888218236255881\n",
      "          policy_loss: -0.07578919834226586\n",
      "          total_loss: -0.062669951826078\n",
      "          vf_explained_var: 0.06537751108407974\n",
      "          vf_loss: 0.0032972558212696597\n",
      "    num_agent_steps_sampled: 991200\n",
      "    num_agent_steps_trained: 991200\n",
      "    num_steps_sampled: 495600\n",
      "    num_steps_trained: 495600\n",
      "  iterations_since_restore: 177\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.904\n",
      "    gpu_util_percent0: 0.06559999999999999\n",
      "    ram_util_percent: 54.70000000000001\n",
      "    vram_util_percent0: 0.2170703125\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.7000000476837158\n",
      "    agent_1: 1.0000000149011612\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.13399999924004077\n",
      "    agent_1: 0.16900000482797622\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05174691991886405\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8942910700322693\n",
      "    mean_inference_ms: 1.7768456978639267\n",
      "    mean_raw_obs_processing_ms: 0.15743594481054848\n",
      "  time_since_restore: 3287.734020471573\n",
      "  time_this_iter_s: 18.39490008354187\n",
      "  time_total_s: 3287.734020471573\n",
      "  timers:\n",
      "    learn_throughput: 666.415\n",
      "    learn_time_ms: 4201.587\n",
      "    load_throughput: 96225.487\n",
      "    load_time_ms: 29.098\n",
      "    sample_throughput: 198.023\n",
      "    sample_time_ms: 14139.776\n",
      "    update_time_ms: 3.176\n",
      "  timestamp: 1658498211\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 495600\n",
      "  training_iteration: 177\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   177</td><td style=\"text-align: right;\">         3287.73</td><td style=\"text-align: right;\">495600</td><td style=\"text-align: right;\">   0.035</td><td style=\"text-align: right;\">                 1.4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            270.15</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 996800\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-57-09\n",
      "  done: false\n",
      "  episode_len_mean: 268.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.4000000581145287\n",
      "  episode_reward_mean: 0.06300000563263893\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1996\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.3147729177560126\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.0144963160191894\n",
      "          policy_loss: -0.09732337051329003\n",
      "          total_loss: -0.08146323971907675\n",
      "          vf_explained_var: 0.07158412039279938\n",
      "          vf_loss: 0.0014686599554135597\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.217965561364378\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01538253595308172\n",
      "          policy_loss: -0.09392166294466213\n",
      "          total_loss: -0.07856090071705\n",
      "          vf_explained_var: 0.1305745393037796\n",
      "          vf_loss: 0.0025711176757405546\n",
      "    num_agent_steps_sampled: 996800\n",
      "    num_agent_steps_trained: 996800\n",
      "    num_steps_sampled: 498400\n",
      "    num_steps_trained: 498400\n",
      "  iterations_since_restore: 178\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.891666666666667\n",
      "    gpu_util_percent0: 0.06\n",
      "    ram_util_percent: 54.70000000000001\n",
      "    vram_util_percent0: 0.21793619791666666\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.7000000476837158\n",
      "    agent_1: 1.0000000149011612\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.12399999924004078\n",
      "    agent_1: 0.1870000048726797\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05174278421166611\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.894193796349382\n",
      "    mean_inference_ms: 1.7766914589800336\n",
      "    mean_raw_obs_processing_ms: 0.15740818109018373\n",
      "  time_since_restore: 3306.0710875988007\n",
      "  time_this_iter_s: 18.337067127227783\n",
      "  time_total_s: 3306.0710875988007\n",
      "  timers:\n",
      "    learn_throughput: 668.589\n",
      "    learn_time_ms: 4187.926\n",
      "    load_throughput: 96482.251\n",
      "    load_time_ms: 29.021\n",
      "    sample_throughput: 198.097\n",
      "    sample_time_ms: 14134.48\n",
      "    update_time_ms: 3.168\n",
      "  timestamp: 1658498229\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 498400\n",
      "  training_iteration: 178\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   178</td><td style=\"text-align: right;\">         3306.07</td><td style=\"text-align: right;\">498400</td><td style=\"text-align: right;\">   0.063</td><td style=\"text-align: right;\">                 1.4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            268.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1002400\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-57-28\n",
      "  done: false\n",
      "  episode_len_mean: 263.14\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.4000000581145287\n",
      "  episode_reward_mean: 0.08700000569224357\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 2006\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.2510052699418295\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.012236382093103293\n",
      "          policy_loss: -0.06051266801044958\n",
      "          total_loss: -0.04675002073452647\n",
      "          vf_explained_var: 0.14526286721229553\n",
      "          vf_loss: 0.0026631323850042896\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.1709045068848702\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.013202125715977701\n",
      "          policy_loss: -0.07727447924544809\n",
      "          total_loss: -0.06375854241265361\n",
      "          vf_explained_var: 0.05263172835111618\n",
      "          vf_loss: 0.0034732258267294604\n",
      "    num_agent_steps_sampled: 1002400\n",
      "    num_agent_steps_trained: 1002400\n",
      "    num_steps_sampled: 501200\n",
      "    num_steps_trained: 501200\n",
      "  iterations_since_restore: 179\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.845833333333334\n",
      "    gpu_util_percent0: 0.05875\n",
      "    ram_util_percent: 54.67500000000001\n",
      "    vram_util_percent0: 0.21715494791666665\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.7000000476837158\n",
      "    agent_1: 1.0000000149011612\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.11699999928474426\n",
      "    agent_1: 0.20400000497698784\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05173891256873282\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.894040906210502\n",
      "    mean_inference_ms: 1.7765545208476015\n",
      "    mean_raw_obs_processing_ms: 0.15738613531999182\n",
      "  time_since_restore: 3324.4678161144257\n",
      "  time_this_iter_s: 18.396728515625\n",
      "  time_total_s: 3324.4678161144257\n",
      "  timers:\n",
      "    learn_throughput: 669.156\n",
      "    learn_time_ms: 4184.375\n",
      "    load_throughput: 96682.65\n",
      "    load_time_ms: 28.961\n",
      "    sample_throughput: 198.46\n",
      "    sample_time_ms: 14108.643\n",
      "    update_time_ms: 3.188\n",
      "  timestamp: 1658498248\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 501200\n",
      "  training_iteration: 179\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   179</td><td style=\"text-align: right;\">         3324.47</td><td style=\"text-align: right;\">501200</td><td style=\"text-align: right;\">   0.087</td><td style=\"text-align: right;\">                 1.4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            263.14</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1008000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-57-46\n",
      "  done: false\n",
      "  episode_len_mean: 255.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.4000000581145287\n",
      "  episode_reward_mean: 0.04400000549852848\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 2020\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.3638733419634046\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.009979476759159212\n",
      "          policy_loss: -0.07331424072340485\n",
      "          total_loss: -0.061948130694439725\n",
      "          vf_explained_var: -0.058540113270282745\n",
      "          vf_loss: 0.0032038643450851808\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.1623114783849036\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010599634356241677\n",
      "          policy_loss: -0.07493592080261026\n",
      "          total_loss: -0.06392966457653008\n",
      "          vf_explained_var: 0.27249065041542053\n",
      "          vf_loss: 0.003727973782328523\n",
      "    num_agent_steps_sampled: 1008000\n",
      "    num_agent_steps_trained: 1008000\n",
      "    num_steps_sampled: 504000\n",
      "    num_steps_trained: 504000\n",
      "  iterations_since_restore: 180\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.933333333333334\n",
      "    gpu_util_percent0: 0.049999999999999996\n",
      "    ram_util_percent: 54.64166666666667\n",
      "    vram_util_percent0: 0.21793619791666666\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.7000000476837158\n",
      "    agent_1: 1.0000000149011612\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.14299999952316284\n",
      "    agent_1: 0.1870000050216913\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051733645506576484\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.893851535063502\n",
      "    mean_inference_ms: 1.7763767320745187\n",
      "    mean_raw_obs_processing_ms: 0.15737661027742184\n",
      "  time_since_restore: 3342.979237318039\n",
      "  time_this_iter_s: 18.51142120361328\n",
      "  time_total_s: 3342.979237318039\n",
      "  timers:\n",
      "    learn_throughput: 669.043\n",
      "    learn_time_ms: 4185.085\n",
      "    load_throughput: 96194.59\n",
      "    load_time_ms: 29.108\n",
      "    sample_throughput: 198.483\n",
      "    sample_time_ms: 14107.035\n",
      "    update_time_ms: 3.179\n",
      "  timestamp: 1658498266\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 504000\n",
      "  training_iteration: 180\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   180</td><td style=\"text-align: right;\">         3342.98</td><td style=\"text-align: right;\">504000</td><td style=\"text-align: right;\">   0.044</td><td style=\"text-align: right;\">                 1.4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            255.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1013600\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-58-05\n",
      "  done: false\n",
      "  episode_len_mean: 233.19\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.4000000581145287\n",
      "  episode_reward_mean: 0.006000005528330803\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 2036\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.3185543091524217\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010050159629687614\n",
      "          policy_loss: -0.06820632701973886\n",
      "          total_loss: -0.055810968916505645\n",
      "          vf_explained_var: -0.05210888385772705\n",
      "          vf_loss: 0.00586121920232905\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.089158482849598\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.012084249121870788\n",
      "          policy_loss: -0.06945754355911049\n",
      "          total_loss: -0.05642617064794259\n",
      "          vf_explained_var: 0.5136433839797974\n",
      "          vf_loss: 0.005184068830290232\n",
      "    num_agent_steps_sampled: 1013600\n",
      "    num_agent_steps_trained: 1013600\n",
      "    num_steps_sampled: 506800\n",
      "    num_steps_trained: 506800\n",
      "  iterations_since_restore: 181\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.974999999999999\n",
      "    gpu_util_percent0: 0.052083333333333336\n",
      "    ram_util_percent: 54.65\n",
      "    vram_util_percent0: 0.21745605468749998\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.7000000476837158\n",
      "    agent_1: 1.0000000149011612\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.16299999952316285\n",
      "    agent_1: 0.16900000505149365\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05172811502724237\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.893640772493334\n",
      "    mean_inference_ms: 1.776177148278694\n",
      "    mean_raw_obs_processing_ms: 0.1574051855673662\n",
      "  time_since_restore: 3361.3726649284363\n",
      "  time_this_iter_s: 18.39342761039734\n",
      "  time_total_s: 3361.3726649284363\n",
      "  timers:\n",
      "    learn_throughput: 668.621\n",
      "    learn_time_ms: 4187.723\n",
      "    load_throughput: 96403.606\n",
      "    load_time_ms: 29.045\n",
      "    sample_throughput: 198.138\n",
      "    sample_time_ms: 14131.536\n",
      "    update_time_ms: 3.201\n",
      "  timestamp: 1658498285\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 506800\n",
      "  training_iteration: 181\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   181</td><td style=\"text-align: right;\">         3361.37</td><td style=\"text-align: right;\">506800</td><td style=\"text-align: right;\">0.00600001</td><td style=\"text-align: right;\">                 1.4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            233.19</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1019200\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-58-23\n",
      "  done: false\n",
      "  episode_len_mean: 238.51\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.4000000581145287\n",
      "  episode_reward_mean: 0.038000005707144735\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 2048\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.347819602915219\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.012971743267425758\n",
      "          policy_loss: -0.0716638876252719\n",
      "          total_loss: -0.05691059768737648\n",
      "          vf_explained_var: 0.1587478220462799\n",
      "          vf_loss: 0.003244413596327095\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.116657760526453\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.013223633511766057\n",
      "          policy_loss: -0.07588407457716953\n",
      "          total_loss: -0.06217634484199239\n",
      "          vf_explained_var: 0.3222033977508545\n",
      "          vf_loss: 0.003894919415220751\n",
      "    num_agent_steps_sampled: 1019200\n",
      "    num_agent_steps_trained: 1019200\n",
      "    num_steps_sampled: 509600\n",
      "    num_steps_trained: 509600\n",
      "  iterations_since_restore: 182\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.916\n",
      "    gpu_util_percent0: 0.0608\n",
      "    ram_util_percent: 54.61600000000001\n",
      "    vram_util_percent0: 0.2170703125\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.7000000476837158\n",
      "    agent_1: 1.0000000149011612\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.15299999952316284\n",
      "    agent_1: 0.19100000523030758\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051724109203356995\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8934997877664608\n",
      "    mean_inference_ms: 1.776024520277719\n",
      "    mean_raw_obs_processing_ms: 0.15742425538090646\n",
      "  time_since_restore: 3379.754533290863\n",
      "  time_this_iter_s: 18.381868362426758\n",
      "  time_total_s: 3379.754533290863\n",
      "  timers:\n",
      "    learn_throughput: 668.351\n",
      "    learn_time_ms: 4189.413\n",
      "    load_throughput: 96440.814\n",
      "    load_time_ms: 29.033\n",
      "    sample_throughput: 198.098\n",
      "    sample_time_ms: 14134.448\n",
      "    update_time_ms: 3.221\n",
      "  timestamp: 1658498303\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 509600\n",
      "  training_iteration: 182\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   182</td><td style=\"text-align: right;\">         3379.75</td><td style=\"text-align: right;\">509600</td><td style=\"text-align: right;\">   0.038</td><td style=\"text-align: right;\">                 1.4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            238.51</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1024800\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-58-41\n",
      "  done: false\n",
      "  episode_len_mean: 239.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0000000149011612\n",
      "  episode_reward_mean: 0.03100000523030758\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 2059\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.415245824626514\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.009705200348189362\n",
      "          policy_loss: -0.12391348475579261\n",
      "          total_loss: -0.11347381936779823\n",
      "          vf_explained_var: -0.12021175026893616\n",
      "          vf_loss: 0.001501006339952125\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.2203822111090026\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011399687726603486\n",
      "          policy_loss: -0.12744651637131893\n",
      "          total_loss: -0.11611857249642656\n",
      "          vf_explained_var: 0.17717497050762177\n",
      "          vf_loss: 0.002431336289408223\n",
      "    num_agent_steps_sampled: 1024800\n",
      "    num_agent_steps_trained: 1024800\n",
      "    num_steps_sampled: 512400\n",
      "    num_steps_trained: 512400\n",
      "  iterations_since_restore: 183\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.087500000000001\n",
      "    gpu_util_percent0: 0.051666666666666666\n",
      "    ram_util_percent: 54.92083333333333\n",
      "    vram_util_percent0: 0.21793619791666666\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.0\n",
      "    agent_1: 1.0000000149011612\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.16\n",
      "    agent_1: 0.19100000523030758\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05172050455568921\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.893371266193676\n",
      "    mean_inference_ms: 1.7758989291188467\n",
      "    mean_raw_obs_processing_ms: 0.1574370550958684\n",
      "  time_since_restore: 3398.199291944504\n",
      "  time_this_iter_s: 18.444758653640747\n",
      "  time_total_s: 3398.199291944504\n",
      "  timers:\n",
      "    learn_throughput: 669.905\n",
      "    learn_time_ms: 4179.698\n",
      "    load_throughput: 96423.236\n",
      "    load_time_ms: 29.039\n",
      "    sample_throughput: 197.979\n",
      "    sample_time_ms: 14142.931\n",
      "    update_time_ms: 3.224\n",
      "  timestamp: 1658498321\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 512400\n",
      "  training_iteration: 183\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   183</td><td style=\"text-align: right;\">          3398.2</td><td style=\"text-align: right;\">512400</td><td style=\"text-align: right;\">   0.031</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            239.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1030400\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-59-00\n",
      "  done: false\n",
      "  episode_len_mean: 236.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.10900000579655171\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 2072\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.3648209330581484\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.008145162767816447\n",
      "          policy_loss: -0.04845380677094605\n",
      "          total_loss: -0.034671294033094976\n",
      "          vf_explained_var: -0.2658732831478119\n",
      "          vf_loss: 0.015993921765454746\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.2045992509949777\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010573171318877674\n",
      "          policy_loss: -0.046300538641801156\n",
      "          total_loss: -0.03308370483691327\n",
      "          vf_explained_var: 0.2978746294975281\n",
      "          vf_loss: 0.010164181993483604\n",
      "    num_agent_steps_sampled: 1030400\n",
      "    num_agent_steps_trained: 1030400\n",
      "    num_steps_sampled: 515200\n",
      "    num_steps_trained: 515200\n",
      "  iterations_since_restore: 184\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.920833333333334\n",
      "    gpu_util_percent0: 0.05291666666666667\n",
      "    ram_util_percent: 54.90833333333333\n",
      "    vram_util_percent0: 0.21715494791666665\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.12\n",
      "    agent_1: 0.2290000057965517\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05171619015952438\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8932189640502757\n",
      "    mean_inference_ms: 1.7757435610408923\n",
      "    mean_raw_obs_processing_ms: 0.15746283079344942\n",
      "  time_since_restore: 3416.5502240657806\n",
      "  time_this_iter_s: 18.350932121276855\n",
      "  time_total_s: 3416.5502240657806\n",
      "  timers:\n",
      "    learn_throughput: 670.655\n",
      "    learn_time_ms: 4175.02\n",
      "    load_throughput: 96551.261\n",
      "    load_time_ms: 29.0\n",
      "    sample_throughput: 198.131\n",
      "    sample_time_ms: 14132.03\n",
      "    update_time_ms: 3.223\n",
      "  timestamp: 1658498340\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 515200\n",
      "  training_iteration: 184\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   184</td><td style=\"text-align: right;\">         3416.55</td><td style=\"text-align: right;\">515200</td><td style=\"text-align: right;\">   0.109</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            236.39</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1036000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-59-18\n",
      "  done: false\n",
      "  episode_len_mean: 235.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.1500000061094761\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 2082\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.3026483058929443\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.013526185960411007\n",
      "          policy_loss: -0.09952015351085547\n",
      "          total_loss: -0.08374448732805224\n",
      "          vf_explained_var: -0.28456738591194153\n",
      "          vf_loss: 0.004328740656568525\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.1633625583989278\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.014455063935555618\n",
      "          policy_loss: -0.10048200677002092\n",
      "          total_loss: -0.08514950076434095\n",
      "          vf_explained_var: -0.011650199070572853\n",
      "          vf_loss: 0.0050732509714310125\n",
      "    num_agent_steps_sampled: 1036000\n",
      "    num_agent_steps_trained: 1036000\n",
      "    num_steps_sampled: 518000\n",
      "    num_steps_trained: 518000\n",
      "  iterations_since_restore: 185\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.841666666666666\n",
      "    gpu_util_percent0: 0.049999999999999996\n",
      "    ram_util_percent: 54.70000000000001\n",
      "    vram_util_percent0: 0.21812337239583335\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.11\n",
      "    agent_1: 0.2600000061094761\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0517130183306099\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8931182727874263\n",
      "    mean_inference_ms: 1.7756309112747999\n",
      "    mean_raw_obs_processing_ms: 0.15748301832391962\n",
      "  time_since_restore: 3434.9903559684753\n",
      "  time_this_iter_s: 18.440131902694702\n",
      "  time_total_s: 3434.9903559684753\n",
      "  timers:\n",
      "    learn_throughput: 670.43\n",
      "    learn_time_ms: 4176.425\n",
      "    load_throughput: 96468.065\n",
      "    load_time_ms: 29.025\n",
      "    sample_throughput: 198.062\n",
      "    sample_time_ms: 14136.967\n",
      "    update_time_ms: 3.233\n",
      "  timestamp: 1658498358\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 518000\n",
      "  training_iteration: 185\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   185</td><td style=\"text-align: right;\">         3434.99</td><td style=\"text-align: right;\">518000</td><td style=\"text-align: right;\">    0.15</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            235.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1041600\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-59-37\n",
      "  done: false\n",
      "  episode_len_mean: 241.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.14600000604987146\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 2092\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.331974999180862\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.015628951192387355\n",
      "          policy_loss: -0.07694550083137333\n",
      "          total_loss: -0.059817208625428886\n",
      "          vf_explained_var: 0.36389556527137756\n",
      "          vf_loss: 0.001471813724406058\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.2608987224243937\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.014919893108521032\n",
      "          policy_loss: -0.08720676362558845\n",
      "          total_loss: -0.07268931738854874\n",
      "          vf_explained_var: 0.1953435242176056\n",
      "          vf_loss: 0.0015353915123341721\n",
      "    num_agent_steps_sampled: 1041600\n",
      "    num_agent_steps_trained: 1041600\n",
      "    num_steps_sampled: 520800\n",
      "    num_steps_trained: 520800\n",
      "  iterations_since_restore: 186\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.867999999999999\n",
      "    gpu_util_percent0: 0.057999999999999996\n",
      "    ram_util_percent: 54.70000000000001\n",
      "    vram_util_percent0: 0.2172890625\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.11\n",
      "    agent_1: 0.25600000604987144\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05171079354904894\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8930435444709928\n",
      "    mean_inference_ms: 1.7755690608107093\n",
      "    mean_raw_obs_processing_ms: 0.15750531360731823\n",
      "  time_since_restore: 3453.791679620743\n",
      "  time_this_iter_s: 18.801323652267456\n",
      "  time_total_s: 3453.791679620743\n",
      "  timers:\n",
      "    learn_throughput: 669.071\n",
      "    learn_time_ms: 4184.908\n",
      "    load_throughput: 96362.553\n",
      "    load_time_ms: 29.057\n",
      "    sample_throughput: 197.597\n",
      "    sample_time_ms: 14170.286\n",
      "    update_time_ms: 3.223\n",
      "  timestamp: 1658498377\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 520800\n",
      "  training_iteration: 186\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   186</td><td style=\"text-align: right;\">         3453.79</td><td style=\"text-align: right;\">520800</td><td style=\"text-align: right;\">   0.146</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             241.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1047200\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_13-59-56\n",
      "  done: false\n",
      "  episode_len_mean: 239.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.15200000613927842\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 2101\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.243077758167471\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011498255487878861\n",
      "          policy_loss: -0.07130333400328666\n",
      "          total_loss: -0.0580702707730303\n",
      "          vf_explained_var: 0.09672737121582031\n",
      "          vf_loss: 0.0035124257027573116\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.177282279800801\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010756675244266727\n",
      "          policy_loss: -0.07177086338578235\n",
      "          total_loss: -0.06075885797881277\n",
      "          vf_explained_var: 0.26852020621299744\n",
      "          vf_loss: 0.0033138615387672743\n",
      "    num_agent_steps_sampled: 1047200\n",
      "    num_agent_steps_trained: 1047200\n",
      "    num_steps_sampled: 523600\n",
      "    num_steps_trained: 523600\n",
      "  iterations_since_restore: 187\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.912000000000001\n",
      "    gpu_util_percent0: 0.0564\n",
      "    ram_util_percent: 54.751999999999995\n",
      "    vram_util_percent0: 0.216921875\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.11\n",
      "    agent_1: 0.26200000613927843\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05170959484633896\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8930026842940197\n",
      "    mean_inference_ms: 1.7755529994371395\n",
      "    mean_raw_obs_processing_ms: 0.15752385759352674\n",
      "  time_since_restore: 3472.618862390518\n",
      "  time_this_iter_s: 18.82718276977539\n",
      "  time_total_s: 3472.618862390518\n",
      "  timers:\n",
      "    learn_throughput: 667.397\n",
      "    learn_time_ms: 4195.405\n",
      "    load_throughput: 96749.396\n",
      "    load_time_ms: 28.941\n",
      "    sample_throughput: 197.1\n",
      "    sample_time_ms: 14206.012\n",
      "    update_time_ms: 3.217\n",
      "  timestamp: 1658498396\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 523600\n",
      "  training_iteration: 187\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   187</td><td style=\"text-align: right;\">         3472.62</td><td style=\"text-align: right;\">523600</td><td style=\"text-align: right;\">   0.152</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            239.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1052800\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-00-15\n",
      "  done: false\n",
      "  episode_len_mean: 235.54\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.17100000612437724\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 2114\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.1882277989671346\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011338263607006209\n",
      "          policy_loss: -0.08184985880645745\n",
      "          total_loss: -0.06884494305926464\n",
      "          vf_explained_var: 0.18007494509220123\n",
      "          vf_loss: 0.0033098419451215748\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.059840379726319\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010516693182354778\n",
      "          policy_loss: -0.06918789127649645\n",
      "          total_loss: -0.057920529146767444\n",
      "          vf_explained_var: 0.1633043736219406\n",
      "          vf_loss: 0.004588441539887017\n",
      "    num_agent_steps_sampled: 1052800\n",
      "    num_agent_steps_trained: 1052800\n",
      "    num_steps_sampled: 526400\n",
      "    num_steps_trained: 526400\n",
      "  iterations_since_restore: 188\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.164\n",
      "    gpu_util_percent0: 0.0576\n",
      "    ram_util_percent: 54.78\n",
      "    vram_util_percent0: 0.21812109375\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.09799999997019768\n",
      "    agent_1: 0.2690000060945749\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051709178626006234\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8930136182456376\n",
      "    mean_inference_ms: 1.7755845709740226\n",
      "    mean_raw_obs_processing_ms: 0.15755568590151772\n",
      "  time_since_restore: 3491.8340151309967\n",
      "  time_this_iter_s: 19.215152740478516\n",
      "  time_total_s: 3491.8340151309967\n",
      "  timers:\n",
      "    learn_throughput: 663.033\n",
      "    learn_time_ms: 4223.016\n",
      "    load_throughput: 96681.536\n",
      "    load_time_ms: 28.961\n",
      "    sample_throughput: 196.231\n",
      "    sample_time_ms: 14268.874\n",
      "    update_time_ms: 3.238\n",
      "  timestamp: 1658498415\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 526400\n",
      "  training_iteration: 188\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   188</td><td style=\"text-align: right;\">         3491.83</td><td style=\"text-align: right;\">526400</td><td style=\"text-align: right;\">   0.171</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            235.54</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1058400\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-00-34\n",
      "  done: false\n",
      "  episode_len_mean: 236.92\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.25100000627338886\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 2129\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.277618822597322\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.0070765173112726\n",
      "          policy_loss: -0.04786688418313306\n",
      "          total_loss: -0.03699056124241906\n",
      "          vf_explained_var: -0.08198591321706772\n",
      "          vf_loss: 0.011026137122739567\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.1543406833495413\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.009947180489989549\n",
      "          policy_loss: -0.07030361817860034\n",
      "          total_loss: -0.05944191885888964\n",
      "          vf_explained_var: 0.27069365978240967\n",
      "          vf_loss: 0.005168377842935678\n",
      "    num_agent_steps_sampled: 1058400\n",
      "    num_agent_steps_trained: 1058400\n",
      "    num_steps_sampled: 529200\n",
      "    num_steps_trained: 529200\n",
      "  iterations_since_restore: 189\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.864\n",
      "    gpu_util_percent0: 0.0556\n",
      "    ram_util_percent: 54.768\n",
      "    vram_util_percent0: 0.2172890625\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.057999999970197676\n",
      "    agent_1: 0.30900000624358653\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051709956197609326\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8930931452008615\n",
      "    mean_inference_ms: 1.7756798785296777\n",
      "    mean_raw_obs_processing_ms: 0.15759421604611812\n",
      "  time_since_restore: 3510.740582704544\n",
      "  time_this_iter_s: 18.906567573547363\n",
      "  time_total_s: 3510.740582704544\n",
      "  timers:\n",
      "    learn_throughput: 660.66\n",
      "    learn_time_ms: 4238.183\n",
      "    load_throughput: 96502.865\n",
      "    load_time_ms: 29.015\n",
      "    sample_throughput: 195.743\n",
      "    sample_time_ms: 14304.442\n",
      "    update_time_ms: 3.244\n",
      "  timestamp: 1658498434\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 529200\n",
      "  training_iteration: 189\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   189</td><td style=\"text-align: right;\">         3510.74</td><td style=\"text-align: right;\">529200</td><td style=\"text-align: right;\">   0.251</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            236.92</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1064000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-00-53\n",
      "  done: false\n",
      "  episode_len_mean: 234.18\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.24800000622868537\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 2142\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.251871709667501\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010989052119702311\n",
      "          policy_loss: -0.07580344589604508\n",
      "          total_loss: -0.06288489058455785\n",
      "          vf_explained_var: -0.031088395044207573\n",
      "          vf_loss: 0.004260354189942258\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.1544219940191223\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011063518849932523\n",
      "          policy_loss: -0.08457219982076258\n",
      "          total_loss: -0.07277092008319284\n",
      "          vf_explained_var: 0.05259788781404495\n",
      "          vf_loss: 0.004663899169480871\n",
      "    num_agent_steps_sampled: 1064000\n",
      "    num_agent_steps_trained: 1064000\n",
      "    num_steps_sampled: 532000\n",
      "    num_steps_trained: 532000\n",
      "  iterations_since_restore: 190\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.9079999999999995\n",
      "    gpu_util_percent0: 0.056799999999999996\n",
      "    ram_util_percent: 54.74799999999999\n",
      "    vram_util_percent0: 0.217890625\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.057999999970197676\n",
      "    agent_1: 0.30600000619888307\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051711508301704956\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8931973006262344\n",
      "    mean_inference_ms: 1.7758058465968711\n",
      "    mean_raw_obs_processing_ms: 0.15762706250180084\n",
      "  time_since_restore: 3529.6434581279755\n",
      "  time_this_iter_s: 18.902875423431396\n",
      "  time_total_s: 3529.6434581279755\n",
      "  timers:\n",
      "    learn_throughput: 658.753\n",
      "    learn_time_ms: 4250.452\n",
      "    load_throughput: 96994.306\n",
      "    load_time_ms: 28.868\n",
      "    sample_throughput: 195.376\n",
      "    sample_time_ms: 14331.332\n",
      "    update_time_ms: 3.259\n",
      "  timestamp: 1658498453\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 532000\n",
      "  training_iteration: 190\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   190</td><td style=\"text-align: right;\">         3529.64</td><td style=\"text-align: right;\">532000</td><td style=\"text-align: right;\">   0.248</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            234.18</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1069600\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-01-12\n",
      "  done: false\n",
      "  episode_len_mean: 236.82\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.19800000607967377\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 2154\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.299890636688187\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.009745555999469796\n",
      "          policy_loss: -0.06560012542364088\n",
      "          total_loss: -0.05297920175841379\n",
      "          vf_explained_var: -0.28733235597610474\n",
      "          vf_loss: 0.0074612119683262704\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.2142901126117933\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010141413892572615\n",
      "          policy_loss: -0.06060161246430306\n",
      "          total_loss: -0.04842225976643108\n",
      "          vf_explained_var: 0.043749623000621796\n",
      "          vf_loss: 0.008446543141958716\n",
      "    num_agent_steps_sampled: 1069600\n",
      "    num_agent_steps_trained: 1069600\n",
      "    num_steps_sampled: 534800\n",
      "    num_steps_trained: 534800\n",
      "  iterations_since_restore: 191\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.041666666666667\n",
      "    gpu_util_percent0: 0.049166666666666664\n",
      "    ram_util_percent: 55.22083333333333\n",
      "    vram_util_percent0: 0.21735026041666664\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.07799999997019767\n",
      "    agent_1: 0.27600000604987146\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051713276804570184\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8933127435527\n",
      "    mean_inference_ms: 1.775941614329501\n",
      "    mean_raw_obs_processing_ms: 0.15765958783860784\n",
      "  time_since_restore: 3548.427179813385\n",
      "  time_this_iter_s: 18.783721685409546\n",
      "  time_total_s: 3548.427179813385\n",
      "  timers:\n",
      "    learn_throughput: 656.738\n",
      "    learn_time_ms: 4263.499\n",
      "    load_throughput: 96760.795\n",
      "    load_time_ms: 28.937\n",
      "    sample_throughput: 195.024\n",
      "    sample_time_ms: 14357.198\n",
      "    update_time_ms: 3.24\n",
      "  timestamp: 1658498472\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 534800\n",
      "  training_iteration: 191\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   191</td><td style=\"text-align: right;\">         3548.43</td><td style=\"text-align: right;\">534800</td><td style=\"text-align: right;\">   0.198</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            236.82</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1075200\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-01-30\n",
      "  done: false\n",
      "  episode_len_mean: 237.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.2090000059455633\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 2164\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.2471946172771\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.013648874787684184\n",
      "          policy_loss: -0.11871703959349404\n",
      "          total_loss: -0.10380046593982548\n",
      "          vf_explained_var: 0.023804714903235435\n",
      "          vf_loss: 0.0014164574703110183\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.2470570117944764\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.013794783376481387\n",
      "          policy_loss: -0.10354315322010302\n",
      "          total_loss: -0.0896296394310498\n",
      "          vf_explained_var: 0.05604880675673485\n",
      "          vf_loss: 0.0030067499580278914\n",
      "    num_agent_steps_sampled: 1075200\n",
      "    num_agent_steps_trained: 1075200\n",
      "    num_steps_sampled: 537600\n",
      "    num_steps_trained: 537600\n",
      "  iterations_since_restore: 192\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.856\n",
      "    gpu_util_percent0: 0.0652\n",
      "    ram_util_percent: 55.123999999999995\n",
      "    vram_util_percent0: 0.21648437500000003\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.06799999997019768\n",
      "    agent_1: 0.277000005915761\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0517148563880494\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.893416853404226\n",
      "    mean_inference_ms: 1.7760470611614088\n",
      "    mean_raw_obs_processing_ms: 0.15767930069659294\n",
      "  time_since_restore: 3566.9618515968323\n",
      "  time_this_iter_s: 18.534671783447266\n",
      "  time_total_s: 3566.9618515968323\n",
      "  timers:\n",
      "    learn_throughput: 653.948\n",
      "    learn_time_ms: 4281.689\n",
      "    load_throughput: 96680.66\n",
      "    load_time_ms: 28.961\n",
      "    sample_throughput: 195.021\n",
      "    sample_time_ms: 14357.398\n",
      "    update_time_ms: 3.205\n",
      "  timestamp: 1658498490\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 537600\n",
      "  training_iteration: 192\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   192</td><td style=\"text-align: right;\">         3566.96</td><td style=\"text-align: right;\">537600</td><td style=\"text-align: right;\">   0.209</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            237.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1080800\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-01-49\n",
      "  done: false\n",
      "  episode_len_mean: 247.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000014901161\n",
      "  episode_reward_mean: 0.16000000551342963\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 2175\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.255256139451549\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.008506928331675075\n",
      "          policy_loss: -0.08727287156285629\n",
      "          total_loss: -0.07713510293424838\n",
      "          vf_explained_var: -0.3814084231853485\n",
      "          vf_loss: 0.0042974957933828205\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.1912412583118392\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010058826738387015\n",
      "          policy_loss: -0.07901819546878826\n",
      "          total_loss: -0.06765361634398644\n",
      "          vf_explained_var: -0.052344877272844315\n",
      "          vf_loss: 0.006329027620224571\n",
      "    num_agent_steps_sampled: 1080800\n",
      "    num_agent_steps_trained: 1080800\n",
      "    num_steps_sampled: 540400\n",
      "    num_steps_trained: 540400\n",
      "  iterations_since_restore: 193\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.9375\n",
      "    gpu_util_percent0: 0.049166666666666664\n",
      "    ram_util_percent: 55.212500000000006\n",
      "    vram_util_percent0: 0.2173502604166667\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.000000014901161\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.09799999997019768\n",
      "    agent_1: 0.25800000548362734\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05171697103202118\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8935251644782887\n",
      "    mean_inference_ms: 1.776185691270545\n",
      "    mean_raw_obs_processing_ms: 0.15769899923437863\n",
      "  time_since_restore: 3585.659546852112\n",
      "  time_this_iter_s: 18.69769525527954\n",
      "  time_total_s: 3585.659546852112\n",
      "  timers:\n",
      "    learn_throughput: 651.808\n",
      "    learn_time_ms: 4295.743\n",
      "    load_throughput: 96811.046\n",
      "    load_time_ms: 28.922\n",
      "    sample_throughput: 194.87\n",
      "    sample_time_ms: 14368.573\n",
      "    update_time_ms: 3.204\n",
      "  timestamp: 1658498509\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 540400\n",
      "  training_iteration: 193\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   193</td><td style=\"text-align: right;\">         3585.66</td><td style=\"text-align: right;\">540400</td><td style=\"text-align: right;\">    0.16</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            247.37</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1086400\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-02-08\n",
      "  done: false\n",
      "  episode_len_mean: 244.54\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000014901161\n",
      "  episode_reward_mean: 0.18000000551342965\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 2185\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.367609848578771\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.013417779284629157\n",
      "          policy_loss: -0.07648924053258573\n",
      "          total_loss: -0.06197721898968753\n",
      "          vf_explained_var: 0.03359437733888626\n",
      "          vf_loss: 0.0011467257546818083\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.2272322550415993\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.015144834287735397\n",
      "          policy_loss: -0.08001755507783466\n",
      "          total_loss: -0.06518364037205651\n",
      "          vf_explained_var: 0.1332409828901291\n",
      "          vf_loss: 0.0017567001196290395\n",
      "    num_agent_steps_sampled: 1086400\n",
      "    num_agent_steps_trained: 1086400\n",
      "    num_steps_sampled: 543200\n",
      "    num_steps_trained: 543200\n",
      "  iterations_since_restore: 194\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.98\n",
      "    gpu_util_percent0: 0.0556\n",
      "    ram_util_percent: 55.04\n",
      "    vram_util_percent0: 0.2169140625\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.000000014901161\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.08799999997019768\n",
      "    agent_1: 0.26800000548362735\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05171899966786788\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.893625977080226\n",
      "    mean_inference_ms: 1.7763073682166952\n",
      "    mean_raw_obs_processing_ms: 0.1577178321219224\n",
      "  time_since_restore: 3604.4646241664886\n",
      "  time_this_iter_s: 18.80507731437683\n",
      "  time_total_s: 3604.4646241664886\n",
      "  timers:\n",
      "    learn_throughput: 649.711\n",
      "    learn_time_ms: 4309.607\n",
      "    load_throughput: 96666.734\n",
      "    load_time_ms: 28.965\n",
      "    sample_throughput: 194.444\n",
      "    sample_time_ms: 14400.024\n",
      "    update_time_ms: 3.194\n",
      "  timestamp: 1658498528\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 543200\n",
      "  training_iteration: 194\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   194</td><td style=\"text-align: right;\">         3604.46</td><td style=\"text-align: right;\">543200</td><td style=\"text-align: right;\">    0.18</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            244.54</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1092000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-02-26\n",
      "  done: false\n",
      "  episode_len_mean: 239.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000014901161\n",
      "  episode_reward_mean: 0.20900000549852848\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 2196\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.2803365456916036\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.009490590039393258\n",
      "          policy_loss: -0.04351785391505592\n",
      "          total_loss: -0.02996003188501986\n",
      "          vf_explained_var: -0.43013373017311096\n",
      "          vf_loss: 0.010931355362983942\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.224459913869699\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.012254663454742517\n",
      "          policy_loss: -0.06650693915415624\n",
      "          total_loss: -0.05366533653137767\n",
      "          vf_explained_var: 0.07758414000272751\n",
      "          vf_loss: 0.004316500936294601\n",
      "    num_agent_steps_sampled: 1092000\n",
      "    num_agent_steps_trained: 1092000\n",
      "    num_steps_sampled: 546000\n",
      "    num_steps_trained: 546000\n",
      "  iterations_since_restore: 195\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.891666666666666\n",
      "    gpu_util_percent0: 0.051666666666666666\n",
      "    ram_util_percent: 54.80416666666665\n",
      "    vram_util_percent0: 0.21752115885416665\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.000000014901161\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.06799999997019768\n",
      "    agent_1: 0.27700000546872616\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05172045281320964\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.893706534936024\n",
      "    mean_inference_ms: 1.7763887164741003\n",
      "    mean_raw_obs_processing_ms: 0.15774211630422375\n",
      "  time_since_restore: 3622.7845873832703\n",
      "  time_this_iter_s: 18.319963216781616\n",
      "  time_total_s: 3622.7845873832703\n",
      "  timers:\n",
      "    learn_throughput: 649.308\n",
      "    learn_time_ms: 4312.286\n",
      "    load_throughput: 96723.021\n",
      "    load_time_ms: 28.949\n",
      "    sample_throughput: 194.644\n",
      "    sample_time_ms: 14385.26\n",
      "    update_time_ms: 3.197\n",
      "  timestamp: 1658498546\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 546000\n",
      "  training_iteration: 195\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   195</td><td style=\"text-align: right;\">         3622.78</td><td style=\"text-align: right;\">546000</td><td style=\"text-align: right;\">   0.209</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            239.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1097600\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-02-45\n",
      "  done: false\n",
      "  episode_len_mean: 240.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000014901161\n",
      "  episode_reward_mean: 0.26100000567734244\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 2208\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.305302212635676\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.016187039530083287\n",
      "          policy_loss: -0.07943266638334362\n",
      "          total_loss: -0.06175375665978728\n",
      "          vf_explained_var: -0.1389782726764679\n",
      "          vf_loss: 0.0012196800865543385\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.187338920221442\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.015762987608160004\n",
      "          policy_loss: -0.0611299534527158\n",
      "          total_loss: -0.045709554011063164\n",
      "          vf_explained_var: 0.35012087225914\n",
      "          vf_loss: 0.001618937565754528\n",
      "    num_agent_steps_sampled: 1097600\n",
      "    num_agent_steps_trained: 1097600\n",
      "    num_steps_sampled: 548800\n",
      "    num_steps_trained: 548800\n",
      "  iterations_since_restore: 196\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.916666666666667\n",
      "    gpu_util_percent0: 0.050416666666666665\n",
      "    ram_util_percent: 54.79999999999999\n",
      "    vram_util_percent0: 0.2169840494791667\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.000000014901161\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.05\n",
      "    agent_1: 0.31100000567734243\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05172068507408044\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8937359944144627\n",
      "    mean_inference_ms: 1.7764188037321997\n",
      "    mean_raw_obs_processing_ms: 0.1577695521717634\n",
      "  time_since_restore: 3641.1843519210815\n",
      "  time_this_iter_s: 18.39976453781128\n",
      "  time_total_s: 3641.1843519210815\n",
      "  timers:\n",
      "    learn_throughput: 649.17\n",
      "    learn_time_ms: 4313.197\n",
      "    load_throughput: 96753.381\n",
      "    load_time_ms: 28.94\n",
      "    sample_throughput: 195.2\n",
      "    sample_time_ms: 14344.234\n",
      "    update_time_ms: 3.193\n",
      "  timestamp: 1658498565\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 548800\n",
      "  training_iteration: 196\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   196</td><td style=\"text-align: right;\">         3641.18</td><td style=\"text-align: right;\">548800</td><td style=\"text-align: right;\">   0.261</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            240.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1103200\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-03-03\n",
      "  done: false\n",
      "  episode_len_mean: 246.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000014901161\n",
      "  episode_reward_mean: 0.19700000546872615\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 2217\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.2759990411854925\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01010891753060737\n",
      "          policy_loss: -0.058899114757878124\n",
      "          total_loss: -0.047477181863671704\n",
      "          vf_explained_var: -0.2877678871154785\n",
      "          vf_loss: 0.002843189784146359\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.2491632547406923\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011223689682672025\n",
      "          policy_loss: -0.0572917343233712\n",
      "          total_loss: -0.04569017955212812\n",
      "          vf_explained_var: -0.09681135416030884\n",
      "          vf_loss: 0.0037489884549965304\n",
      "    num_agent_steps_sampled: 1103200\n",
      "    num_agent_steps_trained: 1103200\n",
      "    num_steps_sampled: 551600\n",
      "    num_steps_trained: 551600\n",
      "  iterations_since_restore: 197\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.837500000000001\n",
      "    gpu_util_percent0: 0.06125\n",
      "    ram_util_percent: 54.79999999999999\n",
      "    vram_util_percent0: 0.21652832031250002\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.000000014901161\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.08\n",
      "    agent_1: 0.27700000546872616\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05172016719240933\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8937120673978587\n",
      "    mean_inference_ms: 1.7764116432314694\n",
      "    mean_raw_obs_processing_ms: 0.1577757504485183\n",
      "  time_since_restore: 3659.4454870224\n",
      "  time_this_iter_s: 18.26113510131836\n",
      "  time_total_s: 3659.4454870224\n",
      "  timers:\n",
      "    learn_throughput: 650.46\n",
      "    learn_time_ms: 4304.648\n",
      "    load_throughput: 96323.509\n",
      "    load_time_ms: 29.069\n",
      "    sample_throughput: 195.856\n",
      "    sample_time_ms: 14296.194\n",
      "    update_time_ms: 3.205\n",
      "  timestamp: 1658498583\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 551600\n",
      "  training_iteration: 197\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   197</td><td style=\"text-align: right;\">         3659.45</td><td style=\"text-align: right;\">551600</td><td style=\"text-align: right;\">   0.197</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            246.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1108800\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-03-21\n",
      "  done: false\n",
      "  episode_len_mean: 249.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000014901161\n",
      "  episode_reward_mean: 0.17300000540912153\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 2228\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.2530952085341727\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.00994683556798864\n",
      "          policy_loss: -0.057785949926169235\n",
      "          total_loss: -0.04620784345821621\n",
      "          vf_explained_var: -0.18372713029384613\n",
      "          vf_loss: 0.0037823568757933876\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.1780009564189684\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.009562180777203657\n",
      "          policy_loss: -0.06418032850238628\n",
      "          total_loss: -0.054291200981034696\n",
      "          vf_explained_var: -0.057493988424539566\n",
      "          vf_loss: 0.003519147153166982\n",
      "    num_agent_steps_sampled: 1108800\n",
      "    num_agent_steps_trained: 1108800\n",
      "    num_steps_sampled: 554400\n",
      "    num_steps_trained: 554400\n",
      "  iterations_since_restore: 198\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.892\n",
      "    gpu_util_percent0: 0.06239999999999999\n",
      "    ram_util_percent: 54.79999999999999\n",
      "    vram_util_percent0: 0.21748437499999998\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.000000014901161\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.09\n",
      "    agent_1: 0.2630000054091215\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05171862479307051\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8936418511716853\n",
      "    mean_inference_ms: 1.7763692875656998\n",
      "    mean_raw_obs_processing_ms: 0.15777105025359225\n",
      "  time_since_restore: 3677.7966873645782\n",
      "  time_this_iter_s: 18.351200342178345\n",
      "  time_total_s: 3677.7966873645782\n",
      "  timers:\n",
      "    learn_throughput: 653.99\n",
      "    learn_time_ms: 4281.408\n",
      "    load_throughput: 96393.32\n",
      "    load_time_ms: 29.048\n",
      "    sample_throughput: 196.766\n",
      "    sample_time_ms: 14230.117\n",
      "    update_time_ms: 3.216\n",
      "  timestamp: 1658498601\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 554400\n",
      "  training_iteration: 198\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   198</td><td style=\"text-align: right;\">          3677.8</td><td style=\"text-align: right;\">554400</td><td style=\"text-align: right;\">   0.173</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            249.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1114400\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-03-40\n",
      "  done: false\n",
      "  episode_len_mean: 252.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000014901161\n",
      "  episode_reward_mean: 0.20200000554323197\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 2242\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.232320226019337\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01045843981919769\n",
      "          policy_loss: -0.058150676224960035\n",
      "          total_loss: -0.046417557988683085\n",
      "          vf_explained_var: 0.038974348455667496\n",
      "          vf_loss: 0.0025571755539290494\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.110845283383415\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011490127057658733\n",
      "          policy_loss: -0.061784797062240875\n",
      "          total_loss: -0.05002683219208848\n",
      "          vf_explained_var: 0.3591993451118469\n",
      "          vf_loss: 0.0032706745265126563\n",
      "    num_agent_steps_sampled: 1114400\n",
      "    num_agent_steps_trained: 1114400\n",
      "    num_steps_sampled: 557200\n",
      "    num_steps_trained: 557200\n",
      "  iterations_since_restore: 199\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.912499999999999\n",
      "    gpu_util_percent0: 0.06041666666666667\n",
      "    ram_util_percent: 55.03333333333333\n",
      "    vram_util_percent0: 0.21656901041666665\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.000000014901161\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.08\n",
      "    agent_1: 0.28200000554323196\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051715601315301854\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.893522343210249\n",
      "    mean_inference_ms: 1.7762542869658717\n",
      "    mean_raw_obs_processing_ms: 0.15776660194068612\n",
      "  time_since_restore: 3696.215793132782\n",
      "  time_this_iter_s: 18.419105768203735\n",
      "  time_total_s: 3696.215793132782\n",
      "  timers:\n",
      "    learn_throughput: 655.753\n",
      "    learn_time_ms: 4269.903\n",
      "    load_throughput: 96485.343\n",
      "    load_time_ms: 29.02\n",
      "    sample_throughput: 197.28\n",
      "    sample_time_ms: 14193.005\n",
      "    update_time_ms: 3.218\n",
      "  timestamp: 1658498620\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 557200\n",
      "  training_iteration: 199\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   199</td><td style=\"text-align: right;\">         3696.22</td><td style=\"text-align: right;\">557200</td><td style=\"text-align: right;\">   0.202</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            252.25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1120000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-03-58\n",
      "  done: false\n",
      "  episode_len_mean: 245.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000014901161\n",
      "  episode_reward_mean: 0.2790000061690807\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 2256\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.209585401273909\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011424113270371235\n",
      "          policy_loss: -0.08333711192201354\n",
      "          total_loss: -0.07067946764332841\n",
      "          vf_explained_var: -0.16223305463790894\n",
      "          vf_loss: 0.0020681429447601354\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.0543773362324353\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.0144472578501991\n",
      "          policy_loss: -0.07288170450209734\n",
      "          total_loss: -0.058628460575813994\n",
      "          vf_explained_var: 0.284562349319458\n",
      "          vf_loss: 0.0018852044052307196\n",
      "    num_agent_steps_sampled: 1120000\n",
      "    num_agent_steps_trained: 1120000\n",
      "    num_steps_sampled: 560000\n",
      "    num_steps_trained: 560000\n",
      "  iterations_since_restore: 200\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.879166666666667\n",
      "    gpu_util_percent0: 0.05541666666666667\n",
      "    ram_util_percent: 55.01666666666667\n",
      "    vram_util_percent0: 0.2173502604166667\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.000000014901161\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.04299999952316284\n",
      "    agent_1: 0.3220000056922436\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051711821073813516\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.893356464532465\n",
      "    mean_inference_ms: 1.7760898294276979\n",
      "    mean_raw_obs_processing_ms: 0.15776795777455538\n",
      "  time_since_restore: 3714.55517578125\n",
      "  time_this_iter_s: 18.339382648468018\n",
      "  time_total_s: 3714.55517578125\n",
      "  timers:\n",
      "    learn_throughput: 656.945\n",
      "    learn_time_ms: 4262.154\n",
      "    load_throughput: 96264.608\n",
      "    load_time_ms: 29.086\n",
      "    sample_throughput: 197.953\n",
      "    sample_time_ms: 14144.772\n",
      "    update_time_ms: 3.2\n",
      "  timestamp: 1658498638\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 560000\n",
      "  training_iteration: 200\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   200</td><td style=\"text-align: right;\">         3714.56</td><td style=\"text-align: right;\">560000</td><td style=\"text-align: right;\">   0.279</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             245.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1125600\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-04-17\n",
      "  done: false\n",
      "  episode_len_mean: 238.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000014901161\n",
      "  episode_reward_mean: 0.34000000603497027\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 2269\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.2395283838822726\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.00666415731379648\n",
      "          policy_loss: -0.040269338856194825\n",
      "          total_loss: -0.030544393114204076\n",
      "          vf_explained_var: -0.8316975831985474\n",
      "          vf_loss: 0.009018420684480503\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.1223953121474812\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010450108360789068\n",
      "          policy_loss: -0.0549575428650271\n",
      "          total_loss: -0.04314734277223969\n",
      "          vf_explained_var: 0.09432928264141083\n",
      "          vf_loss: 0.0064024224603989365\n",
      "    num_agent_steps_sampled: 1125600\n",
      "    num_agent_steps_trained: 1125600\n",
      "    num_steps_sampled: 562800\n",
      "    num_steps_trained: 562800\n",
      "  iterations_since_restore: 201\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.991666666666667\n",
      "    gpu_util_percent0: 0.05333333333333334\n",
      "    ram_util_percent: 54.916666666666664\n",
      "    vram_util_percent0: 0.21713053385416667\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.000000014901161\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.012999999523162841\n",
      "    agent_1: 0.3530000055581331\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051708161962687293\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.893200095699797\n",
      "    mean_inference_ms: 1.7759249399371695\n",
      "    mean_raw_obs_processing_ms: 0.1577813586178545\n",
      "  time_since_restore: 3732.9296412467957\n",
      "  time_this_iter_s: 18.374465465545654\n",
      "  time_total_s: 3732.9296412467957\n",
      "  timers:\n",
      "    learn_throughput: 660.678\n",
      "    learn_time_ms: 4238.072\n",
      "    load_throughput: 96195.614\n",
      "    load_time_ms: 29.107\n",
      "    sample_throughput: 198.192\n",
      "    sample_time_ms: 14127.702\n",
      "    update_time_ms: 3.198\n",
      "  timestamp: 1658498657\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 562800\n",
      "  training_iteration: 201\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   201</td><td style=\"text-align: right;\">         3732.93</td><td style=\"text-align: right;\">562800</td><td style=\"text-align: right;\">    0.34</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            238.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1131200\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-04-35\n",
      "  done: false\n",
      "  episode_len_mean: 246.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000014901161\n",
      "  episode_reward_mean: 0.3590000060200691\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 2277\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.265105938272817\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.013455292090297636\n",
      "          policy_loss: -0.1446694445417268\n",
      "          total_loss: -0.12977960988155246\n",
      "          vf_explained_var: -0.1222805380821228\n",
      "          vf_loss: 0.001983075827367776\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.1723239964672496\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010466729366903299\n",
      "          policy_loss: -0.1471493353143333\n",
      "          total_loss: -0.13698436228337196\n",
      "          vf_explained_var: 0.22278249263763428\n",
      "          vf_loss: 0.0017176692765912907\n",
      "    num_agent_steps_sampled: 1131200\n",
      "    num_agent_steps_trained: 1131200\n",
      "    num_steps_sampled: 565600\n",
      "    num_steps_trained: 565600\n",
      "  iterations_since_restore: 202\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.925\n",
      "    gpu_util_percent0: 0.050416666666666665\n",
      "    ram_util_percent: 54.89166666666666\n",
      "    vram_util_percent0: 0.21652832031250002\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.000000014901161\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.0029999995231628418\n",
      "    agent_1: 0.362000005543232\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051705524070326705\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.89311040372652\n",
      "    mean_inference_ms: 1.7757976884699485\n",
      "    mean_raw_obs_processing_ms: 0.15778191642538572\n",
      "  time_since_restore: 3751.168192386627\n",
      "  time_this_iter_s: 18.238551139831543\n",
      "  time_total_s: 3751.168192386627\n",
      "  timers:\n",
      "    learn_throughput: 664.509\n",
      "    learn_time_ms: 4213.636\n",
      "    load_throughput: 95916.864\n",
      "    load_time_ms: 29.192\n",
      "    sample_throughput: 198.266\n",
      "    sample_time_ms: 14122.41\n",
      "    update_time_ms: 3.191\n",
      "  timestamp: 1658498675\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 565600\n",
      "  training_iteration: 202\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   202</td><td style=\"text-align: right;\">         3751.17</td><td style=\"text-align: right;\">565600</td><td style=\"text-align: right;\">   0.359</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            246.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1136800\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-04-53\n",
      "  done: false\n",
      "  episode_len_mean: 240.13\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000014901161\n",
      "  episode_reward_mean: 0.3430000060796738\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 2290\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.3084286582611857\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.009748872345996301\n",
      "          policy_loss: -0.07304210421681914\n",
      "          total_loss: -0.062089628612002776\n",
      "          vf_explained_var: -0.41675204038619995\n",
      "          vf_loss: 0.002697837688737352\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.1478212209684506\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011627049785163315\n",
      "          policy_loss: -0.06377945996150845\n",
      "          total_loss: -0.05194426550700709\n",
      "          vf_explained_var: 0.2544882297515869\n",
      "          vf_loss: 0.003144175303878867\n",
      "    num_agent_steps_sampled: 1136800\n",
      "    num_agent_steps_trained: 1136800\n",
      "    num_steps_sampled: 568400\n",
      "    num_steps_trained: 568400\n",
      "  iterations_since_restore: 203\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.8875\n",
      "    gpu_util_percent0: 0.050833333333333335\n",
      "    ram_util_percent: 54.854166666666664\n",
      "    vram_util_percent0: 0.21753743489583333\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.000000014901161\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.012999999523162841\n",
      "    agent_1: 0.3560000056028366\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051700627692463266\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8929446019923386\n",
      "    mean_inference_ms: 1.7755740450780177\n",
      "    mean_raw_obs_processing_ms: 0.15779274748401328\n",
      "  time_since_restore: 3769.51854968071\n",
      "  time_this_iter_s: 18.35035729408264\n",
      "  time_total_s: 3769.51854968071\n",
      "  timers:\n",
      "    learn_throughput: 666.515\n",
      "    learn_time_ms: 4200.956\n",
      "    load_throughput: 95701.761\n",
      "    load_time_ms: 29.258\n",
      "    sample_throughput: 198.576\n",
      "    sample_time_ms: 14100.4\n",
      "    update_time_ms: 3.179\n",
      "  timestamp: 1658498693\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 568400\n",
      "  training_iteration: 203\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   203</td><td style=\"text-align: right;\">         3769.52</td><td style=\"text-align: right;\">568400</td><td style=\"text-align: right;\">   0.343</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            240.13</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1142400\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-05-12\n",
      "  done: false\n",
      "  episode_len_mean: 240.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.999999985098839\n",
      "  episode_reward_mean: 0.2750000061094761\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 2301\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.269288568979218\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010320824261771241\n",
      "          policy_loss: -0.0462862034286705\n",
      "          total_loss: -0.03424943128235706\n",
      "          vf_explained_var: -0.48194313049316406\n",
      "          vf_loss: 0.003909846935563283\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.1564925958712897\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010522957991271672\n",
      "          policy_loss: -0.0393859655083101\n",
      "          total_loss: -0.0282429823810595\n",
      "          vf_explained_var: 0.21195325255393982\n",
      "          vf_loss: 0.004330250615956694\n",
      "    num_agent_steps_sampled: 1142400\n",
      "    num_agent_steps_trained: 1142400\n",
      "    num_steps_sampled: 571200\n",
      "    num_steps_trained: 571200\n",
      "  iterations_since_restore: 204\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.908333333333332\n",
      "    gpu_util_percent0: 0.050416666666666665\n",
      "    ram_util_percent: 54.9\n",
      "    vram_util_percent0: 0.21675618489583334\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 1.9999999850988388\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.05299999952316284\n",
      "    agent_1: 0.3280000056326389\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051696271117240775\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.892806155612175\n",
      "    mean_inference_ms: 1.7753823906812007\n",
      "    mean_raw_obs_processing_ms: 0.15779984544929881\n",
      "  time_since_restore: 3787.8140110969543\n",
      "  time_this_iter_s: 18.295461416244507\n",
      "  time_total_s: 3787.8140110969543\n",
      "  timers:\n",
      "    learn_throughput: 669.349\n",
      "    learn_time_ms: 4183.17\n",
      "    load_throughput: 95685.075\n",
      "    load_time_ms: 29.263\n",
      "    sample_throughput: 199.042\n",
      "    sample_time_ms: 14067.364\n",
      "    update_time_ms: 3.174\n",
      "  timestamp: 1658498712\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 571200\n",
      "  training_iteration: 204\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   204</td><td style=\"text-align: right;\">         3787.81</td><td style=\"text-align: right;\">571200</td><td style=\"text-align: right;\">   0.275</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            240.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1148000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-05-30\n",
      "  done: false\n",
      "  episode_len_mean: 225.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.3570000070333481\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 2317\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.1996034564716473\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.0077372558826876514\n",
      "          policy_loss: -0.05978834948239507\n",
      "          total_loss: -0.044817318178025915\n",
      "          vf_explained_var: -0.624127984046936\n",
      "          vf_loss: 0.020500576262877854\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.0055049934557507\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.008626547177720954\n",
      "          policy_loss: -0.06098863722469278\n",
      "          total_loss: -0.04812942595928386\n",
      "          vf_explained_var: 0.39512598514556885\n",
      "          vf_loss: 0.014463879084533324\n",
      "    num_agent_steps_sampled: 1148000\n",
      "    num_agent_steps_trained: 1148000\n",
      "    num_steps_sampled: 574000\n",
      "    num_steps_trained: 574000\n",
      "  iterations_since_restore: 205\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.858333333333334\n",
      "    gpu_util_percent0: 0.050416666666666665\n",
      "    ram_util_percent: 54.9\n",
      "    vram_util_percent0: 0.21753743489583333\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.008999999463558197\n",
      "    agent_1: 0.3660000064969063\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05168987955719309\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.892638615479359\n",
      "    mean_inference_ms: 1.7751066697524254\n",
      "    mean_raw_obs_processing_ms: 0.157837189605621\n",
      "  time_since_restore: 3806.2912707328796\n",
      "  time_this_iter_s: 18.477259635925293\n",
      "  time_total_s: 3806.2912707328796\n",
      "  timers:\n",
      "    learn_throughput: 669.918\n",
      "    learn_time_ms: 4179.618\n",
      "    load_throughput: 95635.596\n",
      "    load_time_ms: 29.278\n",
      "    sample_throughput: 198.771\n",
      "    sample_time_ms: 14086.564\n",
      "    update_time_ms: 3.182\n",
      "  timestamp: 1658498730\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 574000\n",
      "  training_iteration: 205\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   205</td><td style=\"text-align: right;\">         3806.29</td><td style=\"text-align: right;\">574000</td><td style=\"text-align: right;\">   0.357</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            225.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1153600\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-05-49\n",
      "  done: false\n",
      "  episode_len_mean: 221.92\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.37800000704824926\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 2330\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.256274668588525\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.014210632210798192\n",
      "          policy_loss: -0.08785823390956336\n",
      "          total_loss: -0.07104328012454582\n",
      "          vf_explained_var: -0.18313951790332794\n",
      "          vf_loss: 0.005042393925469653\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.064065843820572\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.013611214185266726\n",
      "          policy_loss: -0.06016898964783953\n",
      "          total_loss: -0.0457519113233069\n",
      "          vf_explained_var: 0.27148833870887756\n",
      "          vf_loss: 0.004751027382683658\n",
      "    num_agent_steps_sampled: 1153600\n",
      "    num_agent_steps_trained: 1153600\n",
      "    num_steps_sampled: 576800\n",
      "    num_steps_trained: 576800\n",
      "  iterations_since_restore: 206\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.912000000000001\n",
      "    gpu_util_percent0: 0.062\n",
      "    ram_util_percent: 54.848\n",
      "    vram_util_percent0: 0.21692187500000004\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: 0.0020000005513429644\n",
      "    agent_1: 0.3760000064969063\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05168508484382166\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.892556410411271\n",
      "    mean_inference_ms: 1.7748870645375883\n",
      "    mean_raw_obs_processing_ms: 0.15787813946647733\n",
      "  time_since_restore: 3824.6413383483887\n",
      "  time_this_iter_s: 18.350067615509033\n",
      "  time_total_s: 3824.6413383483887\n",
      "  timers:\n",
      "    learn_throughput: 671.061\n",
      "    learn_time_ms: 4172.497\n",
      "    load_throughput: 95770.908\n",
      "    load_time_ms: 29.236\n",
      "    sample_throughput: 198.782\n",
      "    sample_time_ms: 14085.757\n",
      "    update_time_ms: 3.18\n",
      "  timestamp: 1658498749\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 576800\n",
      "  training_iteration: 206\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   206</td><td style=\"text-align: right;\">         3824.64</td><td style=\"text-align: right;\">576800</td><td style=\"text-align: right;\">   0.378</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            221.92</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1159200\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-06-07\n",
      "  done: false\n",
      "  episode_len_mean: 221.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.35800000667572024\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 2343\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.2946697494813373\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011444271557326666\n",
      "          policy_loss: -0.0748056602105429\n",
      "          total_loss: -0.06110811152306962\n",
      "          vf_explained_var: -0.10926342010498047\n",
      "          vf_loss: 0.00507299273790739\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.1366322331485295\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.013665261261970641\n",
      "          policy_loss: -0.07629289219316672\n",
      "          total_loss: -0.06159168791184909\n",
      "          vf_explained_var: 0.47146621346473694\n",
      "          vf_loss: 0.005493975924071579\n",
      "    num_agent_steps_sampled: 1159200\n",
      "    num_agent_steps_trained: 1159200\n",
      "    num_steps_sampled: 579600\n",
      "    num_steps_trained: 579600\n",
      "  iterations_since_restore: 207\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.9750000000000005\n",
      "    gpu_util_percent0: 0.0525\n",
      "    ram_util_percent: 54.85\n",
      "    vram_util_percent0: 0.21656901041666668\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.014999999925494193\n",
      "    agent_1: 0.3730000066012144\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05168136264701017\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.892510441034925\n",
      "    mean_inference_ms: 1.7747376894634714\n",
      "    mean_raw_obs_processing_ms: 0.1579189360384077\n",
      "  time_since_restore: 3843.51247048378\n",
      "  time_this_iter_s: 18.871132135391235\n",
      "  time_total_s: 3843.51247048378\n",
      "  timers:\n",
      "    learn_throughput: 670.491\n",
      "    learn_time_ms: 4176.047\n",
      "    load_throughput: 96006.566\n",
      "    load_time_ms: 29.165\n",
      "    sample_throughput: 197.978\n",
      "    sample_time_ms: 14142.993\n",
      "    update_time_ms: 3.188\n",
      "  timestamp: 1658498767\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 579600\n",
      "  training_iteration: 207\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   207</td><td style=\"text-align: right;\">         3843.51</td><td style=\"text-align: right;\">579600</td><td style=\"text-align: right;\">   0.358</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            221.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1164800\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-06-26\n",
      "  done: false\n",
      "  episode_len_mean: 223.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.29200000651180746\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 2355\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.0860981281314577\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.012650942046960923\n",
      "          policy_loss: -0.06938455066570202\n",
      "          total_loss: -0.05447800878813983\n",
      "          vf_explained_var: 0.11742069572210312\n",
      "          vf_loss: 0.004401427681538432\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.0964876168540547\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.012582551470029185\n",
      "          policy_loss: -0.05811048595101705\n",
      "          total_loss: -0.04434552096885741\n",
      "          vf_explained_var: 0.3467280864715576\n",
      "          vf_loss: 0.00586445087416602\n",
      "    num_agent_steps_sampled: 1164800\n",
      "    num_agent_steps_trained: 1164800\n",
      "    num_steps_sampled: 582400\n",
      "    num_steps_trained: 582400\n",
      "  iterations_since_restore: 208\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.848\n",
      "    gpu_util_percent0: 0.0552\n",
      "    ram_util_percent: 54.9\n",
      "    vram_util_percent0: 0.21746874999999996\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.039999999925494197\n",
      "    agent_1: 0.33200000643730165\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051679004766958254\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.89251790381168\n",
      "    mean_inference_ms: 1.7746658583779356\n",
      "    mean_raw_obs_processing_ms: 0.15795278866106247\n",
      "  time_since_restore: 3862.40292596817\n",
      "  time_this_iter_s: 18.89045548439026\n",
      "  time_total_s: 3862.40292596817\n",
      "  timers:\n",
      "    learn_throughput: 670.524\n",
      "    learn_time_ms: 4175.838\n",
      "    load_throughput: 88975.747\n",
      "    load_time_ms: 31.469\n",
      "    sample_throughput: 197.214\n",
      "    sample_time_ms: 14197.788\n",
      "    update_time_ms: 3.177\n",
      "  timestamp: 1658498786\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 582400\n",
      "  training_iteration: 208\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   208</td><td style=\"text-align: right;\">          3862.4</td><td style=\"text-align: right;\">582400</td><td style=\"text-align: right;\">   0.292</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            223.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1170400\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-06-45\n",
      "  done: false\n",
      "  episode_len_mean: 237.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.2650000068545342\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 2363\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.2547670482170012\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.013585509962138092\n",
      "          policy_loss: -0.08818102834907006\n",
      "          total_loss: -0.07325514855550691\n",
      "          vf_explained_var: -0.056824684143066406\n",
      "          vf_loss: 0.0016555096156350504\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.1985757624109588\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.015707843657611603\n",
      "          policy_loss: -0.09817359403158273\n",
      "          total_loss: -0.0824877680528776\n",
      "          vf_explained_var: 0.2837788760662079\n",
      "          vf_loss: 0.0025474016907710826\n",
      "    num_agent_steps_sampled: 1170400\n",
      "    num_agent_steps_trained: 1170400\n",
      "    num_steps_sampled: 585200\n",
      "    num_steps_trained: 585200\n",
      "  iterations_since_restore: 209\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.9\n",
      "    gpu_util_percent0: 0.0588\n",
      "    ram_util_percent: 54.9\n",
      "    vram_util_percent0: 0.21670312500000002\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.059999999925494193\n",
      "    agent_1: 0.3250000067800283\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051677989694788595\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8925556320795867\n",
      "    mean_inference_ms: 1.77464719175717\n",
      "    mean_raw_obs_processing_ms: 0.157964916480532\n",
      "  time_since_restore: 3881.0780923366547\n",
      "  time_this_iter_s: 18.675166368484497\n",
      "  time_total_s: 3881.0780923366547\n",
      "  timers:\n",
      "    learn_throughput: 672.94\n",
      "    learn_time_ms: 4160.845\n",
      "    load_throughput: 88878.379\n",
      "    load_time_ms: 31.504\n",
      "    sample_throughput: 196.661\n",
      "    sample_time_ms: 14237.7\n",
      "    update_time_ms: 3.374\n",
      "  timestamp: 1658498805\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 585200\n",
      "  training_iteration: 209\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   209</td><td style=\"text-align: right;\">         3881.08</td><td style=\"text-align: right;\">585200</td><td style=\"text-align: right;\">   0.265</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            237.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1176000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-07-04\n",
      "  done: false\n",
      "  episode_len_mean: 231.77\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.22800000689923763\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 2374\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.2442305244150615\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.009835360969440102\n",
      "          policy_loss: -0.07005123575730804\n",
      "          total_loss: -0.05852313122782756\n",
      "          vf_explained_var: -0.27717164158821106\n",
      "          vf_loss: 0.0039870926437808\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.155743655704317\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01023830641438595\n",
      "          policy_loss: -0.08446408893319997\n",
      "          total_loss: -0.0732642490741758\n",
      "          vf_explained_var: 0.3615424931049347\n",
      "          vf_loss: 0.005304259176184479\n",
      "    num_agent_steps_sampled: 1176000\n",
      "    num_agent_steps_trained: 1176000\n",
      "    num_steps_sampled: 588000\n",
      "    num_steps_trained: 588000\n",
      "  iterations_since_restore: 210\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.895833333333333\n",
      "    gpu_util_percent0: 0.050833333333333335\n",
      "    ram_util_percent: 54.9\n",
      "    vram_util_percent0: 0.2173502604166667\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.0799999999254942\n",
      "    agent_1: 0.30800000682473183\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05167729893759875\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.892615232799452\n",
      "    mean_inference_ms: 1.7746550939244767\n",
      "    mean_raw_obs_processing_ms: 0.15798707383130967\n",
      "  time_since_restore: 3899.780364513397\n",
      "  time_this_iter_s: 18.702272176742554\n",
      "  time_total_s: 3899.780364513397\n",
      "  timers:\n",
      "    learn_throughput: 673.748\n",
      "    learn_time_ms: 4155.859\n",
      "    load_throughput: 89099.076\n",
      "    load_time_ms: 31.426\n",
      "    sample_throughput: 196.092\n",
      "    sample_time_ms: 14278.993\n",
      "    update_time_ms: 3.386\n",
      "  timestamp: 1658498824\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 588000\n",
      "  training_iteration: 210\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   210</td><td style=\"text-align: right;\">         3899.78</td><td style=\"text-align: right;\">588000</td><td style=\"text-align: right;\">   0.228</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            231.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1181600\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-07-23\n",
      "  done: false\n",
      "  episode_len_mean: 235.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.20800000689923764\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 2383\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.2350881160015152\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.0113162103658626\n",
      "          policy_loss: -0.06435979980181271\n",
      "          total_loss: -0.05080456348133296\n",
      "          vf_explained_var: 0.11129055917263031\n",
      "          vf_loss: 0.005007272302723972\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.195890312748296\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.012420443482920939\n",
      "          policy_loss: -0.07662751386288576\n",
      "          total_loss: -0.0630947501600271\n",
      "          vf_explained_var: 0.13404551148414612\n",
      "          vf_loss: 0.005782363232616002\n",
      "    num_agent_steps_sampled: 1181600\n",
      "    num_agent_steps_trained: 1181600\n",
      "    num_steps_sampled: 590800\n",
      "    num_steps_trained: 590800\n",
      "  iterations_since_restore: 211\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.876\n",
      "    gpu_util_percent0: 0.05600000000000001\n",
      "    ram_util_percent: 54.9\n",
      "    vram_util_percent0: 0.21692187500000004\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.08999999992549419\n",
      "    agent_1: 0.2980000068247318\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05167730808828727\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8926740584164112\n",
      "    mean_inference_ms: 1.7746913750558222\n",
      "    mean_raw_obs_processing_ms: 0.15800184318485905\n",
      "  time_since_restore: 3918.5455691814423\n",
      "  time_this_iter_s: 18.765204668045044\n",
      "  time_total_s: 3918.5455691814423\n",
      "  timers:\n",
      "    learn_throughput: 669.176\n",
      "    learn_time_ms: 4184.249\n",
      "    load_throughput: 89279.927\n",
      "    load_time_ms: 31.362\n",
      "    sample_throughput: 195.945\n",
      "    sample_time_ms: 14289.748\n",
      "    update_time_ms: 3.415\n",
      "  timestamp: 1658498843\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 590800\n",
      "  training_iteration: 211\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   211</td><td style=\"text-align: right;\">         3918.55</td><td style=\"text-align: right;\">590800</td><td style=\"text-align: right;\">   0.208</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            235.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1187200\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-07-42\n",
      "  done: false\n",
      "  episode_len_mean: 227.55\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.20500000685453415\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 2398\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.248212658578441\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010788252277579704\n",
      "          policy_loss: -0.07363635798278015\n",
      "          total_loss: -0.060619669878497415\n",
      "          vf_explained_var: -0.13103854656219482\n",
      "          vf_loss: 0.005181018890999397\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.102951229328201\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010429029529447766\n",
      "          policy_loss: -0.08686206449194633\n",
      "          total_loss: -0.07514715237387766\n",
      "          vf_explained_var: 0.2441716492176056\n",
      "          vf_loss: 0.006167483153971955\n",
      "    num_agent_steps_sampled: 1187200\n",
      "    num_agent_steps_trained: 1187200\n",
      "    num_steps_sampled: 593600\n",
      "    num_steps_trained: 593600\n",
      "  iterations_since_restore: 212\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.856000000000002\n",
      "    gpu_util_percent0: 0.0576\n",
      "    ram_util_percent: 54.9\n",
      "    vram_util_percent0: 0.21633593750000002\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.08999999992549419\n",
      "    agent_1: 0.29500000678002836\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051678449641071766\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8928319789931503\n",
      "    mean_inference_ms: 1.774816596448065\n",
      "    mean_raw_obs_processing_ms: 0.15804393917361043\n",
      "  time_since_restore: 3937.527730703354\n",
      "  time_this_iter_s: 18.98216152191162\n",
      "  time_total_s: 3937.527730703354\n",
      "  timers:\n",
      "    learn_throughput: 667.519\n",
      "    learn_time_ms: 4194.638\n",
      "    load_throughput: 89570.41\n",
      "    load_time_ms: 31.26\n",
      "    sample_throughput: 195.068\n",
      "    sample_time_ms: 14354.001\n",
      "    update_time_ms: 3.425\n",
      "  timestamp: 1658498862\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 593600\n",
      "  training_iteration: 212\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   212</td><td style=\"text-align: right;\">         3937.53</td><td style=\"text-align: right;\">593600</td><td style=\"text-align: right;\">   0.205</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            227.55</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1192800\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-08-01\n",
      "  done: false\n",
      "  episode_len_mean: 229.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.300000011920929\n",
      "  episode_reward_mean: 0.13200000613927843\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 2414\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.226041253833544\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01071612517218005\n",
      "          policy_loss: -0.07279354422315096\n",
      "          total_loss: -0.060425280037071344\n",
      "          vf_explained_var: -0.21477444469928741\n",
      "          vf_loss: 0.0035352993123697594\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.0435231381228993\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010917123451610757\n",
      "          policy_loss: -0.07756509954067171\n",
      "          total_loss: -0.06603015756200832\n",
      "          vf_explained_var: 0.3610760271549225\n",
      "          vf_loss: 0.0041898314429133846\n",
      "    num_agent_steps_sampled: 1192800\n",
      "    num_agent_steps_trained: 1192800\n",
      "    num_steps_sampled: 596400\n",
      "    num_steps_trained: 596400\n",
      "  iterations_since_restore: 213\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.895999999999999\n",
      "    gpu_util_percent0: 0.056799999999999996\n",
      "    ram_util_percent: 54.895999999999994\n",
      "    vram_util_percent0: 0.2173046875\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.5\n",
      "    agent_1: 1.0000000149011612\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.13099999986588956\n",
      "    agent_1: 0.263000006005168\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05168133453919595\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8930488660841207\n",
      "    mean_inference_ms: 1.7750110378330022\n",
      "    mean_raw_obs_processing_ms: 0.15809786530231354\n",
      "  time_since_restore: 3956.480615377426\n",
      "  time_this_iter_s: 18.952884674072266\n",
      "  time_total_s: 3956.480615377426\n",
      "  timers:\n",
      "    learn_throughput: 666.236\n",
      "    learn_time_ms: 4202.717\n",
      "    load_throughput: 89725.89\n",
      "    load_time_ms: 31.206\n",
      "    sample_throughput: 194.367\n",
      "    sample_time_ms: 14405.703\n",
      "    update_time_ms: 3.511\n",
      "  timestamp: 1658498881\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 596400\n",
      "  training_iteration: 213\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   213</td><td style=\"text-align: right;\">         3956.48</td><td style=\"text-align: right;\">596400</td><td style=\"text-align: right;\">   0.132</td><td style=\"text-align: right;\">                 1.3</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            229.39</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1198400\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-08-19\n",
      "  done: false\n",
      "  episode_len_mean: 233.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.300000011920929\n",
      "  episode_reward_mean: 0.08400000602006913\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 2427\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.2103192387592223\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.009751016517357663\n",
      "          policy_loss: -0.052532435232846876\n",
      "          total_loss: -0.04098565500178319\n",
      "          vf_explained_var: -0.11920948326587677\n",
      "          vf_loss: 0.004271016568704259\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.0984761839111647\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011949634119861708\n",
      "          policy_loss: -0.06412039832433775\n",
      "          total_loss: -0.05109529298109313\n",
      "          vf_explained_var: 0.1613747477531433\n",
      "          vf_loss: 0.005561533096451534\n",
      "    num_agent_steps_sampled: 1198400\n",
      "    num_agent_steps_trained: 1198400\n",
      "    num_steps_sampled: 599200\n",
      "    num_steps_trained: 599200\n",
      "  iterations_since_restore: 214\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.95\n",
      "    gpu_util_percent0: 0.06\n",
      "    ram_util_percent: 54.916666666666664\n",
      "    vram_util_percent0: 0.21683349609375\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.5\n",
      "    agent_1: 1.0000000149011612\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.15199999988079071\n",
      "    agent_1: 0.23600000590085984\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051683636686249104\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.893196493423642\n",
      "    mean_inference_ms: 1.775176872517754\n",
      "    mean_raw_obs_processing_ms: 0.15814003731758766\n",
      "  time_since_restore: 3974.9430849552155\n",
      "  time_this_iter_s: 18.462469577789307\n",
      "  time_total_s: 3974.9430849552155\n",
      "  timers:\n",
      "    learn_throughput: 663.952\n",
      "    learn_time_ms: 4217.171\n",
      "    load_throughput: 89711.496\n",
      "    load_time_ms: 31.211\n",
      "    sample_throughput: 194.341\n",
      "    sample_time_ms: 14407.7\n",
      "    update_time_ms: 3.51\n",
      "  timestamp: 1658498899\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 599200\n",
      "  training_iteration: 214\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   214</td><td style=\"text-align: right;\">         3974.94</td><td style=\"text-align: right;\">599200</td><td style=\"text-align: right;\">   0.084</td><td style=\"text-align: right;\">                 1.3</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            233.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1204000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-08-38\n",
      "  done: false\n",
      "  episode_len_mean: 229.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.300000011920929\n",
      "  episode_reward_mean: 0.10300000607967377\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 2440\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.2587497791364077\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.015284822063681657\n",
      "          policy_loss: -0.10643068063143887\n",
      "          total_loss: -0.08974230569964718\n",
      "          vf_explained_var: -0.12311085313558578\n",
      "          vf_loss: 0.0012342339559836546\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.115053539120016\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.015048961507876686\n",
      "          policy_loss: -0.09034139034797374\n",
      "          total_loss: -0.07544601004021734\n",
      "          vf_explained_var: 0.33294761180877686\n",
      "          vf_loss: 0.0020726721227003423\n",
      "    num_agent_steps_sampled: 1204000\n",
      "    num_agent_steps_trained: 1204000\n",
      "    num_steps_sampled: 602000\n",
      "    num_steps_trained: 602000\n",
      "  iterations_since_restore: 215\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.0120000000000005\n",
      "    gpu_util_percent0: 0.0556\n",
      "    ram_util_percent: 55.076000000000015\n",
      "    vram_util_percent0: 0.2173046875\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.5\n",
      "    agent_1: 1.0000000149011612\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.1369999997317791\n",
      "    agent_1: 0.24000000581145287\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051686156520832825\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8933429815728595\n",
      "    mean_inference_ms: 1.7753517992827272\n",
      "    mean_raw_obs_processing_ms: 0.15818211672156388\n",
      "  time_since_restore: 3993.949856519699\n",
      "  time_this_iter_s: 19.006771564483643\n",
      "  time_total_s: 3993.949856519699\n",
      "  timers:\n",
      "    learn_throughput: 660.576\n",
      "    learn_time_ms: 4238.722\n",
      "    load_throughput: 89946.55\n",
      "    load_time_ms: 31.13\n",
      "    sample_throughput: 193.916\n",
      "    sample_time_ms: 14439.25\n",
      "    update_time_ms: 3.505\n",
      "  timestamp: 1658498918\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 602000\n",
      "  training_iteration: 215\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   215</td><td style=\"text-align: right;\">         3993.95</td><td style=\"text-align: right;\">602000</td><td style=\"text-align: right;\">   0.103</td><td style=\"text-align: right;\">                 1.3</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            229.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1209600\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-08-57\n",
      "  done: false\n",
      "  episode_len_mean: 223.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.700000062584877\n",
      "  episode_reward_mean: 0.16700000658631325\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 2454\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.2012126286114966\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.0103654602093234\n",
      "          policy_loss: -0.06862171643366483\n",
      "          total_loss: -0.05646366276674338\n",
      "          vf_explained_var: -0.4288288354873657\n",
      "          vf_loss: 0.004031892820161634\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.0815346872522715\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011342051289878804\n",
      "          policy_loss: -0.06801624813704707\n",
      "          total_loss: -0.05615243940666828\n",
      "          vf_explained_var: 0.12070903182029724\n",
      "          vf_loss: 0.0039607299838410525\n",
      "    num_agent_steps_sampled: 1209600\n",
      "    num_agent_steps_trained: 1209600\n",
      "    num_steps_sampled: 604800\n",
      "    num_steps_trained: 604800\n",
      "  iterations_since_restore: 216\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.870833333333334\n",
      "    gpu_util_percent0: 0.049166666666666664\n",
      "    ram_util_percent: 55.12916666666666\n",
      "    vram_util_percent0: 0.2173502604166667\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.7000000476837158\n",
      "    agent_1: 1.0000000149011612\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.10499999925494194\n",
      "    agent_1: 0.27200000584125517\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05168768597298625\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8934552741131654\n",
      "    mean_inference_ms: 1.7754638687572895\n",
      "    mean_raw_obs_processing_ms: 0.1582309111610197\n",
      "  time_since_restore: 4012.3278613090515\n",
      "  time_this_iter_s: 18.378004789352417\n",
      "  time_total_s: 4012.3278613090515\n",
      "  timers:\n",
      "    learn_throughput: 659.712\n",
      "    learn_time_ms: 4244.278\n",
      "    load_throughput: 89868.635\n",
      "    load_time_ms: 31.157\n",
      "    sample_throughput: 193.913\n",
      "    sample_time_ms: 14439.452\n",
      "    update_time_ms: 3.506\n",
      "  timestamp: 1658498937\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 604800\n",
      "  training_iteration: 216\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   216</td><td style=\"text-align: right;\">         4012.33</td><td style=\"text-align: right;\">604800</td><td style=\"text-align: right;\">   0.167</td><td style=\"text-align: right;\">                 1.7</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            223.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1215200\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-09-15\n",
      "  done: false\n",
      "  episode_len_mean: 216.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.700000062584877\n",
      "  episode_reward_mean: 0.1660000065714121\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 2468\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.3382021621579216\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010446030178133704\n",
      "          policy_loss: -0.09145306590445605\n",
      "          total_loss: -0.07991312954982277\n",
      "          vf_explained_var: -0.32467401027679443\n",
      "          vf_loss: 0.0021712744712983423\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.17117904233081\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010082784310891515\n",
      "          policy_loss: -0.07122305178667634\n",
      "          total_loss: -0.06105177120792048\n",
      "          vf_explained_var: 0.23718607425689697\n",
      "          vf_loss: 0.0028303521625968556\n",
      "    num_agent_steps_sampled: 1215200\n",
      "    num_agent_steps_trained: 1215200\n",
      "    num_steps_sampled: 607600\n",
      "    num_steps_trained: 607600\n",
      "  iterations_since_restore: 217\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.8999999999999995\n",
      "    gpu_util_percent0: 0.048749999999999995\n",
      "    ram_util_percent: 54.925000000000004\n",
      "    vram_util_percent0: 0.21675618489583334\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.7000000476837158\n",
      "    agent_1: 1.0000000149011612\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.10499999925494194\n",
      "    agent_1: 0.27100000582635403\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05168772559350687\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8935018690338357\n",
      "    mean_inference_ms: 1.7755208280819987\n",
      "    mean_raw_obs_processing_ms: 0.15830007990224207\n",
      "  time_since_restore: 4030.834394931793\n",
      "  time_this_iter_s: 18.5065336227417\n",
      "  time_total_s: 4030.834394931793\n",
      "  timers:\n",
      "    learn_throughput: 658.202\n",
      "    learn_time_ms: 4254.016\n",
      "    load_throughput: 89814.889\n",
      "    load_time_ms: 31.175\n",
      "    sample_throughput: 194.534\n",
      "    sample_time_ms: 14393.341\n",
      "    update_time_ms: 3.48\n",
      "  timestamp: 1658498955\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 607600\n",
      "  training_iteration: 217\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   217</td><td style=\"text-align: right;\">         4030.83</td><td style=\"text-align: right;\">607600</td><td style=\"text-align: right;\">   0.166</td><td style=\"text-align: right;\">                 1.7</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            216.04</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1220800\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-09-34\n",
      "  done: false\n",
      "  episode_len_mean: 218.44\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.700000062584877\n",
      "  episode_reward_mean: 0.18400000654160975\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 2476\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.274534333319891\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01183102759723417\n",
      "          policy_loss: -0.07921071276567611\n",
      "          total_loss: -0.06582386404001486\n",
      "          vf_explained_var: -0.07704822719097137\n",
      "          vf_loss: 0.0029200816100326066\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.202819187016714\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01249810066288066\n",
      "          policy_loss: -0.07899228796218051\n",
      "          total_loss: -0.06612553181531396\n",
      "          vf_explained_var: -0.13519909977912903\n",
      "          vf_loss: 0.0036676777350900616\n",
      "    num_agent_steps_sampled: 1220800\n",
      "    num_agent_steps_trained: 1220800\n",
      "    num_steps_sampled: 610400\n",
      "    num_steps_trained: 610400\n",
      "  iterations_since_restore: 218\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.808\n",
      "    gpu_util_percent0: 0.0608\n",
      "    ram_util_percent: 54.891999999999996\n",
      "    vram_util_percent0: 0.21748437499999998\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.7000000476837158\n",
      "    agent_1: 1.0000000149011612\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.09499999925494194\n",
      "    agent_1: 0.2790000057965517\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05168738921409115\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.893509757245921\n",
      "    mean_inference_ms: 1.7755412716361227\n",
      "    mean_raw_obs_processing_ms: 0.15833312298870328\n",
      "  time_since_restore: 4049.2954075336456\n",
      "  time_this_iter_s: 18.461012601852417\n",
      "  time_total_s: 4049.2954075336456\n",
      "  timers:\n",
      "    learn_throughput: 658.111\n",
      "    learn_time_ms: 4254.602\n",
      "    load_throughput: 97023.875\n",
      "    load_time_ms: 28.859\n",
      "    sample_throughput: 195.092\n",
      "    sample_time_ms: 14352.204\n",
      "    update_time_ms: 3.497\n",
      "  timestamp: 1658498974\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 610400\n",
      "  training_iteration: 218\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   218</td><td style=\"text-align: right;\">          4049.3</td><td style=\"text-align: right;\">610400</td><td style=\"text-align: right;\">   0.184</td><td style=\"text-align: right;\">                 1.7</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            218.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1226400\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-09-52\n",
      "  done: false\n",
      "  episode_len_mean: 202.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.700000062584877\n",
      "  episode_reward_mean: 0.2210000070184469\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 2494\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.212047114258721\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.008207894856626482\n",
      "          policy_loss: -0.044623640831126805\n",
      "          total_loss: -0.034557939163765435\n",
      "          vf_explained_var: -0.33225685358047485\n",
      "          vf_loss: 0.005000819831322934\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.0026363655924797\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.00958449579207652\n",
      "          policy_loss: -0.05313949713239535\n",
      "          total_loss: -0.04264469789389057\n",
      "          vf_explained_var: 0.5244190096855164\n",
      "          vf_loss: 0.004976234416279199\n",
      "    num_agent_steps_sampled: 1226400\n",
      "    num_agent_steps_trained: 1226400\n",
      "    num_steps_sampled: 613200\n",
      "    num_steps_trained: 613200\n",
      "  iterations_since_restore: 219\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.879166666666666\n",
      "    gpu_util_percent0: 0.050416666666666665\n",
      "    ram_util_percent: 54.9\n",
      "    vram_util_percent0: 0.21656901041666665\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.7000000476837158\n",
      "    agent_1: 1.0000000149011612\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.07599999889731407\n",
      "    agent_1: 0.297000005915761\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051685822329602636\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8934792886557905\n",
      "    mean_inference_ms: 1.7755441599526103\n",
      "    mean_raw_obs_processing_ms: 0.15843691244970404\n",
      "  time_since_restore: 4067.860402584076\n",
      "  time_this_iter_s: 18.564995050430298\n",
      "  time_total_s: 4067.860402584076\n",
      "  timers:\n",
      "    learn_throughput: 657.178\n",
      "    learn_time_ms: 4260.644\n",
      "    load_throughput: 89824.506\n",
      "    load_time_ms: 31.172\n",
      "    sample_throughput: 195.349\n",
      "    sample_time_ms: 14333.344\n",
      "    update_time_ms: 3.292\n",
      "  timestamp: 1658498992\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 613200\n",
      "  training_iteration: 219\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   219</td><td style=\"text-align: right;\">         4067.86</td><td style=\"text-align: right;\">613200</td><td style=\"text-align: right;\">   0.221</td><td style=\"text-align: right;\">                 1.7</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            202.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1232000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-10-11\n",
      "  done: false\n",
      "  episode_len_mean: 204.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.700000062584877\n",
      "  episode_reward_mean: 0.20300000704824925\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 2508\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.236823142639228\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010779254667492046\n",
      "          policy_loss: -0.059666670843615155\n",
      "          total_loss: -0.04718840140741945\n",
      "          vf_explained_var: -0.4906153082847595\n",
      "          vf_loss: 0.0036593851002960165\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.0948890887555622\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011297027818877928\n",
      "          policy_loss: -0.06869117813073986\n",
      "          total_loss: -0.05644108161019782\n",
      "          vf_explained_var: 0.3362027704715729\n",
      "          vf_loss: 0.005207839773599214\n",
      "    num_agent_steps_sampled: 1232000\n",
      "    num_agent_steps_trained: 1232000\n",
      "    num_steps_sampled: 616000\n",
      "    num_steps_trained: 616000\n",
      "  iterations_since_restore: 220\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.939999999999999\n",
      "    gpu_util_percent0: 0.056799999999999996\n",
      "    ram_util_percent: 54.92800000000001\n",
      "    vram_util_percent0: 0.21774609375\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.7000000476837158\n",
      "    agent_1: 1.0000000149011612\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.08599999889731408\n",
      "    agent_1: 0.2890000059455633\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05168391437480626\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8934089329184394\n",
      "    mean_inference_ms: 1.775500623010039\n",
      "    mean_raw_obs_processing_ms: 0.1585059355778362\n",
      "  time_since_restore: 4086.46133184433\n",
      "  time_this_iter_s: 18.600929260253906\n",
      "  time_total_s: 4086.46133184433\n",
      "  timers:\n",
      "    learn_throughput: 656.32\n",
      "    learn_time_ms: 4266.213\n",
      "    load_throughput: 89722.805\n",
      "    load_time_ms: 31.207\n",
      "    sample_throughput: 195.605\n",
      "    sample_time_ms: 14314.532\n",
      "    update_time_ms: 3.294\n",
      "  timestamp: 1658499011\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 616000\n",
      "  training_iteration: 220\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   220</td><td style=\"text-align: right;\">         4086.46</td><td style=\"text-align: right;\">616000</td><td style=\"text-align: right;\">   0.203</td><td style=\"text-align: right;\">                 1.7</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            204.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1237600\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-10-30\n",
      "  done: false\n",
      "  episode_len_mean: 195.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000037252903\n",
      "  episode_reward_mean: 0.32300000786781313\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 2527\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.2004877339516367\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.009274671596343811\n",
      "          policy_loss: -0.05230121332115665\n",
      "          total_loss: -0.03485922993857977\n",
      "          vf_explained_var: -0.5411741733551025\n",
      "          vf_loss: 0.022617965510497533\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.9666926977889878\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011538323413449287\n",
      "          policy_loss: -0.05368927962367057\n",
      "          total_loss: -0.03971679581029873\n",
      "          vf_explained_var: 0.26503339409828186\n",
      "          vf_loss: 0.009283674481377516\n",
      "    num_agent_steps_sampled: 1237600\n",
      "    num_agent_steps_trained: 1237600\n",
      "    num_steps_sampled: 618800\n",
      "    num_steps_trained: 618800\n",
      "  iterations_since_restore: 221\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.170833333333333\n",
      "    gpu_util_percent0: 0.049166666666666664\n",
      "    ram_util_percent: 54.9\n",
      "    vram_util_percent0: 0.2173502604166667\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.000000037252903\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.021999998539686202\n",
      "    agent_1: 0.34500000640749934\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05168139436684847\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8933766387492166\n",
      "    mean_inference_ms: 1.7754379214453408\n",
      "    mean_raw_obs_processing_ms: 0.1586270630649329\n",
      "  time_since_restore: 4105.248631954193\n",
      "  time_this_iter_s: 18.78730010986328\n",
      "  time_total_s: 4105.248631954193\n",
      "  timers:\n",
      "    learn_throughput: 657.696\n",
      "    learn_time_ms: 4257.288\n",
      "    load_throughput: 89467.034\n",
      "    load_time_ms: 31.296\n",
      "    sample_throughput: 195.451\n",
      "    sample_time_ms: 14325.849\n",
      "    update_time_ms: 3.278\n",
      "  timestamp: 1658499030\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 618800\n",
      "  training_iteration: 221\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   221</td><td style=\"text-align: right;\">         4105.25</td><td style=\"text-align: right;\">618800</td><td style=\"text-align: right;\">   0.323</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             195.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1243200\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-10-48\n",
      "  done: false\n",
      "  episode_len_mean: 187.49\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000037252903\n",
      "  episode_reward_mean: 0.32100000776350496\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 2543\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.213653951883316\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.014865962926521653\n",
      "          policy_loss: -0.08711241258268794\n",
      "          total_loss: -0.07023645409145458\n",
      "          vf_explained_var: -0.01785941980779171\n",
      "          vf_loss: 0.003061354112841876\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.0482585352091562\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01440238324062193\n",
      "          policy_loss: -0.08848347577111867\n",
      "          total_loss: -0.07368830266793902\n",
      "          vf_explained_var: 0.5383889675140381\n",
      "          vf_loss: 0.003553070757161116\n",
      "    num_agent_steps_sampled: 1243200\n",
      "    num_agent_steps_trained: 1243200\n",
      "    num_steps_sampled: 621600\n",
      "    num_steps_trained: 621600\n",
      "  iterations_since_restore: 222\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.888000000000001\n",
      "    gpu_util_percent0: 0.0596\n",
      "    ram_util_percent: 54.9\n",
      "    vram_util_percent0: 0.21670312500000002\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.000000037252903\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.026999998688697815\n",
      "    agent_1: 0.3480000064522028\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05167830275516408\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8933160962235025\n",
      "    mean_inference_ms: 1.7753282217558217\n",
      "    mean_raw_obs_processing_ms: 0.15873462898738588\n",
      "  time_since_restore: 4123.738299846649\n",
      "  time_this_iter_s: 18.489667892456055\n",
      "  time_total_s: 4123.738299846649\n",
      "  timers:\n",
      "    learn_throughput: 656.696\n",
      "    learn_time_ms: 4263.766\n",
      "    load_throughput: 89529.918\n",
      "    load_time_ms: 31.274\n",
      "    sample_throughput: 196.218\n",
      "    sample_time_ms: 14269.823\n",
      "    update_time_ms: 3.352\n",
      "  timestamp: 1658499048\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 621600\n",
      "  training_iteration: 222\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   222</td><td style=\"text-align: right;\">         4123.74</td><td style=\"text-align: right;\">621600</td><td style=\"text-align: right;\">   0.321</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            187.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1248800\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-11-07\n",
      "  done: false\n",
      "  episode_len_mean: 200.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000037252903\n",
      "  episode_reward_mean: 0.2920000072568655\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 2552\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.2239314406400634\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011687179462942014\n",
      "          policy_loss: -0.07672657268925659\n",
      "          total_loss: -0.06291030298087294\n",
      "          vf_explained_var: 0.06937041878700256\n",
      "          vf_loss: 0.004547822287956175\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.127769724244163\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.013103996606319208\n",
      "          policy_loss: -0.08131806312961826\n",
      "          total_loss: -0.06754895694287673\n",
      "          vf_explained_var: 0.10807622224092484\n",
      "          vf_loss: 0.004424850765818043\n",
      "    num_agent_steps_sampled: 1248800\n",
      "    num_agent_steps_trained: 1248800\n",
      "    num_steps_sampled: 624400\n",
      "    num_steps_trained: 624400\n",
      "  iterations_since_restore: 223\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.8625\n",
      "    gpu_util_percent0: 0.049166666666666664\n",
      "    ram_util_percent: 54.90416666666666\n",
      "    vram_util_percent0: 0.2173502604166667\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.000000037252903\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.04399999916553497\n",
      "    agent_1: 0.33600000642240047\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05167671316966263\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.893319608688955\n",
      "    mean_inference_ms: 1.7752758520728509\n",
      "    mean_raw_obs_processing_ms: 0.15878408614944417\n",
      "  time_since_restore: 4142.506217241287\n",
      "  time_this_iter_s: 18.76791739463806\n",
      "  time_total_s: 4142.506217241287\n",
      "  timers:\n",
      "    learn_throughput: 655.9\n",
      "    learn_time_ms: 4268.94\n",
      "    load_throughput: 83466.66\n",
      "    load_time_ms: 33.546\n",
      "    sample_throughput: 196.57\n",
      "    sample_time_ms: 14244.306\n",
      "    update_time_ms: 3.289\n",
      "  timestamp: 1658499067\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 624400\n",
      "  training_iteration: 223\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   223</td><td style=\"text-align: right;\">         4142.51</td><td style=\"text-align: right;\">624400</td><td style=\"text-align: right;\">   0.292</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            200.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1254400\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-11-26\n",
      "  done: false\n",
      "  episode_len_mean: 198.66\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.32400000773370263\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 2567\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.2059687592443966\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.006230638778903149\n",
      "          policy_loss: 0.0665432852529193\n",
      "          total_loss: 0.07338240709647098\n",
      "          vf_explained_var: 0.020280497148633003\n",
      "          vf_loss: 0.002132745124721883\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.168981440010525\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.007972998814272185\n",
      "          policy_loss: 0.0338602766437578\n",
      "          total_loss: 0.041901904768781695\n",
      "          vf_explained_var: 0.1758691966533661\n",
      "          vf_loss: 0.0027710265079630993\n",
      "    num_agent_steps_sampled: 1254400\n",
      "    num_agent_steps_trained: 1254400\n",
      "    num_steps_sampled: 627200\n",
      "    num_steps_trained: 627200\n",
      "  iterations_since_restore: 224\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.92\n",
      "    gpu_util_percent0: 0.056799999999999996\n",
      "    ram_util_percent: 54.908\n",
      "    vram_util_percent0: 0.21670312500000002\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.023999999165534972\n",
      "    agent_1: 0.34800000689923766\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0516741511409586\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.893327156469711\n",
      "    mean_inference_ms: 1.7751907987632105\n",
      "    mean_raw_obs_processing_ms: 0.15887134013467846\n",
      "  time_since_restore: 4161.193264961243\n",
      "  time_this_iter_s: 18.687047719955444\n",
      "  time_total_s: 4161.193264961243\n",
      "  timers:\n",
      "    learn_throughput: 654.272\n",
      "    learn_time_ms: 4279.563\n",
      "    load_throughput: 83273.13\n",
      "    load_time_ms: 33.624\n",
      "    sample_throughput: 196.407\n",
      "    sample_time_ms: 14256.13\n",
      "    update_time_ms: 3.294\n",
      "  timestamp: 1658499086\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 627200\n",
      "  training_iteration: 224\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   224</td><td style=\"text-align: right;\">         4161.19</td><td style=\"text-align: right;\">627200</td><td style=\"text-align: right;\">   0.324</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            198.66</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1260000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-11-44\n",
      "  done: false\n",
      "  episode_len_mean: 191.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.3430000081658363\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 2577\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.322220630234196\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.009647785799863754\n",
      "          policy_loss: -0.08926822847564338\n",
      "          total_loss: -0.07841423356495354\n",
      "          vf_explained_var: -0.07639440149068832\n",
      "          vf_loss: 0.0027577280663232764\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.146805904096081\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011798279428401356\n",
      "          policy_loss: -0.06175105315352474\n",
      "          total_loss: -0.049790339529517086\n",
      "          vf_explained_var: 0.19748401641845703\n",
      "          vf_loss: 0.003012490364935296\n",
      "    num_agent_steps_sampled: 1260000\n",
      "    num_agent_steps_trained: 1260000\n",
      "    num_steps_sampled: 630000\n",
      "    num_steps_trained: 630000\n",
      "  iterations_since_restore: 225\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.870833333333334\n",
      "    gpu_util_percent0: 0.048749999999999995\n",
      "    ram_util_percent: 54.9\n",
      "    vram_util_percent0: 0.2173502604166667\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.009999998807907105\n",
      "    agent_1: 0.35300000697374345\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051672472506219036\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.893334799739218\n",
      "    mean_inference_ms: 1.7751330630149307\n",
      "    mean_raw_obs_processing_ms: 0.1589311266513639\n",
      "  time_since_restore: 4179.735465764999\n",
      "  time_this_iter_s: 18.542200803756714\n",
      "  time_total_s: 4179.735465764999\n",
      "  timers:\n",
      "    learn_throughput: 655.285\n",
      "    learn_time_ms: 4272.949\n",
      "    load_throughput: 78193.743\n",
      "    load_time_ms: 35.808\n",
      "    sample_throughput: 196.986\n",
      "    sample_time_ms: 14214.207\n",
      "    update_time_ms: 3.31\n",
      "  timestamp: 1658499104\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 630000\n",
      "  training_iteration: 225\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   225</td><td style=\"text-align: right;\">         4179.74</td><td style=\"text-align: right;\">630000</td><td style=\"text-align: right;\">   0.343</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            191.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1265600\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-12-03\n",
      "  done: false\n",
      "  episode_len_mean: 203.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.2970000077784061\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 2589\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.224302890754881\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011942409273746346\n",
      "          policy_loss: -0.06747318351132153\n",
      "          total_loss: -0.05350742033607232\n",
      "          vf_explained_var: -0.0864642783999443\n",
      "          vf_loss: 0.004155342683502351\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.082972714233966\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011559761051473019\n",
      "          policy_loss: -0.06608957752368519\n",
      "          total_loss: -0.05377273565820707\n",
      "          vf_explained_var: 0.0545034296810627\n",
      "          vf_loss: 0.004634220711089992\n",
      "    num_agent_steps_sampled: 1265600\n",
      "    num_agent_steps_trained: 1265600\n",
      "    num_steps_sampled: 632800\n",
      "    num_steps_trained: 632800\n",
      "  iterations_since_restore: 226\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.004000000000001\n",
      "    gpu_util_percent0: 0.06\n",
      "    ram_util_percent: 54.916000000000004\n",
      "    vram_util_percent0: 0.2175390625\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.03299999915063381\n",
      "    agent_1: 0.33000000692903997\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05167033786131812\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.893353825344262\n",
      "    mean_inference_ms: 1.7750544717494632\n",
      "    mean_raw_obs_processing_ms: 0.15898470254190022\n",
      "  time_since_restore: 4198.433100700378\n",
      "  time_this_iter_s: 18.69763493537903\n",
      "  time_total_s: 4198.433100700378\n",
      "  timers:\n",
      "    learn_throughput: 652.897\n",
      "    learn_time_ms: 4288.577\n",
      "    load_throughput: 78124.405\n",
      "    load_time_ms: 35.84\n",
      "    sample_throughput: 196.762\n",
      "    sample_time_ms: 14230.385\n",
      "    update_time_ms: 3.337\n",
      "  timestamp: 1658499123\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 632800\n",
      "  training_iteration: 226\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   226</td><td style=\"text-align: right;\">         4198.43</td><td style=\"text-align: right;\">632800</td><td style=\"text-align: right;\">   0.297</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            203.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1271200\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-12-22\n",
      "  done: false\n",
      "  episode_len_mean: 201.44\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.3080000082403421\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 2605\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.2415576885853494\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.009383425377840194\n",
      "          policy_loss: -0.04425276181092115\n",
      "          total_loss: -0.03252312684864072\n",
      "          vf_explained_var: -0.2150016576051712\n",
      "          vf_loss: 0.006010612081982323\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.0357430705002377\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011164475184832429\n",
      "          policy_loss: -0.07287399537467752\n",
      "          total_loss: -0.060399559908719744\n",
      "          vf_explained_var: 0.25861436128616333\n",
      "          vf_loss: 0.006156429776594797\n",
      "    num_agent_steps_sampled: 1271200\n",
      "    num_agent_steps_trained: 1271200\n",
      "    num_steps_sampled: 635600\n",
      "    num_steps_trained: 635600\n",
      "  iterations_since_restore: 227\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.958333333333333\n",
      "    gpu_util_percent0: 0.04791666666666666\n",
      "    ram_util_percent: 54.99583333333334\n",
      "    vram_util_percent0: 0.21675618489583334\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.020999998673796653\n",
      "    agent_1: 0.3290000069141388\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051666991784685014\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8933728153817997\n",
      "    mean_inference_ms: 1.7749306445948034\n",
      "    mean_raw_obs_processing_ms: 0.1590685950513588\n",
      "  time_since_restore: 4217.0758764743805\n",
      "  time_this_iter_s: 18.642775774002075\n",
      "  time_total_s: 4217.0758764743805\n",
      "  timers:\n",
      "    learn_throughput: 652.501\n",
      "    learn_time_ms: 4291.183\n",
      "    load_throughput: 77849.001\n",
      "    load_time_ms: 35.967\n",
      "    sample_throughput: 196.614\n",
      "    sample_time_ms: 14241.126\n",
      "    update_time_ms: 3.376\n",
      "  timestamp: 1658499142\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 635600\n",
      "  training_iteration: 227\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   227</td><td style=\"text-align: right;\">         4217.08</td><td style=\"text-align: right;\">635600</td><td style=\"text-align: right;\">   0.308</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            201.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1276800\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-12-40\n",
      "  done: false\n",
      "  episode_len_mean: 204.58\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.30000000789761544\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 2620\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.2496647430317744\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010343551987450628\n",
      "          policy_loss: -0.0655565130802556\n",
      "          total_loss: -0.049014424243242025\n",
      "          vf_explained_var: -0.21039454638957977\n",
      "          vf_loss: 0.016674748654622817\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.061481987081823\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011229073314700197\n",
      "          policy_loss: -0.06868177730315697\n",
      "          total_loss: -0.05471833021718859\n",
      "          vf_explained_var: 0.16152112185955048\n",
      "          vf_loss: 0.010253215967254835\n",
      "    num_agent_steps_sampled: 1276800\n",
      "    num_agent_steps_trained: 1276800\n",
      "    num_steps_sampled: 638400\n",
      "    num_steps_trained: 638400\n",
      "  iterations_since_restore: 228\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.8839999999999995\n",
      "    gpu_util_percent0: 0.0604\n",
      "    ram_util_percent: 54.976000000000006\n",
      "    vram_util_percent0: 0.21748437499999998\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.030999998673796655\n",
      "    agent_1: 0.3310000065714121\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05166383392656144\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.893365914670469\n",
      "    mean_inference_ms: 1.7748289668295718\n",
      "    mean_raw_obs_processing_ms: 0.15913246928229838\n",
      "  time_since_restore: 4235.65752530098\n",
      "  time_this_iter_s: 18.58164882659912\n",
      "  time_total_s: 4235.65752530098\n",
      "  timers:\n",
      "    learn_throughput: 652.139\n",
      "    learn_time_ms: 4293.564\n",
      "    load_throughput: 77827.23\n",
      "    load_time_ms: 35.977\n",
      "    sample_throughput: 196.48\n",
      "    sample_time_ms: 14250.8\n",
      "    update_time_ms: 3.341\n",
      "  timestamp: 1658499160\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 638400\n",
      "  training_iteration: 228\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   228</td><td style=\"text-align: right;\">         4235.66</td><td style=\"text-align: right;\">638400</td><td style=\"text-align: right;\">     0.3</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            204.58</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1282400\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-12-59\n",
      "  done: false\n",
      "  episode_len_mean: 203.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.24400000736117364\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 2638\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.234297418878192\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.012858182043895909\n",
      "          policy_loss: -0.07747381023795069\n",
      "          total_loss: -0.06182786913788212\n",
      "          vf_explained_var: 0.16418935358524323\n",
      "          vf_loss: 0.006022556979414297\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.0454455156411444\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.012219457624572142\n",
      "          policy_loss: -0.06674298469131047\n",
      "          total_loss: -0.052974983307378695\n",
      "          vf_explained_var: 0.4582589864730835\n",
      "          vf_loss: 0.006849023725191641\n",
      "    num_agent_steps_sampled: 1282400\n",
      "    num_agent_steps_trained: 1282400\n",
      "    num_steps_sampled: 641200\n",
      "    num_steps_trained: 641200\n",
      "  iterations_since_restore: 229\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.808333333333334\n",
      "    gpu_util_percent0: 0.04958333333333333\n",
      "    ram_util_percent: 54.90833333333333\n",
      "    vram_util_percent0: 0.21656901041666665\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.061999998912215235\n",
      "    agent_1: 0.30600000627338886\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051660218946724526\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8933582050597533\n",
      "    mean_inference_ms: 1.774730380022147\n",
      "    mean_raw_obs_processing_ms: 0.15922032203177822\n",
      "  time_since_restore: 4254.2928721904755\n",
      "  time_this_iter_s: 18.63534688949585\n",
      "  time_total_s: 4254.2928721904755\n",
      "  timers:\n",
      "    learn_throughput: 651.777\n",
      "    learn_time_ms: 4295.947\n",
      "    load_throughput: 83146.374\n",
      "    load_time_ms: 33.676\n",
      "    sample_throughput: 196.381\n",
      "    sample_time_ms: 14257.971\n",
      "    update_time_ms: 3.355\n",
      "  timestamp: 1658499179\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 641200\n",
      "  training_iteration: 229\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   229</td><td style=\"text-align: right;\">         4254.29</td><td style=\"text-align: right;\">641200</td><td style=\"text-align: right;\">   0.244</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            203.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1288000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-13-18\n",
      "  done: false\n",
      "  episode_len_mean: 187.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.31100000731647015\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 2656\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.2064413669563474\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011454088639673751\n",
      "          policy_loss: -0.07278049896038803\n",
      "          total_loss: -0.05668559776226868\n",
      "          vf_explained_var: 0.3201155960559845\n",
      "          vf_loss: 0.011780362481832049\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.0023306789142743\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01437068476935442\n",
      "          policy_loss: -0.08328039056366487\n",
      "          total_loss: -0.06747548149637407\n",
      "          vf_explained_var: 0.55748051404953\n",
      "          vf_loss: 0.006471488057286479\n",
      "    num_agent_steps_sampled: 1288000\n",
      "    num_agent_steps_trained: 1288000\n",
      "    num_steps_sampled: 644000\n",
      "    num_steps_trained: 644000\n",
      "  iterations_since_restore: 230\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.888000000000001\n",
      "    gpu_util_percent0: 0.0588\n",
      "    ram_util_percent: 54.988\n",
      "    vram_util_percent0: 0.21748437499999998\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.020999998897314073\n",
      "    agent_1: 0.3320000062137842\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051657154548857755\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8933319930808876\n",
      "    mean_inference_ms: 1.7746492482995877\n",
      "    mean_raw_obs_processing_ms: 0.15933703183355907\n",
      "  time_since_restore: 4272.944270610809\n",
      "  time_this_iter_s: 18.651398420333862\n",
      "  time_total_s: 4272.944270610809\n",
      "  timers:\n",
      "    learn_throughput: 651.952\n",
      "    learn_time_ms: 4294.798\n",
      "    load_throughput: 83008.913\n",
      "    load_time_ms: 33.731\n",
      "    sample_throughput: 196.262\n",
      "    sample_time_ms: 14266.675\n",
      "    update_time_ms: 3.573\n",
      "  timestamp: 1658499198\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 644000\n",
      "  training_iteration: 230\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   230</td><td style=\"text-align: right;\">         4272.94</td><td style=\"text-align: right;\">644000</td><td style=\"text-align: right;\">   0.311</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            187.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1293600\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-13-36\n",
      "  done: false\n",
      "  episode_len_mean: 180.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000014901161\n",
      "  episode_reward_mean: 0.24000000655651094\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 2673\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.1992702051287605\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011961379126473553\n",
      "          policy_loss: -0.051856346821988974\n",
      "          total_loss: -0.036340225177506603\n",
      "          vf_explained_var: 0.09650807082653046\n",
      "          vf_loss: 0.008490462918070122\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.087770832081636\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01014303264746099\n",
      "          policy_loss: -0.06909304119714459\n",
      "          total_loss: -0.05694955060191985\n",
      "          vf_explained_var: 0.40286725759506226\n",
      "          vf_loss: 0.008189336138977004\n",
      "    num_agent_steps_sampled: 1293600\n",
      "    num_agent_steps_trained: 1293600\n",
      "    num_steps_sampled: 646800\n",
      "    num_steps_trained: 646800\n",
      "  iterations_since_restore: 231\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.954166666666667\n",
      "    gpu_util_percent0: 0.050416666666666665\n",
      "    ram_util_percent: 55.05833333333334\n",
      "    vram_util_percent0: 0.2173502604166667\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.000000014901161\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.06499999925494195\n",
      "    agent_1: 0.30500000581145287\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051654535162133114\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8933071222032716\n",
      "    mean_inference_ms: 1.7745855959608874\n",
      "    mean_raw_obs_processing_ms: 0.15946533352777156\n",
      "  time_since_restore: 4291.571401357651\n",
      "  time_this_iter_s: 18.62713074684143\n",
      "  time_total_s: 4291.571401357651\n",
      "  timers:\n",
      "    learn_throughput: 652.419\n",
      "    learn_time_ms: 4291.722\n",
      "    load_throughput: 83290.434\n",
      "    load_time_ms: 33.617\n",
      "    sample_throughput: 196.44\n",
      "    sample_time_ms: 14253.699\n",
      "    update_time_ms: 3.582\n",
      "  timestamp: 1658499216\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 646800\n",
      "  training_iteration: 231\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   231</td><td style=\"text-align: right;\">         4291.57</td><td style=\"text-align: right;\">646800</td><td style=\"text-align: right;\">    0.24</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            180.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1299200\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-13-55\n",
      "  done: false\n",
      "  episode_len_mean: 167.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000014901161\n",
      "  episode_reward_mean: 0.2280000064522028\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 2689\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.2340045362001373\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010282431698353084\n",
      "          policy_loss: -0.08649963250103437\n",
      "          total_loss: -0.07430974808414162\n",
      "          vf_explained_var: 0.2669382691383362\n",
      "          vf_loss: 0.0044283437639803325\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.012225422476019\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010101538413586044\n",
      "          policy_loss: -0.08136019770845018\n",
      "          total_loss: -0.07022541222319151\n",
      "          vf_explained_var: 0.3771377205848694\n",
      "          vf_loss: 0.0053385914027222455\n",
      "    num_agent_steps_sampled: 1299200\n",
      "    num_agent_steps_trained: 1299200\n",
      "    num_steps_sampled: 649600\n",
      "    num_steps_trained: 649600\n",
      "  iterations_since_restore: 232\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.904\n",
      "    gpu_util_percent0: 0.0548\n",
      "    ram_util_percent: 55.20000000000001\n",
      "    vram_util_percent0: 0.2167109375\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.000000014901161\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.0709999992698431\n",
      "    agent_1: 0.2990000057220459\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05165222584880449\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.893292690437125\n",
      "    mean_inference_ms: 1.7745324934918079\n",
      "    mean_raw_obs_processing_ms: 0.1596084289668597\n",
      "  time_since_restore: 4310.162281990051\n",
      "  time_this_iter_s: 18.590880632400513\n",
      "  time_total_s: 4310.162281990051\n",
      "  timers:\n",
      "    learn_throughput: 653.448\n",
      "    learn_time_ms: 4284.962\n",
      "    load_throughput: 83212.888\n",
      "    load_time_ms: 33.649\n",
      "    sample_throughput: 196.243\n",
      "    sample_time_ms: 14267.991\n",
      "    update_time_ms: 3.501\n",
      "  timestamp: 1658499235\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 649600\n",
      "  training_iteration: 232\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   232</td><td style=\"text-align: right;\">         4310.16</td><td style=\"text-align: right;\">649600</td><td style=\"text-align: right;\">   0.228</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             167.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1304800\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-14-14\n",
      "  done: false\n",
      "  episode_len_mean: 168.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000014901161\n",
      "  episode_reward_mean: 0.30400000542402267\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 2705\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.2185290274875507\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011648643067249902\n",
      "          policy_loss: -0.06817000182421734\n",
      "          total_loss: -0.051216889488322134\n",
      "          vf_explained_var: 0.26040664315223694\n",
      "          vf_loss: 0.013619820568454174\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.0926122711527917\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01259681602794826\n",
      "          policy_loss: -0.08266846643632994\n",
      "          total_loss: -0.06853002120658944\n",
      "          vf_explained_var: 0.4117833077907562\n",
      "          vf_loss: 0.006885283731814222\n",
      "    num_agent_steps_sampled: 1304800\n",
      "    num_agent_steps_trained: 1304800\n",
      "    num_steps_sampled: 652400\n",
      "    num_steps_trained: 652400\n",
      "  iterations_since_restore: 233\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.966666666666666\n",
      "    gpu_util_percent0: 0.05375\n",
      "    ram_util_percent: 55.09583333333333\n",
      "    vram_util_percent0: 0.21779378255208334\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.000000014901161\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.03299999974668026\n",
      "    agent_1: 0.33700000517070294\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05165033717461576\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8932806496591432\n",
      "    mean_inference_ms: 1.7744943002883389\n",
      "    mean_raw_obs_processing_ms: 0.15974943908094816\n",
      "  time_since_restore: 4328.793079614639\n",
      "  time_this_iter_s: 18.630797624588013\n",
      "  time_total_s: 4328.793079614639\n",
      "  timers:\n",
      "    learn_throughput: 653.811\n",
      "    learn_time_ms: 4282.584\n",
      "    load_throughput: 89034.972\n",
      "    load_time_ms: 31.448\n",
      "    sample_throughput: 196.369\n",
      "    sample_time_ms: 14258.883\n",
      "    update_time_ms: 3.507\n",
      "  timestamp: 1658499254\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 652400\n",
      "  training_iteration: 233\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   233</td><td style=\"text-align: right;\">         4328.79</td><td style=\"text-align: right;\">652400</td><td style=\"text-align: right;\">   0.304</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            168.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1310400\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-14-32\n",
      "  done: false\n",
      "  episode_len_mean: 176.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000014901161\n",
      "  episode_reward_mean: 0.29300000593066217\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 2715\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.201354216961634\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01445531451356054\n",
      "          policy_loss: -0.07563228297415965\n",
      "          total_loss: -0.05908178375587644\n",
      "          vf_explained_var: 0.18218789994716644\n",
      "          vf_loss: 0.003436464742532865\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.109284172810259\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.012151564158041755\n",
      "          policy_loss: -0.08438126818111327\n",
      "          total_loss: -0.07203721870939175\n",
      "          vf_explained_var: 0.3185807764530182\n",
      "          vf_loss: 0.003053729834694726\n",
      "    num_agent_steps_sampled: 1310400\n",
      "    num_agent_steps_trained: 1310400\n",
      "    num_steps_sampled: 655200\n",
      "    num_steps_trained: 655200\n",
      "  iterations_since_restore: 234\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.962500000000001\n",
      "    gpu_util_percent0: 0.04958333333333333\n",
      "    ram_util_percent: 55.012499999999996\n",
      "    vram_util_percent0: 0.21695149739583333\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.000000014901161\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.042999999746680256\n",
      "    agent_1: 0.3360000056773424\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05164927224993484\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8932576690282628\n",
      "    mean_inference_ms: 1.7744566007424316\n",
      "    mean_raw_obs_processing_ms: 0.15982259293254106\n",
      "  time_since_restore: 4347.23118686676\n",
      "  time_this_iter_s: 18.43810725212097\n",
      "  time_total_s: 4347.23118686676\n",
      "  timers:\n",
      "    learn_throughput: 656.22\n",
      "    learn_time_ms: 4266.862\n",
      "    load_throughput: 89154.202\n",
      "    load_time_ms: 31.406\n",
      "    sample_throughput: 196.495\n",
      "    sample_time_ms: 14249.751\n",
      "    update_time_ms: 3.516\n",
      "  timestamp: 1658499272\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 655200\n",
      "  training_iteration: 234\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   234</td><td style=\"text-align: right;\">         4347.23</td><td style=\"text-align: right;\">655200</td><td style=\"text-align: right;\">   0.293</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            176.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1316000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-14-51\n",
      "  done: false\n",
      "  episode_len_mean: 179.54\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000014901161\n",
      "  episode_reward_mean: 0.31100000597536565\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 2729\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.2332220446495783\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011110123058648494\n",
      "          policy_loss: -0.06230131124077941\n",
      "          total_loss: -0.0495602948150398\n",
      "          vf_explained_var: 0.39542898535728455\n",
      "          vf_loss: 0.003342578982613174\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.9990298265502566\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011819581954949502\n",
      "          policy_loss: -0.07763590532873163\n",
      "          total_loss: -0.06530861616470031\n",
      "          vf_explained_var: 0.300923228263855\n",
      "          vf_loss: 0.003822676224704732\n",
      "    num_agent_steps_sampled: 1316000\n",
      "    num_agent_steps_trained: 1316000\n",
      "    num_steps_sampled: 658000\n",
      "    num_steps_trained: 658000\n",
      "  iterations_since_restore: 235\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.868\n",
      "    gpu_util_percent0: 0.0564\n",
      "    ram_util_percent: 54.992000000000004\n",
      "    vram_util_percent0: 0.21767968750000002\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.000000014901161\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.03299999974668026\n",
      "    agent_1: 0.3440000057220459\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051647729471022114\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.893242795693876\n",
      "    mean_inference_ms: 1.7743948536325618\n",
      "    mean_raw_obs_processing_ms: 0.1599178963531737\n",
      "  time_since_restore: 4365.986661672592\n",
      "  time_this_iter_s: 18.75547480583191\n",
      "  time_total_s: 4365.986661672592\n",
      "  timers:\n",
      "    learn_throughput: 656.575\n",
      "    learn_time_ms: 4264.555\n",
      "    load_throughput: 95742.254\n",
      "    load_time_ms: 29.245\n",
      "    sample_throughput: 196.144\n",
      "    sample_time_ms: 14275.256\n",
      "    update_time_ms: 3.514\n",
      "  timestamp: 1658499291\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 658000\n",
      "  training_iteration: 235\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   235</td><td style=\"text-align: right;\">         4365.99</td><td style=\"text-align: right;\">658000</td><td style=\"text-align: right;\">   0.311</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            179.54</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1321600\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-15-10\n",
      "  done: false\n",
      "  episode_len_mean: 186.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.999999962747097\n",
      "  episode_reward_mean: 0.2890000058710575\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 2742\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.2206082294384637\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011591316301422346\n",
      "          policy_loss: -0.08460040916110129\n",
      "          total_loss: -0.07145381058552673\n",
      "          vf_explained_var: 0.09782636910676956\n",
      "          vf_loss: 0.0029400490550306997\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.0477341498647417\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.012762770942900506\n",
      "          policy_loss: -0.09367342174075366\n",
      "          total_loss: -0.08087396499398719\n",
      "          vf_explained_var: 0.21945635974407196\n",
      "          vf_loss: 0.0025358907990136815\n",
      "    num_agent_steps_sampled: 1321600\n",
      "    num_agent_steps_trained: 1321600\n",
      "    num_steps_sampled: 660800\n",
      "    num_steps_trained: 660800\n",
      "  iterations_since_restore: 236\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.866666666666667\n",
      "    gpu_util_percent0: 0.049166666666666664\n",
      "    ram_util_percent: 54.99583333333334\n",
      "    vram_util_percent0: 0.21754557291666665\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 1.999999962747097\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.053999999985098836\n",
      "    agent_1: 0.34300000585615636\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051646156242461085\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.893234509731075\n",
      "    mean_inference_ms: 1.7743402067635072\n",
      "    mean_raw_obs_processing_ms: 0.15999535548364052\n",
      "  time_since_restore: 4384.64005279541\n",
      "  time_this_iter_s: 18.653391122817993\n",
      "  time_total_s: 4384.64005279541\n",
      "  timers:\n",
      "    learn_throughput: 658.463\n",
      "    learn_time_ms: 4252.33\n",
      "    load_throughput: 95506.025\n",
      "    load_time_ms: 29.318\n",
      "    sample_throughput: 196.037\n",
      "    sample_time_ms: 14282.995\n",
      "    update_time_ms: 3.504\n",
      "  timestamp: 1658499310\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 660800\n",
      "  training_iteration: 236\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   236</td><td style=\"text-align: right;\">         4384.64</td><td style=\"text-align: right;\">660800</td><td style=\"text-align: right;\">   0.289</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            186.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1327200\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-15-28\n",
      "  done: false\n",
      "  episode_len_mean: 198.18\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000037252903\n",
      "  episode_reward_mean: 0.22400000616908072\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 2755\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.145117626303718\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.009948488956360631\n",
      "          policy_loss: -0.05232237592108071\n",
      "          total_loss: -0.038458894470403765\n",
      "          vf_explained_var: -0.09758009016513824\n",
      "          vf_loss: 0.010172826081023751\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.0206606512268386\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01032787639136226\n",
      "          policy_loss: -0.0615171818365828\n",
      "          total_loss: -0.04916728167778014\n",
      "          vf_explained_var: 0.12594056129455566\n",
      "          vf_loss: 0.008171219614009064\n",
      "    num_agent_steps_sampled: 1327200\n",
      "    num_agent_steps_trained: 1327200\n",
      "    num_steps_sampled: 663600\n",
      "    num_steps_trained: 663600\n",
      "  iterations_since_restore: 237\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.892\n",
      "    gpu_util_percent0: 0.055999999999999994\n",
      "    ram_util_percent: 54.96\n",
      "    vram_util_percent0: 0.2168984375\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.000000037252903\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.085\n",
      "    agent_1: 0.30900000616908074\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05164488264751208\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.89322660301267\n",
      "    mean_inference_ms: 1.7742944729765213\n",
      "    mean_raw_obs_processing_ms: 0.16006231845506394\n",
      "  time_since_restore: 4403.297813177109\n",
      "  time_this_iter_s: 18.65776038169861\n",
      "  time_total_s: 4403.297813177109\n",
      "  timers:\n",
      "    learn_throughput: 659.367\n",
      "    learn_time_ms: 4246.494\n",
      "    load_throughput: 95837.886\n",
      "    load_time_ms: 29.216\n",
      "    sample_throughput: 195.934\n",
      "    sample_time_ms: 14290.547\n",
      "    update_time_ms: 3.482\n",
      "  timestamp: 1658499328\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 663600\n",
      "  training_iteration: 237\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   237</td><td style=\"text-align: right;\">          4403.3</td><td style=\"text-align: right;\">663600</td><td style=\"text-align: right;\">   0.224</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            198.18</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1332800\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-15-47\n",
      "  done: false\n",
      "  episode_len_mean: 195.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000037252903\n",
      "  episode_reward_mean: 0.25000000625848773\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 2771\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.126127943751358\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.012651068787143378\n",
      "          policy_loss: -0.09029555836847673\n",
      "          total_loss: -0.0757326015733443\n",
      "          vf_explained_var: 0.17357279360294342\n",
      "          vf_loss: 0.0034677145659294355\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.9743558668664523\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.012708004198332946\n",
      "          policy_loss: -0.08013608649586483\n",
      "          total_loss: -0.06670428791853954\n",
      "          vf_explained_var: 0.15558941662311554\n",
      "          vf_loss: 0.004410243217578335\n",
      "    num_agent_steps_sampled: 1332800\n",
      "    num_agent_steps_trained: 1332800\n",
      "    num_steps_sampled: 666400\n",
      "    num_steps_trained: 666400\n",
      "  iterations_since_restore: 238\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.868\n",
      "    gpu_util_percent0: 0.0552\n",
      "    ram_util_percent: 55.0\n",
      "    vram_util_percent0: 0.21767968750000002\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.000000037252903\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.07\n",
      "    agent_1: 0.3200000062584877\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05164391314552532\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8932597944781935\n",
      "    mean_inference_ms: 1.7742658910284934\n",
      "    mean_raw_obs_processing_ms: 0.16014285260050962\n",
      "  time_since_restore: 4422.302310228348\n",
      "  time_this_iter_s: 19.004497051239014\n",
      "  time_total_s: 4422.302310228348\n",
      "  timers:\n",
      "    learn_throughput: 658.418\n",
      "    learn_time_ms: 4252.62\n",
      "    load_throughput: 95757.632\n",
      "    load_time_ms: 29.24\n",
      "    sample_throughput: 195.438\n",
      "    sample_time_ms: 14326.762\n",
      "    update_time_ms: 3.519\n",
      "  timestamp: 1658499347\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 666400\n",
      "  training_iteration: 238\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   238</td><td style=\"text-align: right;\">          4422.3</td><td style=\"text-align: right;\">666400</td><td style=\"text-align: right;\">    0.25</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            195.47</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1338400\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-16-06\n",
      "  done: false\n",
      "  episode_len_mean: 198.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000037252903\n",
      "  episode_reward_mean: 0.26600000634789467\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 2788\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.208290601060504\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.009941131958951552\n",
      "          policy_loss: -0.06341908886852685\n",
      "          total_loss: -0.05202534207234878\n",
      "          vf_explained_var: 0.303507000207901\n",
      "          vf_loss: 0.003221189169423832\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.0263912684860683\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011266908557056022\n",
      "          policy_loss: -0.06386480539991421\n",
      "          total_loss: -0.05187701671301121\n",
      "          vf_explained_var: 0.4037622809410095\n",
      "          vf_loss: 0.004463700967400135\n",
      "    num_agent_steps_sampled: 1338400\n",
      "    num_agent_steps_trained: 1338400\n",
      "    num_steps_sampled: 669200\n",
      "    num_steps_trained: 669200\n",
      "  iterations_since_restore: 239\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.952000000000001\n",
      "    gpu_util_percent0: 0.05\n",
      "    ram_util_percent: 55.02\n",
      "    vram_util_percent0: 0.21703125000000004\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.000000037252903\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.06899999998509884\n",
      "    agent_1: 0.3350000063329935\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051643700145646954\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.893349286257279\n",
      "    mean_inference_ms: 1.7742597804200597\n",
      "    mean_raw_obs_processing_ms: 0.16023524256514093\n",
      "  time_since_restore: 4441.45139670372\n",
      "  time_this_iter_s: 19.149086475372314\n",
      "  time_total_s: 4441.45139670372\n",
      "  timers:\n",
      "    learn_throughput: 655.344\n",
      "    learn_time_ms: 4272.567\n",
      "    load_throughput: 95938.725\n",
      "    load_time_ms: 29.185\n",
      "    sample_throughput: 195.013\n",
      "    sample_time_ms: 14357.992\n",
      "    update_time_ms: 3.508\n",
      "  timestamp: 1658499366\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 669200\n",
      "  training_iteration: 239\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   239</td><td style=\"text-align: right;\">         4441.45</td><td style=\"text-align: right;\">669200</td><td style=\"text-align: right;\">   0.266</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            198.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1344000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-16-25\n",
      "  done: false\n",
      "  episode_len_mean: 197.14\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000037252903\n",
      "  episode_reward_mean: 0.28400000765919686\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 2804\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.2081884532457305\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.008582255612089891\n",
      "          policy_loss: -0.06165201650288682\n",
      "          total_loss: -0.04784534496242746\n",
      "          vf_explained_var: -0.4451819956302643\n",
      "          vf_loss: 0.014473200671740674\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.0524085753020787\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011901956169738754\n",
      "          policy_loss: -0.05848976472595625\n",
      "          total_loss: -0.045407038415327235\n",
      "          vf_explained_var: 0.2835250496864319\n",
      "          vf_loss: 0.005807414119079199\n",
      "    num_agent_steps_sampled: 1344000\n",
      "    num_agent_steps_trained: 1344000\n",
      "    num_steps_sampled: 672000\n",
      "    num_steps_trained: 672000\n",
      "  iterations_since_restore: 240\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.9\n",
      "    gpu_util_percent0: 0.057999999999999996\n",
      "    ram_util_percent: 55.008\n",
      "    vram_util_percent0: 0.21756640625000004\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.000000037252903\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.0559999992698431\n",
      "    agent_1: 0.34000000692904\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051643855755897104\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.893445696518113\n",
      "    mean_inference_ms: 1.774264587080385\n",
      "    mean_raw_obs_processing_ms: 0.16032116401178745\n",
      "  time_since_restore: 4460.121467828751\n",
      "  time_this_iter_s: 18.670071125030518\n",
      "  time_total_s: 4460.121467828751\n",
      "  timers:\n",
      "    learn_throughput: 654.388\n",
      "    learn_time_ms: 4278.809\n",
      "    load_throughput: 96099.346\n",
      "    load_time_ms: 29.137\n",
      "    sample_throughput: 195.064\n",
      "    sample_time_ms: 14354.26\n",
      "    update_time_ms: 3.283\n",
      "  timestamp: 1658499385\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 672000\n",
      "  training_iteration: 240\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   240</td><td style=\"text-align: right;\">         4460.12</td><td style=\"text-align: right;\">672000</td><td style=\"text-align: right;\">   0.284</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            197.14</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1349600\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-16-44\n",
      "  done: false\n",
      "  episode_len_mean: 185.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000037252903\n",
      "  episode_reward_mean: 0.2780000075697899\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 2821\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.2108364694175267\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011834914585178475\n",
      "          policy_loss: -0.0769128147678982\n",
      "          total_loss: -0.06319606163318829\n",
      "          vf_explained_var: 0.12280973792076111\n",
      "          vf_loss: 0.003773729205758649\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.0617482300315584\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.012059043057676038\n",
      "          policy_loss: -0.07728985856720967\n",
      "          total_loss: -0.06493167502897752\n",
      "          vf_explained_var: 0.4110589325428009\n",
      "          vf_loss: 0.003301751443013061\n",
      "    num_agent_steps_sampled: 1349600\n",
      "    num_agent_steps_trained: 1349600\n",
      "    num_steps_sampled: 674800\n",
      "    num_steps_trained: 674800\n",
      "  iterations_since_restore: 241\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.864\n",
      "    gpu_util_percent0: 0.0584\n",
      "    ram_util_percent: 55.0\n",
      "    vram_util_percent0: 0.21740234375\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.000000037252903\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.05399999924004078\n",
      "    agent_1: 0.33200000680983066\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05164432724401764\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8935848847264314\n",
      "    mean_inference_ms: 1.7742917090391916\n",
      "    mean_raw_obs_processing_ms: 0.16043936390968777\n",
      "  time_since_restore: 4478.9256772994995\n",
      "  time_this_iter_s: 18.8042094707489\n",
      "  time_total_s: 4478.9256772994995\n",
      "  timers:\n",
      "    learn_throughput: 655.026\n",
      "    learn_time_ms: 4274.638\n",
      "    load_throughput: 95871.371\n",
      "    load_time_ms: 29.206\n",
      "    sample_throughput: 194.805\n",
      "    sample_time_ms: 14373.373\n",
      "    update_time_ms: 3.267\n",
      "  timestamp: 1658499404\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 674800\n",
      "  training_iteration: 241\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   241</td><td style=\"text-align: right;\">         4478.93</td><td style=\"text-align: right;\">674800</td><td style=\"text-align: right;\">   0.278</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            185.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1355200\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-17-03\n",
      "  done: false\n",
      "  episode_len_mean: 178.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000037252903\n",
      "  episode_reward_mean: 0.26400000758469105\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 2835\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.1991096415690015\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011971901235200326\n",
      "          policy_loss: -0.06759506860018541\n",
      "          total_loss: -0.05311903739580191\n",
      "          vf_explained_var: 0.3872721493244171\n",
      "          vf_loss: 0.005487364526960023\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.024714458911192\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010900389780093708\n",
      "          policy_loss: -0.05593612351055656\n",
      "          total_loss: -0.043630209629468265\n",
      "          vf_explained_var: 0.3695572018623352\n",
      "          vf_loss: 0.006416130219198142\n",
      "    num_agent_steps_sampled: 1355200\n",
      "    num_agent_steps_trained: 1355200\n",
      "    num_steps_sampled: 677600\n",
      "    num_steps_trained: 677600\n",
      "  iterations_since_restore: 242\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.908333333333334\n",
      "    gpu_util_percent0: 0.049166666666666664\n",
      "    ram_util_percent: 54.94583333333333\n",
      "    vram_util_percent0: 0.21666666666666665\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.000000037252903\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.06399999924004078\n",
      "    agent_1: 0.32800000682473185\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05164527269263484\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8936974168130347\n",
      "    mean_inference_ms: 1.7743395972789573\n",
      "    mean_raw_obs_processing_ms: 0.1605383490560352\n",
      "  time_since_restore: 4497.75608587265\n",
      "  time_this_iter_s: 18.830408573150635\n",
      "  time_total_s: 4497.75608587265\n",
      "  timers:\n",
      "    learn_throughput: 653.362\n",
      "    learn_time_ms: 4285.529\n",
      "    load_throughput: 95823.81\n",
      "    load_time_ms: 29.22\n",
      "    sample_throughput: 194.588\n",
      "    sample_time_ms: 14389.374\n",
      "    update_time_ms: 3.273\n",
      "  timestamp: 1658499423\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 677600\n",
      "  training_iteration: 242\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   242</td><td style=\"text-align: right;\">         4497.76</td><td style=\"text-align: right;\">677600</td><td style=\"text-align: right;\">   0.264</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            178.39</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1360800\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-17-22\n",
      "  done: false\n",
      "  episode_len_mean: 180.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000037252903\n",
      "  episode_reward_mean: 0.2990000076591969\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 2850\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.1971942150876638\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01145175397319291\n",
      "          policy_loss: -0.07219684372484196\n",
      "          total_loss: -0.0591621657256924\n",
      "          vf_explained_var: 0.289036363363266\n",
      "          vf_loss: 0.0030409683056080083\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.0358733314843405\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.012632973154188109\n",
      "          policy_loss: -0.062286076752657425\n",
      "          total_loss: -0.04876544118867189\n",
      "          vf_explained_var: 0.40946871042251587\n",
      "          vf_loss: 0.004951062684302174\n",
      "    num_agent_steps_sampled: 1360800\n",
      "    num_agent_steps_trained: 1360800\n",
      "    num_steps_sampled: 680400\n",
      "    num_steps_trained: 680400\n",
      "  iterations_since_restore: 243\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.895999999999999\n",
      "    gpu_util_percent0: 0.0552\n",
      "    ram_util_percent: 54.92\n",
      "    vram_util_percent0: 0.21758203125\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.000000037252903\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.04599999912083149\n",
      "    agent_1: 0.34500000678002835\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05164674033568916\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8938568513052565\n",
      "    mean_inference_ms: 1.7743966998206167\n",
      "    mean_raw_obs_processing_ms: 0.16065133336640589\n",
      "  time_since_restore: 4516.61553311348\n",
      "  time_this_iter_s: 18.859447240829468\n",
      "  time_total_s: 4516.61553311348\n",
      "  timers:\n",
      "    learn_throughput: 654.851\n",
      "    learn_time_ms: 4275.781\n",
      "    load_throughput: 89019.383\n",
      "    load_time_ms: 31.454\n",
      "    sample_throughput: 194.178\n",
      "    sample_time_ms: 14419.757\n",
      "    update_time_ms: 3.275\n",
      "  timestamp: 1658499442\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 680400\n",
      "  training_iteration: 243\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   243</td><td style=\"text-align: right;\">         4516.62</td><td style=\"text-align: right;\">680400</td><td style=\"text-align: right;\">   0.299</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            180.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1366400\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-17-41\n",
      "  done: false\n",
      "  episode_len_mean: 182.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.3330000078678131\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 2864\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.2554527464367093\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.008172182580960344\n",
      "          policy_loss: -0.0652775279844978\n",
      "          total_loss: -0.052968120422779714\n",
      "          vf_explained_var: 0.01232117135077715\n",
      "          vf_loss: 0.011572077852872013\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.039413963754972\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.012446474161228535\n",
      "          policy_loss: -0.10620884880093702\n",
      "          total_loss: -0.09278463460941566\n",
      "          vf_explained_var: 0.1070164367556572\n",
      "          vf_loss: 0.005212415540535446\n",
      "    num_agent_steps_sampled: 1366400\n",
      "    num_agent_steps_trained: 1366400\n",
      "    num_steps_sampled: 683200\n",
      "    num_steps_trained: 683200\n",
      "  iterations_since_restore: 244\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.86\n",
      "    gpu_util_percent0: 0.048799999999999996\n",
      "    ram_util_percent: 54.92800000000001\n",
      "    vram_util_percent0: 0.21680078125\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.03099999912083149\n",
      "    agent_1: 0.3640000069886446\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05164827788325122\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8940192966474307\n",
      "    mean_inference_ms: 1.7744654853173478\n",
      "    mean_raw_obs_processing_ms: 0.16075191909782574\n",
      "  time_since_restore: 4535.587874174118\n",
      "  time_this_iter_s: 18.972341060638428\n",
      "  time_total_s: 4535.587874174118\n",
      "  timers:\n",
      "    learn_throughput: 655.34\n",
      "    learn_time_ms: 4272.591\n",
      "    load_throughput: 89068.668\n",
      "    load_time_ms: 31.436\n",
      "    sample_throughput: 193.418\n",
      "    sample_time_ms: 14476.443\n",
      "    update_time_ms: 3.286\n",
      "  timestamp: 1658499461\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 683200\n",
      "  training_iteration: 244\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   244</td><td style=\"text-align: right;\">         4535.59</td><td style=\"text-align: right;\">683200</td><td style=\"text-align: right;\">   0.333</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            182.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1372000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-18-00\n",
      "  done: false\n",
      "  episode_len_mean: 181.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.3390000081807375\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 2881\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.108636812085197\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011372930393752126\n",
      "          policy_loss: -0.057007514347787946\n",
      "          total_loss: -0.043066774623336336\n",
      "          vf_explained_var: -0.07129595428705215\n",
      "          vf_loss: 0.005775468347391801\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.9851545822762309\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01069058152126908\n",
      "          policy_loss: -0.07172841312933001\n",
      "          total_loss: -0.060386053881562214\n",
      "          vf_explained_var: 0.42914190888404846\n",
      "          vf_loss: 0.00421746914590975\n",
      "    num_agent_steps_sampled: 1372000\n",
      "    num_agent_steps_trained: 1372000\n",
      "    num_steps_sampled: 686000\n",
      "    num_steps_trained: 686000\n",
      "  iterations_since_restore: 245\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.879999999999999\n",
      "    gpu_util_percent0: 0.0544\n",
      "    ram_util_percent: 54.94400000000002\n",
      "    vram_util_percent0: 0.21758203125\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.013999998569488525\n",
      "    agent_1: 0.353000006750226\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05165029423791086\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.894189821164931\n",
      "    mean_inference_ms: 1.7745491740616728\n",
      "    mean_raw_obs_processing_ms: 0.16087446084317644\n",
      "  time_since_restore: 4554.391361474991\n",
      "  time_this_iter_s: 18.803487300872803\n",
      "  time_total_s: 4554.391361474991\n",
      "  timers:\n",
      "    learn_throughput: 656.479\n",
      "    learn_time_ms: 4265.176\n",
      "    load_throughput: 88938.013\n",
      "    load_time_ms: 31.483\n",
      "    sample_throughput: 193.253\n",
      "    sample_time_ms: 14488.758\n",
      "    update_time_ms: 3.289\n",
      "  timestamp: 1658499480\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 686000\n",
      "  training_iteration: 245\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   245</td><td style=\"text-align: right;\">         4554.39</td><td style=\"text-align: right;\">686000</td><td style=\"text-align: right;\">   0.339</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            181.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1377600\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-18-19\n",
      "  done: false\n",
      "  episode_len_mean: 174.23\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.33400000773370264\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 2899\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.1797374547237442\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.012470674951609716\n",
      "          policy_loss: -0.07701831051409022\n",
      "          total_loss: -0.06228613985530544\n",
      "          vf_explained_var: 0.20286805927753448\n",
      "          vf_loss: 0.0045937539296227485\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.011502005869434\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01247928385970035\n",
      "          policy_loss: -0.05221468998960128\n",
      "          total_loss: -0.03908611087358406\n",
      "          vf_explained_var: 0.5634720325469971\n",
      "          vf_loss: 0.0042416713058628375\n",
      "    num_agent_steps_sampled: 1377600\n",
      "    num_agent_steps_trained: 1377600\n",
      "    num_steps_sampled: 688800\n",
      "    num_steps_trained: 688800\n",
      "  iterations_since_restore: 246\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.9639999999999995\n",
      "    gpu_util_percent0: 0.0592\n",
      "    ram_util_percent: 55.016000000000005\n",
      "    vram_util_percent0: 0.217875\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.01699999898672104\n",
      "    agent_1: 0.3510000067204237\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0516529683865144\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.894370663895295\n",
      "    mean_inference_ms: 1.7746557485074521\n",
      "    mean_raw_obs_processing_ms: 0.1610109460343486\n",
      "  time_since_restore: 4573.326755523682\n",
      "  time_this_iter_s: 18.935394048690796\n",
      "  time_total_s: 4573.326755523682\n",
      "  timers:\n",
      "    learn_throughput: 657.334\n",
      "    learn_time_ms: 4259.634\n",
      "    load_throughput: 89158.06\n",
      "    load_time_ms: 31.405\n",
      "    sample_throughput: 192.841\n",
      "    sample_time_ms: 14519.728\n",
      "    update_time_ms: 3.279\n",
      "  timestamp: 1658499499\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 688800\n",
      "  training_iteration: 246\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   246</td><td style=\"text-align: right;\">         4573.33</td><td style=\"text-align: right;\">688800</td><td style=\"text-align: right;\">   0.334</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            174.23</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1383200\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-18-37\n",
      "  done: false\n",
      "  episode_len_mean: 175.17\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.2950000075250864\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 2915\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.162825135248048\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01028403416481204\n",
      "          policy_loss: -0.04879405590470144\n",
      "          total_loss: -0.03656087160509612\n",
      "          vf_explained_var: 0.008358342573046684\n",
      "          vf_loss: 0.004462291099982741\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.0151049970161345\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01064206707026884\n",
      "          policy_loss: -0.05013007635716349\n",
      "          total_loss: -0.03855517698961887\n",
      "          vf_explained_var: 0.07831686735153198\n",
      "          vf_loss: 0.005055356070702796\n",
      "    num_agent_steps_sampled: 1383200\n",
      "    num_agent_steps_trained: 1383200\n",
      "    num_steps_sampled: 691600\n",
      "    num_steps_trained: 691600\n",
      "  iterations_since_restore: 247\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.216666666666666\n",
      "    gpu_util_percent0: 0.050416666666666665\n",
      "    ram_util_percent: 55.19166666666666\n",
      "    vram_util_percent0: 0.21666666666666665\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.04399999924004078\n",
      "    agent_1: 0.3390000067651272\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051655955470180856\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8945238881640853\n",
      "    mean_inference_ms: 1.7747854700996077\n",
      "    mean_raw_obs_processing_ms: 0.16112807951394623\n",
      "  time_since_restore: 4592.09333896637\n",
      "  time_this_iter_s: 18.76658344268799\n",
      "  time_total_s: 4592.09333896637\n",
      "  timers:\n",
      "    learn_throughput: 659.459\n",
      "    learn_time_ms: 4245.902\n",
      "    load_throughput: 89114.22\n",
      "    load_time_ms: 31.42\n",
      "    sample_throughput: 192.515\n",
      "    sample_time_ms: 14544.316\n",
      "    update_time_ms: 3.26\n",
      "  timestamp: 1658499517\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 691600\n",
      "  training_iteration: 247\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   247</td><td style=\"text-align: right;\">         4592.09</td><td style=\"text-align: right;\">691600</td><td style=\"text-align: right;\">   0.295</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            175.17</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1388800\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-18-56\n",
      "  done: false\n",
      "  episode_len_mean: 176.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.39600000746548175\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 2931\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.120469709946996\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.009821917459111873\n",
      "          policy_loss: -0.05350568562356611\n",
      "          total_loss: -0.038263551120291506\n",
      "          vf_explained_var: -0.29294705390930176\n",
      "          vf_loss: 0.014485644671021007\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.9694840702272596\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.014399127520201116\n",
      "          policy_loss: -0.08182460516320048\n",
      "          total_loss: -0.06626983725462716\n",
      "          vf_explained_var: 0.22802114486694336\n",
      "          vf_loss: 0.005637233581399501\n",
      "    num_agent_steps_sampled: 1388800\n",
      "    num_agent_steps_trained: 1388800\n",
      "    num_steps_sampled: 694400\n",
      "    num_steps_trained: 694400\n",
      "  iterations_since_restore: 248\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.9159999999999995\n",
      "    gpu_util_percent0: 0.062\n",
      "    ram_util_percent: 55.33599999999999\n",
      "    vram_util_percent0: 0.21758203125\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: 0.02000000074505806\n",
      "    agent_1: 0.3760000067204237\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05165854196070738\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.894688188636386\n",
      "    mean_inference_ms: 1.7749056134256376\n",
      "    mean_raw_obs_processing_ms: 0.16124878332938106\n",
      "  time_since_restore: 4610.79762172699\n",
      "  time_this_iter_s: 18.704282760620117\n",
      "  time_total_s: 4610.79762172699\n",
      "  timers:\n",
      "    learn_throughput: 660.817\n",
      "    learn_time_ms: 4237.176\n",
      "    load_throughput: 89050.5\n",
      "    load_time_ms: 31.443\n",
      "    sample_throughput: 192.799\n",
      "    sample_time_ms: 14522.896\n",
      "    update_time_ms: 3.248\n",
      "  timestamp: 1658499536\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 694400\n",
      "  training_iteration: 248\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   248</td><td style=\"text-align: right;\">          4610.8</td><td style=\"text-align: right;\">694400</td><td style=\"text-align: right;\">   0.396</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            176.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1394400\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-19-15\n",
      "  done: false\n",
      "  episode_len_mean: 177.69\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.40200000777840617\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 2943\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.207377820852257\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011954260416150558\n",
      "          policy_loss: -0.06935532747853099\n",
      "          total_loss: -0.05187414462523607\n",
      "          vf_explained_var: -0.19174149632453918\n",
      "          vf_loss: 0.014132549199053929\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.0957665525022007\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.012838808705585356\n",
      "          policy_loss: -0.07127468217905039\n",
      "          total_loss: -0.05633515172216687\n",
      "          vf_explained_var: -0.07143045961856842\n",
      "          vf_loss: 0.008485052430409095\n",
      "    num_agent_steps_sampled: 1394400\n",
      "    num_agent_steps_trained: 1394400\n",
      "    num_steps_sampled: 697200\n",
      "    num_steps_trained: 697200\n",
      "  iterations_since_restore: 249\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.883333333333333\n",
      "    gpu_util_percent0: 0.04958333333333333\n",
      "    ram_util_percent: 55.07916666666667\n",
      "    vram_util_percent0: 0.21666666666666665\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: 0.027000000625848772\n",
      "    agent_1: 0.3750000071525574\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05165997965256337\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.894783517852953\n",
      "    mean_inference_ms: 1.7749715616703725\n",
      "    mean_raw_obs_processing_ms: 0.16132926021132815\n",
      "  time_since_restore: 4629.445432424545\n",
      "  time_this_iter_s: 18.647810697555542\n",
      "  time_total_s: 4629.445432424545\n",
      "  timers:\n",
      "    learn_throughput: 662.309\n",
      "    learn_time_ms: 4227.632\n",
      "    load_throughput: 89051.243\n",
      "    load_time_ms: 31.443\n",
      "    sample_throughput: 193.338\n",
      "    sample_time_ms: 14482.395\n",
      "    update_time_ms: 3.268\n",
      "  timestamp: 1658499555\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 697200\n",
      "  training_iteration: 249\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   249</td><td style=\"text-align: right;\">         4629.45</td><td style=\"text-align: right;\">697200</td><td style=\"text-align: right;\">   0.402</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            177.69</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1400000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-19-33\n",
      "  done: false\n",
      "  episode_len_mean: 185.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.37700000770390035\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 2955\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.0622715829383758\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.014160016408965607\n",
      "          policy_loss: -0.07380892302015647\n",
      "          total_loss: -0.05606920982280003\n",
      "          vf_explained_var: -0.3539069592952728\n",
      "          vf_loss: 0.007614496678971115\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.0258205277579173\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01323895306901319\n",
      "          policy_loss: -0.06519493950916166\n",
      "          total_loss: -0.050389532500135135\n",
      "          vf_explained_var: 0.0057224007323384285\n",
      "          vf_loss: 0.006876852837974313\n",
      "    num_agent_steps_sampled: 1400000\n",
      "    num_agent_steps_trained: 1400000\n",
      "    num_steps_sampled: 700000\n",
      "    num_steps_trained: 700000\n",
      "  iterations_since_restore: 250\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.864000000000001\n",
      "    gpu_util_percent0: 0.064\n",
      "    ram_util_percent: 55.0\n",
      "    vram_util_percent0: 0.21758203125\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: 0.01700000062584877\n",
      "    agent_1: 0.3600000070780516\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05166103850885646\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.894852408308253\n",
      "    mean_inference_ms: 1.7750181265471574\n",
      "    mean_raw_obs_processing_ms: 0.1614029425716168\n",
      "  time_since_restore: 4647.940121650696\n",
      "  time_this_iter_s: 18.494689226150513\n",
      "  time_total_s: 4647.940121650696\n",
      "  timers:\n",
      "    learn_throughput: 662.54\n",
      "    learn_time_ms: 4226.157\n",
      "    load_throughput: 89038.55\n",
      "    load_time_ms: 31.447\n",
      "    sample_throughput: 193.557\n",
      "    sample_time_ms: 14466.043\n",
      "    update_time_ms: 3.266\n",
      "  timestamp: 1658499573\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 700000\n",
      "  training_iteration: 250\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   250</td><td style=\"text-align: right;\">         4647.94</td><td style=\"text-align: right;\">700000</td><td style=\"text-align: right;\">   0.377</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             185.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1405600\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-19-52\n",
      "  done: false\n",
      "  episode_len_mean: 189.14\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.3520000073313713\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 2965\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.094019481823558\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.014148709536650986\n",
      "          policy_loss: -0.10076356412970372\n",
      "          total_loss: -0.08460619819597273\n",
      "          vf_explained_var: 0.36731481552124023\n",
      "          vf_loss: 0.003171425221851331\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.0243369294773963\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.015753868925843954\n",
      "          policy_loss: -0.10504424223334008\n",
      "          total_loss: -0.08925969387257196\n",
      "          vf_explained_var: 0.2784593999385834\n",
      "          vf_loss: 0.0024909664598020582\n",
      "    num_agent_steps_sampled: 1405600\n",
      "    num_agent_steps_trained: 1405600\n",
      "    num_steps_sampled: 702800\n",
      "    num_steps_trained: 702800\n",
      "  iterations_since_restore: 251\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.891666666666666\n",
      "    gpu_util_percent0: 0.049166666666666664\n",
      "    ram_util_percent: 55.02916666666666\n",
      "    vram_util_percent0: 0.21744791666666666\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.0029999993741512297\n",
      "    agent_1: 0.35500000670552256\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051661628513684554\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.89490463145605\n",
      "    mean_inference_ms: 1.7750448327420463\n",
      "    mean_raw_obs_processing_ms: 0.1614545428924303\n",
      "  time_since_restore: 4666.6222195625305\n",
      "  time_this_iter_s: 18.682097911834717\n",
      "  time_total_s: 4666.6222195625305\n",
      "  timers:\n",
      "    learn_throughput: 661.889\n",
      "    learn_time_ms: 4230.315\n",
      "    load_throughput: 87196.819\n",
      "    load_time_ms: 32.111\n",
      "    sample_throughput: 193.75\n",
      "    sample_time_ms: 14451.605\n",
      "    update_time_ms: 3.255\n",
      "  timestamp: 1658499592\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 702800\n",
      "  training_iteration: 251\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   251</td><td style=\"text-align: right;\">         4666.62</td><td style=\"text-align: right;\">702800</td><td style=\"text-align: right;\">   0.352</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            189.14</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1411200\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-20-11\n",
      "  done: false\n",
      "  episode_len_mean: 201.65\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.37400000646710396\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 2975\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.121466378016131\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.009332236076096208\n",
      "          policy_loss: -0.07108641367437399\n",
      "          total_loss: -0.058210630870321654\n",
      "          vf_explained_var: -0.07499543577432632\n",
      "          vf_loss: 0.009304301542475255\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.107142090442635\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010112353286588518\n",
      "          policy_loss: -0.07860445324920529\n",
      "          total_loss: -0.06712885721035058\n",
      "          vf_explained_var: 0.1275554895401001\n",
      "          vf_loss: 0.006393306216306796\n",
      "    num_agent_steps_sampled: 1411200\n",
      "    num_agent_steps_trained: 1411200\n",
      "    num_steps_sampled: 705600\n",
      "    num_steps_trained: 705600\n",
      "  iterations_since_restore: 252\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.94\n",
      "    gpu_util_percent0: 0.057999999999999996\n",
      "    ram_util_percent: 55.14\n",
      "    vram_util_percent0: 0.21750390625000002\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: 0.005000000149011612\n",
      "    agent_1: 0.3690000063180923\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05166184169765382\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.89495827828102\n",
      "    mean_inference_ms: 1.7750614590453861\n",
      "    mean_raw_obs_processing_ms: 0.1614891493483919\n",
      "  time_since_restore: 4685.297646045685\n",
      "  time_this_iter_s: 18.675426483154297\n",
      "  time_total_s: 4685.297646045685\n",
      "  timers:\n",
      "    learn_throughput: 661.999\n",
      "    learn_time_ms: 4229.613\n",
      "    load_throughput: 87348.253\n",
      "    load_time_ms: 32.056\n",
      "    sample_throughput: 193.951\n",
      "    sample_time_ms: 14436.654\n",
      "    update_time_ms: 3.26\n",
      "  timestamp: 1658499611\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 705600\n",
      "  training_iteration: 252\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   252</td><td style=\"text-align: right;\">          4685.3</td><td style=\"text-align: right;\">705600</td><td style=\"text-align: right;\">   0.374</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            201.65</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1416800\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-20-29\n",
      "  done: false\n",
      "  episode_len_mean: 212.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.3590000062435865\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 2988\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.1415329671331813\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.013016733690835981\n",
      "          policy_loss: -0.09386509419779204\n",
      "          total_loss: -0.07950469752644754\n",
      "          vf_explained_var: -0.09974360466003418\n",
      "          vf_loss: 0.0017334414642391494\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.0414522226367677\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.013724786975376227\n",
      "          policy_loss: -0.0673999080227943\n",
      "          total_loss: -0.053039881843211505\n",
      "          vf_explained_var: 0.2001527100801468\n",
      "          vf_loss: 0.004237101014128018\n",
      "    num_agent_steps_sampled: 1416800\n",
      "    num_agent_steps_trained: 1416800\n",
      "    num_steps_sampled: 708400\n",
      "    num_steps_trained: 708400\n",
      "  iterations_since_restore: 253\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.229166666666667\n",
      "    gpu_util_percent0: 0.057499999999999996\n",
      "    ram_util_percent: 55.09166666666667\n",
      "    vram_util_percent0: 0.21803385416666665\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.005\n",
      "    agent_1: 0.3640000062435865\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05166146657097612\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.894991873078062\n",
      "    mean_inference_ms: 1.775073097458187\n",
      "    mean_raw_obs_processing_ms: 0.16152550200050972\n",
      "  time_since_restore: 4703.714275121689\n",
      "  time_this_iter_s: 18.41662907600403\n",
      "  time_total_s: 4703.714275121689\n",
      "  timers:\n",
      "    learn_throughput: 660.088\n",
      "    learn_time_ms: 4241.86\n",
      "    load_throughput: 93955.942\n",
      "    load_time_ms: 29.801\n",
      "    sample_throughput: 194.684\n",
      "    sample_time_ms: 14382.303\n",
      "    update_time_ms: 3.265\n",
      "  timestamp: 1658499629\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 708400\n",
      "  training_iteration: 253\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   253</td><td style=\"text-align: right;\">         4703.71</td><td style=\"text-align: right;\">708400</td><td style=\"text-align: right;\">   0.359</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             212.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1422400\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-20-48\n",
      "  done: false\n",
      "  episode_len_mean: 214.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.3330000061541796\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 3005\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.152968618131819\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010951343282677032\n",
      "          policy_loss: -0.09143646315039534\n",
      "          total_loss: -0.07886153088405817\n",
      "          vf_explained_var: 0.03336339816451073\n",
      "          vf_loss: 0.003283102216214285\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.053900684983957\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011233421922744125\n",
      "          policy_loss: -0.08361482701383509\n",
      "          total_loss: -0.07191609153371592\n",
      "          vf_explained_var: 0.3464082181453705\n",
      "          vf_loss: 0.0037668026133836803\n",
      "    num_agent_steps_sampled: 1422400\n",
      "    num_agent_steps_trained: 1422400\n",
      "    num_steps_sampled: 711200\n",
      "    num_steps_trained: 711200\n",
      "  iterations_since_restore: 254\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.854166666666667\n",
      "    gpu_util_percent0: 0.050416666666666665\n",
      "    ram_util_percent: 55.02916666666666\n",
      "    vram_util_percent0: 0.21743977864583333\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.013999999985098838\n",
      "    agent_1: 0.3470000061392784\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05165978040795851\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8950116761947946\n",
      "    mean_inference_ms: 1.7750473081791427\n",
      "    mean_raw_obs_processing_ms: 0.16157290506858574\n",
      "  time_since_restore: 4722.239296913147\n",
      "  time_this_iter_s: 18.52502179145813\n",
      "  time_total_s: 4722.239296913147\n",
      "  timers:\n",
      "    learn_throughput: 659.648\n",
      "    learn_time_ms: 4244.686\n",
      "    load_throughput: 94120.094\n",
      "    load_time_ms: 29.749\n",
      "    sample_throughput: 195.329\n",
      "    sample_time_ms: 14334.815\n",
      "    update_time_ms: 3.242\n",
      "  timestamp: 1658499648\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 711200\n",
      "  training_iteration: 254\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   254</td><td style=\"text-align: right;\">         4722.24</td><td style=\"text-align: right;\">711200</td><td style=\"text-align: right;\">   0.333</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            214.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1428000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-21-06\n",
      "  done: false\n",
      "  episode_len_mean: 211.19\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.33900000624358656\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 3020\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.1744722667194547\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.009314305976158349\n",
      "          policy_loss: -0.05838609102509162\n",
      "          total_loss: -0.047330542045669756\n",
      "          vf_explained_var: -0.04868106544017792\n",
      "          vf_loss: 0.004228651945249155\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.064558850512618\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01008486804015007\n",
      "          policy_loss: -0.06710206989962553\n",
      "          total_loss: -0.05620461722996663\n",
      "          vf_explained_var: 0.38201212882995605\n",
      "          vf_loss: 0.004770799438293103\n",
      "    num_agent_steps_sampled: 1428000\n",
      "    num_agent_steps_trained: 1428000\n",
      "    num_steps_sampled: 714000\n",
      "    num_steps_trained: 714000\n",
      "  iterations_since_restore: 255\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.883999999999999\n",
      "    gpu_util_percent0: 0.0596\n",
      "    ram_util_percent: 55.0\n",
      "    vram_util_percent0: 0.21816796875000002\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.01099999986588955\n",
      "    agent_1: 0.35000000610947607\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05165732396992931\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.895017996979422\n",
      "    mean_inference_ms: 1.7749758139767136\n",
      "    mean_raw_obs_processing_ms: 0.16160835223088554\n",
      "  time_since_restore: 4740.841595411301\n",
      "  time_this_iter_s: 18.602298498153687\n",
      "  time_total_s: 4740.841595411301\n",
      "  timers:\n",
      "    learn_throughput: 658.59\n",
      "    learn_time_ms: 4251.509\n",
      "    load_throughput: 94109.309\n",
      "    load_time_ms: 29.753\n",
      "    sample_throughput: 195.695\n",
      "    sample_time_ms: 14307.968\n",
      "    update_time_ms: 3.226\n",
      "  timestamp: 1658499666\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 714000\n",
      "  training_iteration: 255\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   255</td><td style=\"text-align: right;\">         4740.84</td><td style=\"text-align: right;\">714000</td><td style=\"text-align: right;\">   0.339</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            211.19</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1433600\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-21-25\n",
      "  done: false\n",
      "  episode_len_mean: 223.26\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.31000000678002837\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 3032\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.0868611786337126\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.009931614361602519\n",
      "          policy_loss: -0.05232501521478688\n",
      "          total_loss: -0.03829011766668243\n",
      "          vf_explained_var: -0.22119058668613434\n",
      "          vf_loss: 0.010647195081609035\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.065326745311419\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011553234994619148\n",
      "          policy_loss: -0.05443535268495041\n",
      "          total_loss: -0.04210712118584968\n",
      "          vf_explained_var: 0.048932842910289764\n",
      "          vf_loss: 0.004664413155773045\n",
      "    num_agent_steps_sampled: 1433600\n",
      "    num_agent_steps_trained: 1433600\n",
      "    num_steps_sampled: 716800\n",
      "    num_steps_trained: 716800\n",
      "  iterations_since_restore: 256\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.866666666666667\n",
      "    gpu_util_percent0: 0.04833333333333333\n",
      "    ram_util_percent: 55.0\n",
      "    vram_util_percent0: 0.21803385416666665\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.03599999986588955\n",
      "    agent_1: 0.3460000066459179\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051655220599065806\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.895002968966622\n",
      "    mean_inference_ms: 1.7749040189998118\n",
      "    mean_raw_obs_processing_ms: 0.16162851973240716\n",
      "  time_since_restore: 4759.340717077255\n",
      "  time_this_iter_s: 18.49912166595459\n",
      "  time_total_s: 4759.340717077255\n",
      "  timers:\n",
      "    learn_throughput: 657.742\n",
      "    learn_time_ms: 4256.986\n",
      "    load_throughput: 93759.864\n",
      "    load_time_ms: 29.864\n",
      "    sample_throughput: 196.328\n",
      "    sample_time_ms: 14261.838\n",
      "    update_time_ms: 3.221\n",
      "  timestamp: 1658499685\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 716800\n",
      "  training_iteration: 256\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   256</td><td style=\"text-align: right;\">         4759.34</td><td style=\"text-align: right;\">716800</td><td style=\"text-align: right;\">    0.31</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            223.26</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1439200\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-21-44\n",
      "  done: false\n",
      "  episode_len_mean: 200.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.4510000069439411\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3052\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.138529654060091\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.008449934073139178\n",
      "          policy_loss: -0.07049238467767455\n",
      "          total_loss: -0.05528142442123377\n",
      "          vf_explained_var: -0.5714693665504456\n",
      "          vf_loss: 0.01882422421996515\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.9339827859685534\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010817905997446232\n",
      "          policy_loss: -0.08336739856618113\n",
      "          total_loss: -0.07095525401295315\n",
      "          vf_explained_var: 0.56476229429245\n",
      "          vf_loss: 0.006847142907818558\n",
      "    num_agent_steps_sampled: 1439200\n",
      "    num_agent_steps_trained: 1439200\n",
      "    num_steps_sampled: 719600\n",
      "    num_steps_trained: 719600\n",
      "  iterations_since_restore: 257\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.843999999999999\n",
      "    gpu_util_percent0: 0.052000000000000005\n",
      "    ram_util_percent: 55.0\n",
      "    vram_util_percent0: 0.21738671875\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: 0.03400000013411045\n",
      "    agent_1: 0.4170000068098307\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05165169168719295\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8949938071886896\n",
      "    mean_inference_ms: 1.7748083373798047\n",
      "    mean_raw_obs_processing_ms: 0.16170166766429397\n",
      "  time_since_restore: 4778.284120559692\n",
      "  time_this_iter_s: 18.943403482437134\n",
      "  time_total_s: 4778.284120559692\n",
      "  timers:\n",
      "    learn_throughput: 654.757\n",
      "    learn_time_ms: 4276.394\n",
      "    load_throughput: 93917.923\n",
      "    load_time_ms: 29.813\n",
      "    sample_throughput: 196.351\n",
      "    sample_time_ms: 14260.204\n",
      "    update_time_ms: 3.231\n",
      "  timestamp: 1658499704\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 719600\n",
      "  training_iteration: 257\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   257</td><td style=\"text-align: right;\">         4778.28</td><td style=\"text-align: right;\">719600</td><td style=\"text-align: right;\">   0.451</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            200.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1444800\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-22-03\n",
      "  done: false\n",
      "  episode_len_mean: 179.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.4350000078231096\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 3073\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.1564742567993345\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010573193304069563\n",
      "          policy_loss: -0.06053198887647817\n",
      "          total_loss: -0.04505381299356616\n",
      "          vf_explained_var: -0.09158143401145935\n",
      "          vf_loss: 0.012789489340710654\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.9552383816667966\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011415237894935874\n",
      "          policy_loss: -0.0734710923071751\n",
      "          total_loss: -0.06028638583943359\n",
      "          vf_explained_var: 0.5786651372909546\n",
      "          vf_loss: 0.007372599622093341\n",
      "    num_agent_steps_sampled: 1444800\n",
      "    num_agent_steps_trained: 1444800\n",
      "    num_steps_sampled: 722400\n",
      "    num_steps_trained: 722400\n",
      "  iterations_since_restore: 258\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.004000000000001\n",
      "    gpu_util_percent0: 0.0572\n",
      "    ram_util_percent: 55.007999999999996\n",
      "    vram_util_percent0: 0.21822265625\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: 0.03000000014901161\n",
      "    agent_1: 0.405000007674098\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05164800858922915\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8949486452970543\n",
      "    mean_inference_ms: 1.7747080690442665\n",
      "    mean_raw_obs_processing_ms: 0.16183607025595861\n",
      "  time_since_restore: 4797.2032289505005\n",
      "  time_this_iter_s: 18.919108390808105\n",
      "  time_total_s: 4797.2032289505005\n",
      "  timers:\n",
      "    learn_throughput: 651.933\n",
      "    learn_time_ms: 4294.923\n",
      "    load_throughput: 94293.602\n",
      "    load_time_ms: 29.694\n",
      "    sample_throughput: 196.31\n",
      "    sample_time_ms: 14263.132\n",
      "    update_time_ms: 3.225\n",
      "  timestamp: 1658499723\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 722400\n",
      "  training_iteration: 258\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   258</td><td style=\"text-align: right;\">          4797.2</td><td style=\"text-align: right;\">722400</td><td style=\"text-align: right;\">   0.435</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            179.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1450400\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-22-21\n",
      "  done: false\n",
      "  episode_len_mean: 172.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.44700000807642937\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 3087\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.0903173575089093\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.012428647945644472\n",
      "          policy_loss: -0.08729848444220677\n",
      "          total_loss: -0.07211405299659375\n",
      "          vf_explained_var: -0.10939348489046097\n",
      "          vf_loss: 0.005913610703005577\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.976565393663588\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.012925234918764305\n",
      "          policy_loss: -0.08389334964894737\n",
      "          total_loss: -0.07032483678111541\n",
      "          vf_explained_var: 0.09748104959726334\n",
      "          vf_loss: 0.00418302191818449\n",
      "    num_agent_steps_sampled: 1450400\n",
      "    num_agent_steps_trained: 1450400\n",
      "    num_steps_sampled: 725200\n",
      "    num_steps_trained: 725200\n",
      "  iterations_since_restore: 259\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.8625\n",
      "    gpu_util_percent0: 0.048749999999999995\n",
      "    ram_util_percent: 55.1\n",
      "    vram_util_percent0: 0.21706949869791667\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: 0.0330000002682209\n",
      "    agent_1: 0.41400000780820845\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051645711360458854\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.894911195411952\n",
      "    mean_inference_ms: 1.7746257421895049\n",
      "    mean_raw_obs_processing_ms: 0.16193378789771476\n",
      "  time_since_restore: 4815.780642271042\n",
      "  time_this_iter_s: 18.577413320541382\n",
      "  time_total_s: 4815.780642271042\n",
      "  timers:\n",
      "    learn_throughput: 651.2\n",
      "    learn_time_ms: 4299.757\n",
      "    load_throughput: 93905.532\n",
      "    load_time_ms: 29.817\n",
      "    sample_throughput: 196.478\n",
      "    sample_time_ms: 14250.929\n",
      "    update_time_ms: 3.199\n",
      "  timestamp: 1658499741\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 725200\n",
      "  training_iteration: 259\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   259</td><td style=\"text-align: right;\">         4815.78</td><td style=\"text-align: right;\">725200</td><td style=\"text-align: right;\">   0.447</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             172.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1456000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-22-40\n",
      "  done: false\n",
      "  episode_len_mean: 170.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.44700000807642937\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 3100\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.097764323155085\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.012116746608440072\n",
      "          policy_loss: -0.0846514795264392\n",
      "          total_loss: -0.07033880604606002\n",
      "          vf_explained_var: -0.19813893735408783\n",
      "          vf_loss: 0.004435548206293745\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.0088911407760213\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.013648555941515669\n",
      "          policy_loss: -0.09426638208554193\n",
      "          total_loss: -0.08000811260093801\n",
      "          vf_explained_var: 0.19858694076538086\n",
      "          vf_loss: 0.0041255865454288485\n",
      "    num_agent_steps_sampled: 1456000\n",
      "    num_agent_steps_trained: 1456000\n",
      "    num_steps_sampled: 728000\n",
      "    num_steps_trained: 728000\n",
      "  iterations_since_restore: 260\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.892\n",
      "    gpu_util_percent0: 0.0576\n",
      "    ram_util_percent: 55.1\n",
      "    vram_util_percent0: 0.21758203125\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: 0.036000000312924385\n",
      "    agent_1: 0.411000007763505\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05164387865654147\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8948877147358782\n",
      "    mean_inference_ms: 1.774557408242151\n",
      "    mean_raw_obs_processing_ms: 0.16201241311695153\n",
      "  time_since_restore: 4834.3638207912445\n",
      "  time_this_iter_s: 18.583178520202637\n",
      "  time_total_s: 4834.3638207912445\n",
      "  timers:\n",
      "    learn_throughput: 649.555\n",
      "    learn_time_ms: 4310.644\n",
      "    load_throughput: 93836.353\n",
      "    load_time_ms: 29.839\n",
      "    sample_throughput: 196.505\n",
      "    sample_time_ms: 14248.984\n",
      "    update_time_ms: 3.201\n",
      "  timestamp: 1658499760\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 728000\n",
      "  training_iteration: 260\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   260</td><td style=\"text-align: right;\">         4834.36</td><td style=\"text-align: right;\">728000</td><td style=\"text-align: right;\">   0.447</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            170.39</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1461600\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-22-59\n",
      "  done: false\n",
      "  episode_len_mean: 178.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.48000000789761543\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 3114\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.13179483796869\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.013838930671968957\n",
      "          policy_loss: -0.10742090877188491\n",
      "          total_loss: -0.09211057908950829\n",
      "          vf_explained_var: -0.09684813767671585\n",
      "          vf_loss: 0.0017931242683040356\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.020045257395222\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.015592714134664544\n",
      "          policy_loss: -0.09698695946956702\n",
      "          total_loss: -0.08124638619448801\n",
      "          vf_explained_var: 0.4042901396751404\n",
      "          vf_loss: 0.002820370541268771\n",
      "    num_agent_steps_sampled: 1461600\n",
      "    num_agent_steps_trained: 1461600\n",
      "    num_steps_sampled: 730800\n",
      "    num_steps_trained: 730800\n",
      "  iterations_since_restore: 261\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.7250000000000005\n",
      "    gpu_util_percent0: 0.05791666666666667\n",
      "    ram_util_percent: 55.12083333333334\n",
      "    vram_util_percent0: 0.22047932942708334\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: 0.0530000001937151\n",
      "    agent_1: 0.42700000770390034\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05164206675418812\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.894843519097941\n",
      "    mean_inference_ms: 1.7744951642624658\n",
      "    mean_raw_obs_processing_ms: 0.16209613125081235\n",
      "  time_since_restore: 4853.134097099304\n",
      "  time_this_iter_s: 18.770276308059692\n",
      "  time_total_s: 4853.134097099304\n",
      "  timers:\n",
      "    learn_throughput: 645.251\n",
      "    learn_time_ms: 4339.396\n",
      "    load_throughput: 96042.683\n",
      "    load_time_ms: 29.154\n",
      "    sample_throughput: 196.773\n",
      "    sample_time_ms: 14229.571\n",
      "    update_time_ms: 3.23\n",
      "  timestamp: 1658499779\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 730800\n",
      "  training_iteration: 261\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   261</td><td style=\"text-align: right;\">         4853.13</td><td style=\"text-align: right;\">730800</td><td style=\"text-align: right;\">    0.48</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">               178</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1467200\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-23-18\n",
      "  done: false\n",
      "  episode_len_mean: 188.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.4730000077933073\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 3122\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.262517374895868\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.013981840329638753\n",
      "          policy_loss: -0.11340948483987068\n",
      "          total_loss: -0.09826586319208477\n",
      "          vf_explained_var: -0.36519867181777954\n",
      "          vf_loss: 0.0010134722061117092\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.191828023110117\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01425434915942793\n",
      "          policy_loss: -0.10407199350101153\n",
      "          total_loss: -0.0903180545653976\n",
      "          vf_explained_var: -0.05218309536576271\n",
      "          vf_loss: 0.0011737301364129443\n",
      "    num_agent_steps_sampled: 1467200\n",
      "    num_agent_steps_trained: 1467200\n",
      "    num_steps_sampled: 733600\n",
      "    num_steps_trained: 733600\n",
      "  iterations_since_restore: 262\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.743999999999999\n",
      "    gpu_util_percent0: 0.0708\n",
      "    ram_util_percent: 55.828\n",
      "    vram_util_percent0: 0.25339453125\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: 0.0530000001937151\n",
      "    agent_1: 0.4200000075995922\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05164130390942731\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8948378121108944\n",
      "    mean_inference_ms: 1.7744761197412962\n",
      "    mean_raw_obs_processing_ms: 0.1621352934097314\n",
      "  time_since_restore: 4872.023023605347\n",
      "  time_this_iter_s: 18.88892650604248\n",
      "  time_total_s: 4872.023023605347\n",
      "  timers:\n",
      "    learn_throughput: 644.644\n",
      "    learn_time_ms: 4343.484\n",
      "    load_throughput: 95682.424\n",
      "    load_time_ms: 29.263\n",
      "    sample_throughput: 196.535\n",
      "    sample_time_ms: 14246.844\n",
      "    update_time_ms: 3.216\n",
      "  timestamp: 1658499798\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 733600\n",
      "  training_iteration: 262\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   262</td><td style=\"text-align: right;\">         4872.02</td><td style=\"text-align: right;\">733600</td><td style=\"text-align: right;\">   0.473</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            188.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1472800\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-23-37\n",
      "  done: false\n",
      "  episode_len_mean: 188.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.383000006750226\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 3138\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.068733297288418\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.008384783524635236\n",
      "          policy_loss: -0.1253006946762547\n",
      "          total_loss: -0.11634856449763036\n",
      "          vf_explained_var: -0.1730664074420929\n",
      "          vf_loss: 0.0010837162087790645\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.9486358634063177\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.009654221732309655\n",
      "          policy_loss: -0.11838202829295326\n",
      "          total_loss: -0.1088001102574968\n",
      "          vf_explained_var: 0.1427183598279953\n",
      "          vf_loss: 0.002107109642175991\n",
      "    num_agent_steps_sampled: 1472800\n",
      "    num_agent_steps_trained: 1472800\n",
      "    num_steps_sampled: 736400\n",
      "    num_steps_trained: 736400\n",
      "  iterations_since_restore: 263\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.031999999999999\n",
      "    gpu_util_percent0: 0.056799999999999996\n",
      "    ram_util_percent: 55.88\n",
      "    vram_util_percent0: 0.250046875\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: 0.004000000208616256\n",
      "    agent_1: 0.37900000654160976\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05163990063713463\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8948489700354094\n",
      "    mean_inference_ms: 1.7744540145451777\n",
      "    mean_raw_obs_processing_ms: 0.16221715809611806\n",
      "  time_since_restore: 4890.800601005554\n",
      "  time_this_iter_s: 18.77757740020752\n",
      "  time_total_s: 4890.800601005554\n",
      "  timers:\n",
      "    learn_throughput: 645.71\n",
      "    learn_time_ms: 4336.309\n",
      "    load_throughput: 95687.102\n",
      "    load_time_ms: 29.262\n",
      "    sample_throughput: 195.98\n",
      "    sample_time_ms: 14287.144\n",
      "    update_time_ms: 3.199\n",
      "  timestamp: 1658499817\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 736400\n",
      "  training_iteration: 263\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   263</td><td style=\"text-align: right;\">          4890.8</td><td style=\"text-align: right;\">736400</td><td style=\"text-align: right;\">   0.383</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            188.03</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1478400\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-23-55\n",
      "  done: false\n",
      "  episode_len_mean: 196.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.3410000067204237\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 3150\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.1346868159515515\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.0061384007325796675\n",
      "          policy_loss: -0.03934436563230563\n",
      "          total_loss: -0.02950465547369926\n",
      "          vf_explained_var: -0.5801922678947449\n",
      "          vf_loss: 0.010910009883532655\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.014039596986203\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.009135460115331326\n",
      "          policy_loss: -0.05213429153991456\n",
      "          total_loss: -0.04189550528120717\n",
      "          vf_explained_var: -0.10182718187570572\n",
      "          vf_loss: 0.0055407803860621895\n",
      "    num_agent_steps_sampled: 1478400\n",
      "    num_agent_steps_trained: 1478400\n",
      "    num_steps_sampled: 739200\n",
      "    num_steps_trained: 739200\n",
      "  iterations_since_restore: 264\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.920833333333334\n",
      "    gpu_util_percent0: 0.05499999999999999\n",
      "    ram_util_percent: 55.9375\n",
      "    vram_util_percent0: 0.248779296875\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.014999999776482581\n",
      "    agent_1: 0.3560000064969063\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051638679127792804\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.894842761259572\n",
      "    mean_inference_ms: 1.7744244365671695\n",
      "    mean_raw_obs_processing_ms: 0.16226363441466282\n",
      "  time_since_restore: 4909.322833299637\n",
      "  time_this_iter_s: 18.52223229408264\n",
      "  time_total_s: 4909.322833299637\n",
      "  timers:\n",
      "    learn_throughput: 645.297\n",
      "    learn_time_ms: 4339.084\n",
      "    load_throughput: 95781.218\n",
      "    load_time_ms: 29.233\n",
      "    sample_throughput: 196.021\n",
      "    sample_time_ms: 14284.148\n",
      "    update_time_ms: 3.209\n",
      "  timestamp: 1658499835\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 739200\n",
      "  training_iteration: 264\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   264</td><td style=\"text-align: right;\">         4909.32</td><td style=\"text-align: right;\">739200</td><td style=\"text-align: right;\">   0.341</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            196.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1484000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-24-14\n",
      "  done: false\n",
      "  episode_len_mean: 208.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.35300000667572023\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 3162\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.0020370295359973\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01365322674614735\n",
      "          policy_loss: -0.10490470736058503\n",
      "          total_loss: -0.08946956738513802\n",
      "          vf_explained_var: -0.28405076265335083\n",
      "          vf_loss: 0.00259175686265475\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.940081703875746\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.014078336353200882\n",
      "          policy_loss: -0.08987679782099652\n",
      "          total_loss: -0.07554250553437755\n",
      "          vf_explained_var: -0.057282764464616776\n",
      "          vf_loss: 0.0030340241806005083\n",
      "    num_agent_steps_sampled: 1484000\n",
      "    num_agent_steps_trained: 1484000\n",
      "    num_steps_sampled: 742000\n",
      "    num_steps_trained: 742000\n",
      "  iterations_since_restore: 265\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.045833333333333\n",
      "    gpu_util_percent0: 0.05333333333333334\n",
      "    ram_util_percent: 55.833333333333336\n",
      "    vram_util_percent0: 0.24567057291666664\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.00799999974668026\n",
      "    agent_1: 0.3610000064224005\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05163749190327751\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8948441609175948\n",
      "    mean_inference_ms: 1.7743948588534193\n",
      "    mean_raw_obs_processing_ms: 0.16228829256809513\n",
      "  time_since_restore: 4927.890392780304\n",
      "  time_this_iter_s: 18.567559480667114\n",
      "  time_total_s: 4927.890392780304\n",
      "  timers:\n",
      "    learn_throughput: 646.684\n",
      "    learn_time_ms: 4329.78\n",
      "    load_throughput: 95965.223\n",
      "    load_time_ms: 29.177\n",
      "    sample_throughput: 195.943\n",
      "    sample_time_ms: 14289.898\n",
      "    update_time_ms: 3.198\n",
      "  timestamp: 1658499854\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 742000\n",
      "  training_iteration: 265\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   265</td><td style=\"text-align: right;\">         4927.89</td><td style=\"text-align: right;\">742000</td><td style=\"text-align: right;\">   0.353</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             208.7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1489600\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-24-32\n",
      "  done: false\n",
      "  episode_len_mean: 210.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000014901161\n",
      "  episode_reward_mean: 0.29900000646710395\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 3179\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.11814141699246\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.009351483955948843\n",
      "          policy_loss: -0.0686249144128808\n",
      "          total_loss: -0.0508696412806395\n",
      "          vf_explained_var: -0.4905558228492737\n",
      "          vf_loss: 0.0231678594091569\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.987683539589246\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011113743611953698\n",
      "          policy_loss: -0.06444045495819799\n",
      "          total_loss: -0.04991113149069514\n",
      "          vf_explained_var: 0.048523131757974625\n",
      "          vf_loss: 0.012110215965332741\n",
      "    num_agent_steps_sampled: 1489600\n",
      "    num_agent_steps_trained: 1489600\n",
      "    num_steps_sampled: 744800\n",
      "    num_steps_trained: 744800\n",
      "  iterations_since_restore: 266\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.9559999999999995\n",
      "    gpu_util_percent0: 0.0672\n",
      "    ram_util_percent: 55.79999999999999\n",
      "    vram_util_percent0: 0.2451171875\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.000000014901161\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.023999999389052392\n",
      "    agent_1: 0.32300000585615635\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051635701661973725\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8948476171093085\n",
      "    mean_inference_ms: 1.7743463990103334\n",
      "    mean_raw_obs_processing_ms: 0.16233446667945753\n",
      "  time_since_restore: 4946.455693721771\n",
      "  time_this_iter_s: 18.565300941467285\n",
      "  time_total_s: 4946.455693721771\n",
      "  timers:\n",
      "    learn_throughput: 647.135\n",
      "    learn_time_ms: 4326.76\n",
      "    load_throughput: 96171.588\n",
      "    load_time_ms: 29.115\n",
      "    sample_throughput: 195.815\n",
      "    sample_time_ms: 14299.206\n",
      "    update_time_ms: 3.224\n",
      "  timestamp: 1658499872\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 744800\n",
      "  training_iteration: 266\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   266</td><td style=\"text-align: right;\">         4946.46</td><td style=\"text-align: right;\">744800</td><td style=\"text-align: right;\">   0.299</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             210.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1495200\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-24-52\n",
      "  done: false\n",
      "  episode_len_mean: 213.49\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.31500000700354575\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 3192\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.0890657759848095\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011994689960429124\n",
      "          policy_loss: -0.07372696890282289\n",
      "          total_loss: -0.054279971592352795\n",
      "          vf_explained_var: -0.08687607198953629\n",
      "          vf_loss: 0.019473997824076963\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.9692218214983033\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.012503775979344363\n",
      "          policy_loss: -0.06952801053176656\n",
      "          total_loss: -0.05387902033301847\n",
      "          vf_explained_var: 0.2087109237909317\n",
      "          vf_loss: 0.01131648214816648\n",
      "    num_agent_steps_sampled: 1495200\n",
      "    num_agent_steps_trained: 1495200\n",
      "    num_steps_sampled: 747600\n",
      "    num_steps_trained: 747600\n",
      "  iterations_since_restore: 267\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 7.872000000000001\n",
      "    gpu_util_percent0: 0.068\n",
      "    ram_util_percent: 55.98400000000001\n",
      "    vram_util_percent0: 0.24656640625\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.013999999389052392\n",
      "    agent_1: 0.32900000639259813\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05163447647151535\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.894881251992615\n",
      "    mean_inference_ms: 1.7743298039591477\n",
      "    mean_raw_obs_processing_ms: 0.16236881900466427\n",
      "  time_since_restore: 4965.67823433876\n",
      "  time_this_iter_s: 19.222540616989136\n",
      "  time_total_s: 4965.67823433876\n",
      "  timers:\n",
      "    learn_throughput: 642.282\n",
      "    learn_time_ms: 4359.456\n",
      "    load_throughput: 96160.17\n",
      "    load_time_ms: 29.118\n",
      "    sample_throughput: 195.885\n",
      "    sample_time_ms: 14294.108\n",
      "    update_time_ms: 3.253\n",
      "  timestamp: 1658499892\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 747600\n",
      "  training_iteration: 267\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   267</td><td style=\"text-align: right;\">         4965.68</td><td style=\"text-align: right;\">747600</td><td style=\"text-align: right;\">   0.315</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            213.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1500800\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-25-11\n",
      "  done: false\n",
      "  episode_len_mean: 211.32\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.31100000746548173\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 3207\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.078022359737328\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011204366102015986\n",
      "          policy_loss: -0.08545515202716897\n",
      "          total_loss: -0.06735963041865034\n",
      "          vf_explained_var: -0.02957305498421192\n",
      "          vf_loss: 0.0181409908331088\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.994073589288053\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.012711293061304897\n",
      "          policy_loss: -0.08410038903336196\n",
      "          total_loss: -0.06886991888487023\n",
      "          vf_explained_var: 0.2284342497587204\n",
      "          vf_loss: 0.009558870464388747\n",
      "    num_agent_steps_sampled: 1500800\n",
      "    num_agent_steps_trained: 1500800\n",
      "    num_steps_sampled: 750400\n",
      "    num_steps_trained: 750400\n",
      "  iterations_since_restore: 268\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.748\n",
      "    gpu_util_percent0: 0.0672\n",
      "    ram_util_percent: 57.611999999999995\n",
      "    vram_util_percent0: 0.26646484375\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.010999998971819878\n",
      "    agent_1: 0.32200000643730164\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05163405711067538\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.894963242631336\n",
      "    mean_inference_ms: 1.7743735370077771\n",
      "    mean_raw_obs_processing_ms: 0.1624181541309278\n",
      "  time_since_restore: 4984.994999170303\n",
      "  time_this_iter_s: 19.31676483154297\n",
      "  time_total_s: 4984.994999170303\n",
      "  timers:\n",
      "    learn_throughput: 642.647\n",
      "    learn_time_ms: 4356.983\n",
      "    load_throughput: 89017.898\n",
      "    load_time_ms: 31.454\n",
      "    sample_throughput: 195.338\n",
      "    sample_time_ms: 14334.121\n",
      "    update_time_ms: 3.257\n",
      "  timestamp: 1658499911\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 750400\n",
      "  training_iteration: 268\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   268</td><td style=\"text-align: right;\">         4984.99</td><td style=\"text-align: right;\">750400</td><td style=\"text-align: right;\">   0.311</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            211.32</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1506400\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-25-30\n",
      "  done: false\n",
      "  episode_len_mean: 198.14\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.3160000082850456\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 3221\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.0634876067439714\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.009898753629741866\n",
      "          policy_loss: -0.08428102137016442\n",
      "          total_loss: -0.06789849270630623\n",
      "          vf_explained_var: -0.045422203838825226\n",
      "          vf_loss: 0.017426678218942556\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.0160394009380114\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.012492879816955852\n",
      "          policy_loss: -0.08841097732173304\n",
      "          total_loss: -0.0734506960939143\n",
      "          vf_explained_var: 0.2986493706703186\n",
      "          vf_loss: 0.009437150239786465\n",
      "    num_agent_steps_sampled: 1506400\n",
      "    num_agent_steps_trained: 1506400\n",
      "    num_steps_sampled: 753200\n",
      "    num_steps_trained: 753200\n",
      "  iterations_since_restore: 269\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.4479999999999995\n",
      "    gpu_util_percent0: 0.0696\n",
      "    ram_util_percent: 57.124\n",
      "    vram_util_percent0: 0.26166406249999996\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.004999998733401298\n",
      "    agent_1: 0.32100000701844694\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05163342847806628\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.895034173875226\n",
      "    mean_inference_ms: 1.7744057660439223\n",
      "    mean_raw_obs_processing_ms: 0.16247385183433505\n",
      "  time_since_restore: 5003.623615026474\n",
      "  time_this_iter_s: 18.628615856170654\n",
      "  time_total_s: 5003.623615026474\n",
      "  timers:\n",
      "    learn_throughput: 642.123\n",
      "    learn_time_ms: 4360.533\n",
      "    load_throughput: 89290.38\n",
      "    load_time_ms: 31.358\n",
      "    sample_throughput: 195.312\n",
      "    sample_time_ms: 14336.052\n",
      "    update_time_ms: 3.273\n",
      "  timestamp: 1658499930\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 753200\n",
      "  training_iteration: 269\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   269</td><td style=\"text-align: right;\">         5003.62</td><td style=\"text-align: right;\">753200</td><td style=\"text-align: right;\">   0.316</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            198.14</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1512000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-25-48\n",
      "  done: false\n",
      "  episode_len_mean: 205.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.26100000835955145\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 3233\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.1636368334293365\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.013016702666769322\n",
      "          policy_loss: -0.1280857754255911\n",
      "          total_loss: -0.11184118417440914\n",
      "          vf_explained_var: 0.1864452064037323\n",
      "          vf_loss: 0.00713851612090366\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.066074417815322\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011133039275902252\n",
      "          policy_loss: -0.1282167215337419\n",
      "          total_loss: -0.11548983157124548\n",
      "          vf_explained_var: 0.2450007051229477\n",
      "          vf_loss: 0.007002858829028334\n",
      "    num_agent_steps_sampled: 1512000\n",
      "    num_agent_steps_trained: 1512000\n",
      "    num_steps_sampled: 756000\n",
      "    num_steps_trained: 756000\n",
      "  iterations_since_restore: 270\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.366666666666667\n",
      "    gpu_util_percent0: 0.05458333333333334\n",
      "    ram_util_percent: 56.87916666666666\n",
      "    vram_util_percent0: 0.26092529296875\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.034999998733401295\n",
      "    agent_1: 0.2960000070929527\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05163289616202395\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8950777978052464\n",
      "    mean_inference_ms: 1.7744302252999007\n",
      "    mean_raw_obs_processing_ms: 0.1625157996301942\n",
      "  time_since_restore: 5022.402970075607\n",
      "  time_this_iter_s: 18.7793550491333\n",
      "  time_total_s: 5022.402970075607\n",
      "  timers:\n",
      "    learn_throughput: 642.643\n",
      "    learn_time_ms: 4357.006\n",
      "    load_throughput: 89328.685\n",
      "    load_time_ms: 31.345\n",
      "    sample_throughput: 195.001\n",
      "    sample_time_ms: 14358.892\n",
      "    update_time_ms: 3.288\n",
      "  timestamp: 1658499948\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 756000\n",
      "  training_iteration: 270\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   270</td><td style=\"text-align: right;\">          5022.4</td><td style=\"text-align: right;\">756000</td><td style=\"text-align: right;\">   0.261</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            205.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1517600\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-26-08\n",
      "  done: false\n",
      "  episode_len_mean: 198.69\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.19300000846385956\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 3248\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.160141537586848\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010903617934158626\n",
      "          policy_loss: -0.06863921030058659\n",
      "          total_loss: -0.05361580999617997\n",
      "          vf_explained_var: 0.3062303960323334\n",
      "          vf_loss: 0.010434454572359322\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.968746393918991\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010164449532157635\n",
      "          policy_loss: -0.06176324526313692\n",
      "          total_loss: -0.049068569480108896\n",
      "          vf_explained_var: 0.26241743564605713\n",
      "          vf_loss: 0.009560340480155511\n",
      "    num_agent_steps_sampled: 1517600\n",
      "    num_agent_steps_trained: 1517600\n",
      "    num_steps_sampled: 758800\n",
      "    num_steps_trained: 758800\n",
      "  iterations_since_restore: 271\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 7.053846153846153\n",
      "    gpu_util_percent0: 0.07576923076923078\n",
      "    ram_util_percent: 57.23461538461538\n",
      "    vram_util_percent0: 0.2592022235576923\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.07299999862909318\n",
      "    agent_1: 0.26600000709295274\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051633408282335244\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8952133153345425\n",
      "    mean_inference_ms: 1.7745287937225738\n",
      "    mean_raw_obs_processing_ms: 0.16258157254329206\n",
      "  time_since_restore: 5042.086665153503\n",
      "  time_this_iter_s: 19.683695077896118\n",
      "  time_total_s: 5042.086665153503\n",
      "  timers:\n",
      "    learn_throughput: 642.527\n",
      "    learn_time_ms: 4357.795\n",
      "    load_throughput: 89375.525\n",
      "    load_time_ms: 31.328\n",
      "    sample_throughput: 193.777\n",
      "    sample_time_ms: 14449.576\n",
      "    update_time_ms: 3.269\n",
      "  timestamp: 1658499968\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 758800\n",
      "  training_iteration: 271\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   271</td><td style=\"text-align: right;\">         5042.09</td><td style=\"text-align: right;\">758800</td><td style=\"text-align: right;\">   0.193</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            198.69</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1523200\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-26-28\n",
      "  done: false\n",
      "  episode_len_mean: 200.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.17400000862777232\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 3260\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.1111785137937185\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.012403320777302376\n",
      "          policy_loss: -0.08915702703979193\n",
      "          total_loss: -0.07405371469752979\n",
      "          vf_explained_var: 0.03980279713869095\n",
      "          vf_loss: 0.005788139642910599\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.031200647354126\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.012904440228727563\n",
      "          policy_loss: -0.09852478710291464\n",
      "          total_loss: -0.08394548135610032\n",
      "          vf_explained_var: 0.13159310817718506\n",
      "          vf_loss: 0.007192725721411296\n",
      "    num_agent_steps_sampled: 1523200\n",
      "    num_agent_steps_trained: 1523200\n",
      "    num_steps_sampled: 761600\n",
      "    num_steps_trained: 761600\n",
      "  iterations_since_restore: 272\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 7.107692307692307\n",
      "    gpu_util_percent0: 0.08230769230769229\n",
      "    ram_util_percent: 57.45\n",
      "    vram_util_percent0: 0.27620567908653837\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.07999999843537807\n",
      "    agent_1: 0.2540000070631504\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05163505918911138\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.895378657720189\n",
      "    mean_inference_ms: 1.774674025317987\n",
      "    mean_raw_obs_processing_ms: 0.16263712771509262\n",
      "  time_since_restore: 5061.752432346344\n",
      "  time_this_iter_s: 19.665767192840576\n",
      "  time_total_s: 5061.752432346344\n",
      "  timers:\n",
      "    learn_throughput: 639.801\n",
      "    learn_time_ms: 4376.361\n",
      "    load_throughput: 88786.593\n",
      "    load_time_ms: 31.536\n",
      "    sample_throughput: 192.992\n",
      "    sample_time_ms: 14508.389\n",
      "    update_time_ms: 3.28\n",
      "  timestamp: 1658499988\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 761600\n",
      "  training_iteration: 272\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   272</td><td style=\"text-align: right;\">         5061.75</td><td style=\"text-align: right;\">761600</td><td style=\"text-align: right;\">   0.174</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            200.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1528800\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-26-47\n",
      "  done: false\n",
      "  episode_len_mean: 201.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.2540000084787607\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 3275\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.097041217344148\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.009218864071643634\n",
      "          policy_loss: -0.0778541194269478\n",
      "          total_loss: -0.0636557743148712\n",
      "          vf_explained_var: 0.05163329094648361\n",
      "          vf_loss: 0.013414867085625051\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.9210033505445434\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.013126747708662586\n",
      "          policy_loss: -0.09057527360917968\n",
      "          total_loss: -0.0761346147091466\n",
      "          vf_explained_var: 0.21542413532733917\n",
      "          vf_loss: 0.006031480333365921\n",
      "    num_agent_steps_sampled: 1528800\n",
      "    num_agent_steps_trained: 1528800\n",
      "    num_steps_sampled: 764400\n",
      "    num_steps_trained: 764400\n",
      "  iterations_since_restore: 273\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 7.253846153846154\n",
      "    gpu_util_percent0: 0.07923076923076923\n",
      "    ram_util_percent: 57.51153846153846\n",
      "    vram_util_percent0: 0.29154146634615385\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.04799999892711639\n",
      "    agent_1: 0.3020000074058771\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051638623562016484\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.895659173858699\n",
      "    mean_inference_ms: 1.7749298683005152\n",
      "    mean_raw_obs_processing_ms: 0.16270659369930163\n",
      "  time_since_restore: 5081.267973423004\n",
      "  time_this_iter_s: 19.515541076660156\n",
      "  time_total_s: 5081.267973423004\n",
      "  timers:\n",
      "    learn_throughput: 637.687\n",
      "    learn_time_ms: 4390.867\n",
      "    load_throughput: 88802.571\n",
      "    load_time_ms: 31.531\n",
      "    sample_throughput: 192.166\n",
      "    sample_time_ms: 14570.759\n",
      "    update_time_ms: 3.275\n",
      "  timestamp: 1658500007\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 764400\n",
      "  training_iteration: 273\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   273</td><td style=\"text-align: right;\">         5081.27</td><td style=\"text-align: right;\">764400</td><td style=\"text-align: right;\">   0.254</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            201.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1534400\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-27-06\n",
      "  done: false\n",
      "  episode_len_mean: 199.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.2790000085532665\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 3291\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.060552236224924\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.009993777627263441\n",
      "          policy_loss: -0.05720022345953372\n",
      "          total_loss: -0.03930404432183076\n",
      "          vf_explained_var: 0.2889639437198639\n",
      "          vf_loss: 0.021438992343194383\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.9320012888028508\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011273146015256884\n",
      "          policy_loss: -0.06811698610406547\n",
      "          total_loss: -0.05465613079390356\n",
      "          vf_explained_var: 0.36499112844467163\n",
      "          vf_loss: 0.00853893689712831\n",
      "    num_agent_steps_sampled: 1534400\n",
      "    num_agent_steps_trained: 1534400\n",
      "    num_steps_sampled: 767200\n",
      "    num_steps_trained: 767200\n",
      "  iterations_since_restore: 274\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.143999999999998\n",
      "    gpu_util_percent0: 0.064\n",
      "    ram_util_percent: 57.20799999999999\n",
      "    vram_util_percent0: 0.27550390625000004\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.03599999889731407\n",
      "    agent_1: 0.3150000074505806\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05164242915358866\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8959650401630785\n",
      "    mean_inference_ms: 1.7751961789389044\n",
      "    mean_raw_obs_processing_ms: 0.1627895197265506\n",
      "  time_since_restore: 5100.170380115509\n",
      "  time_this_iter_s: 18.902406692504883\n",
      "  time_total_s: 5100.170380115509\n",
      "  timers:\n",
      "    learn_throughput: 637.233\n",
      "    learn_time_ms: 4393.998\n",
      "    load_throughput: 88698.077\n",
      "    load_time_ms: 31.568\n",
      "    sample_throughput: 191.708\n",
      "    sample_time_ms: 14605.571\n",
      "    update_time_ms: 3.285\n",
      "  timestamp: 1658500026\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 767200\n",
      "  training_iteration: 274\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   274</td><td style=\"text-align: right;\">         5100.17</td><td style=\"text-align: right;\">767200</td><td style=\"text-align: right;\">   0.279</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            199.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1540000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-27-25\n",
      "  done: false\n",
      "  episode_len_mean: 200.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.3270000086724758\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 3304\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.1218633818484487\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011565352685455195\n",
      "          policy_loss: -0.08450494054713567\n",
      "          total_loss: -0.06813247994848227\n",
      "          vf_explained_var: -0.08784927427768707\n",
      "          vf_loss: 0.012114965400195658\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.0487161147452535\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.012614832328451535\n",
      "          policy_loss: -0.06432960861205911\n",
      "          total_loss: -0.050602514689325334\n",
      "          vf_explained_var: 0.09304846823215485\n",
      "          vf_loss: 0.005607465938651114\n",
      "    num_agent_steps_sampled: 1540000\n",
      "    num_agent_steps_trained: 1540000\n",
      "    num_steps_sampled: 770000\n",
      "    num_steps_trained: 770000\n",
      "  iterations_since_restore: 275\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.600000000000001\n",
      "    gpu_util_percent0: 0.086\n",
      "    ram_util_percent: 59.492\n",
      "    vram_util_percent0: 0.26563671875\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.009999998807907105\n",
      "    agent_1: 0.33700000748038295\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05164532208882976\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8961991517593697\n",
      "    mean_inference_ms: 1.7753937011029735\n",
      "    mean_raw_obs_processing_ms: 0.16284905556490967\n",
      "  time_since_restore: 5119.360362291336\n",
      "  time_this_iter_s: 19.189982175827026\n",
      "  time_total_s: 5119.360362291336\n",
      "  timers:\n",
      "    learn_throughput: 633.295\n",
      "    learn_time_ms: 4421.32\n",
      "    load_throughput: 88640.972\n",
      "    load_time_ms: 31.588\n",
      "    sample_throughput: 191.255\n",
      "    sample_time_ms: 14640.176\n",
      "    update_time_ms: 3.323\n",
      "  timestamp: 1658500045\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 770000\n",
      "  training_iteration: 275\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   275</td><td style=\"text-align: right;\">         5119.36</td><td style=\"text-align: right;\">770000</td><td style=\"text-align: right;\">   0.327</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            200.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1545600\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-27-44\n",
      "  done: false\n",
      "  episode_len_mean: 200.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.33700000777840616\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 3317\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.9979051873087883\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.012369305898197154\n",
      "          policy_loss: -0.0856417702768903\n",
      "          total_loss: -0.06806133590466093\n",
      "          vf_explained_var: -0.09521068632602692\n",
      "          vf_loss: 0.012834259005189995\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.9606459332363946\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.0136072598636962\n",
      "          policy_loss: -0.09789664449531951\n",
      "          total_loss: -0.08344054355291605\n",
      "          vf_explained_var: -0.08884367346763611\n",
      "          vf_loss: 0.00475093202372331\n",
      "    num_agent_steps_sampled: 1545600\n",
      "    num_agent_steps_trained: 1545600\n",
      "    num_steps_sampled: 772800\n",
      "    num_steps_trained: 772800\n",
      "  iterations_since_restore: 276\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 7.291666666666667\n",
      "    gpu_util_percent0: 0.05666666666666667\n",
      "    ram_util_percent: 61.083333333333336\n",
      "    vram_util_percent0: 0.2702311197916667\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.008999999463558197\n",
      "    agent_1: 0.34600000724196434\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0516482473403451\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.896429609746789\n",
      "    mean_inference_ms: 1.7755980060117906\n",
      "    mean_raw_obs_processing_ms: 0.1629064193888773\n",
      "  time_since_restore: 5138.089119434357\n",
      "  time_this_iter_s: 18.72875714302063\n",
      "  time_total_s: 5138.089119434357\n",
      "  timers:\n",
      "    learn_throughput: 632.616\n",
      "    learn_time_ms: 4426.063\n",
      "    load_throughput: 88340.325\n",
      "    load_time_ms: 31.696\n",
      "    sample_throughput: 191.102\n",
      "    sample_time_ms: 14651.871\n",
      "    update_time_ms: 3.299\n",
      "  timestamp: 1658500064\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 772800\n",
      "  training_iteration: 276\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   276</td><td style=\"text-align: right;\">         5138.09</td><td style=\"text-align: right;\">772800</td><td style=\"text-align: right;\">   0.337</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            200.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1551200\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-28-03\n",
      "  done: false\n",
      "  episode_len_mean: 192.13\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.4850000083446503\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 3335\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.9767267051197233\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.014051396755343292\n",
      "          policy_loss: -0.08076256956805342\n",
      "          total_loss: -0.05755726950814105\n",
      "          vf_explained_var: 0.11409880220890045\n",
      "          vf_loss: 0.023464205026054213\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.8635687732270785\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010363650063736898\n",
      "          policy_loss: -0.09256261025542958\n",
      "          total_loss: -0.0790790733401428\n",
      "          vf_explained_var: 0.11743713915348053\n",
      "          vf_loss: 0.011118749234280282\n",
      "    num_agent_steps_sampled: 1551200\n",
      "    num_agent_steps_trained: 1551200\n",
      "    num_steps_sampled: 775600\n",
      "    num_steps_trained: 775600\n",
      "  iterations_since_restore: 277\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.664000000000001\n",
      "    gpu_util_percent0: 0.076\n",
      "    ram_util_percent: 60.46\n",
      "    vram_util_percent0: 0.2716953125\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: 0.0810000005364418\n",
      "    agent_1: 0.40400000780820844\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051652159469967526\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.896744488986278\n",
      "    mean_inference_ms: 1.7758690536562471\n",
      "    mean_raw_obs_processing_ms: 0.16300666990253848\n",
      "  time_since_restore: 5156.987365484238\n",
      "  time_this_iter_s: 18.89824604988098\n",
      "  time_total_s: 5156.987365484238\n",
      "  timers:\n",
      "    learn_throughput: 636.114\n",
      "    learn_time_ms: 4401.724\n",
      "    load_throughput: 88172.921\n",
      "    load_time_ms: 31.756\n",
      "    sample_throughput: 191.216\n",
      "    sample_time_ms: 14643.15\n",
      "    update_time_ms: 3.271\n",
      "  timestamp: 1658500083\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 775600\n",
      "  training_iteration: 277\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   277</td><td style=\"text-align: right;\">         5156.99</td><td style=\"text-align: right;\">775600</td><td style=\"text-align: right;\">   0.485</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            192.13</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1556800\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-28-22\n",
      "  done: false\n",
      "  episode_len_mean: 191.66\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.6130000082403422\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 3349\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.023100903346425\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010464068205136921\n",
      "          policy_loss: -0.08057521995579009\n",
      "          total_loss: -0.06489390433610727\n",
      "          vf_explained_var: 0.04821770638227463\n",
      "          vf_loss: 0.013561493422977427\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.8859800252886045\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.014882014555847468\n",
      "          policy_loss: -0.1119592177754038\n",
      "          total_loss: -0.09605049529706039\n",
      "          vf_explained_var: 0.2895279824733734\n",
      "          vf_loss: 0.0051700267425379325\n",
      "    num_agent_steps_sampled: 1556800\n",
      "    num_agent_steps_trained: 1556800\n",
      "    num_steps_sampled: 778400\n",
      "    num_steps_trained: 778400\n",
      "  iterations_since_restore: 278\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.343999999999999\n",
      "    gpu_util_percent0: 0.06280000000000001\n",
      "    ram_util_percent: 59.343999999999994\n",
      "    vram_util_percent0: 0.2774140625\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: 0.14800000041723252\n",
      "    agent_1: 0.46500000782310963\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05165429159369465\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8969437588752203\n",
      "    mean_inference_ms: 1.7760333402689563\n",
      "    mean_raw_obs_processing_ms: 0.16307887705500818\n",
      "  time_since_restore: 5175.8967497348785\n",
      "  time_this_iter_s: 18.90938425064087\n",
      "  time_total_s: 5175.8967497348785\n",
      "  timers:\n",
      "    learn_throughput: 636.047\n",
      "    learn_time_ms: 4402.192\n",
      "    load_throughput: 94988.391\n",
      "    load_time_ms: 29.477\n",
      "    sample_throughput: 191.726\n",
      "    sample_time_ms: 14604.202\n",
      "    update_time_ms: 3.281\n",
      "  timestamp: 1658500102\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 778400\n",
      "  training_iteration: 278\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   278</td><td style=\"text-align: right;\">          5175.9</td><td style=\"text-align: right;\">778400</td><td style=\"text-align: right;\">   0.613</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            191.66</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1562400\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-28-41\n",
      "  done: false\n",
      "  episode_len_mean: 188.26\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.7130000095069409\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 3362\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.090972898971467\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.008783565729172457\n",
      "          policy_loss: -0.07308495195216633\n",
      "          total_loss: -0.055785128385390896\n",
      "          vf_explained_var: -0.09258435666561127\n",
      "          vf_loss: 0.023659317577593833\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.9883594527131034\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01010814754998135\n",
      "          policy_loss: -0.07862953763105907\n",
      "          total_loss: -0.06564351440147762\n",
      "          vf_explained_var: 0.0264142919331789\n",
      "          vf_loss: 0.010576063654558052\n",
      "    num_agent_steps_sampled: 1562400\n",
      "    num_agent_steps_trained: 1562400\n",
      "    num_steps_sampled: 781200\n",
      "    num_steps_trained: 781200\n",
      "  iterations_since_restore: 279\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.228\n",
      "    gpu_util_percent0: 0.0584\n",
      "    ram_util_percent: 59.516000000000005\n",
      "    vram_util_percent0: 0.27053906250000004\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: 0.20900000065565108\n",
      "    agent_1: 0.5040000088512897\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051655117440193334\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8970840783284\n",
      "    mean_inference_ms: 1.7761286983163758\n",
      "    mean_raw_obs_processing_ms: 0.16314481674412473\n",
      "  time_since_restore: 5194.743850231171\n",
      "  time_this_iter_s: 18.847100496292114\n",
      "  time_total_s: 5194.743850231171\n",
      "  timers:\n",
      "    learn_throughput: 637.949\n",
      "    learn_time_ms: 4389.065\n",
      "    load_throughput: 94773.451\n",
      "    load_time_ms: 29.544\n",
      "    sample_throughput: 191.271\n",
      "    sample_time_ms: 14638.896\n",
      "    update_time_ms: 3.256\n",
      "  timestamp: 1658500121\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 781200\n",
      "  training_iteration: 279\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   279</td><td style=\"text-align: right;\">         5194.74</td><td style=\"text-align: right;\">781200</td><td style=\"text-align: right;\">   0.713</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            188.26</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1568000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-29-00\n",
      "  done: false\n",
      "  episode_len_mean: 196.92\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.6600000092387199\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 3373\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.0984816820848557\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01462071106598508\n",
      "          policy_loss: -0.11208066686092588\n",
      "          total_loss: -0.09488411939396645\n",
      "          vf_explained_var: 0.11324720084667206\n",
      "          vf_loss: 0.004627415318931786\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.021617439531145\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.013000771594511993\n",
      "          policy_loss: -0.11301308946609165\n",
      "          total_loss: -0.09948704110878939\n",
      "          vf_explained_var: -0.002164715901017189\n",
      "          vf_loss: 0.003899652225949727\n",
      "    num_agent_steps_sampled: 1568000\n",
      "    num_agent_steps_trained: 1568000\n",
      "    num_steps_sampled: 784000\n",
      "    num_steps_trained: 784000\n",
      "  iterations_since_restore: 280\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.120000000000001\n",
      "    gpu_util_percent0: 0.06520000000000001\n",
      "    ram_util_percent: 59.47599999999999\n",
      "    vram_util_percent0: 0.27484374999999994\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: 0.17900000065565108\n",
      "    agent_1: 0.48100000858306885\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05165505379362019\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8971728933707737\n",
      "    mean_inference_ms: 1.776175720891294\n",
      "    mean_raw_obs_processing_ms: 0.1631891983235369\n",
      "  time_since_restore: 5213.651546001434\n",
      "  time_this_iter_s: 18.907695770263672\n",
      "  time_total_s: 5213.651546001434\n",
      "  timers:\n",
      "    learn_throughput: 636.973\n",
      "    learn_time_ms: 4395.794\n",
      "    load_throughput: 95001.915\n",
      "    load_time_ms: 29.473\n",
      "    sample_throughput: 191.186\n",
      "    sample_time_ms: 14645.392\n",
      "    update_time_ms: 3.225\n",
      "  timestamp: 1658500140\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 784000\n",
      "  training_iteration: 280\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   280</td><td style=\"text-align: right;\">         5213.65</td><td style=\"text-align: right;\">784000</td><td style=\"text-align: right;\">    0.66</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            196.92</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1573600\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-29-19\n",
      "  done: false\n",
      "  episode_len_mean: 199.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.5620000091940165\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 3389\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.112908508451212\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010855219434789901\n",
      "          policy_loss: -0.0767384423760675\n",
      "          total_loss: -0.062411154986919085\n",
      "          vf_explained_var: 0.15690690279006958\n",
      "          vf_loss: 0.008546639852630463\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.9290577545762062\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011684767937069487\n",
      "          policy_loss: -0.07324588498436026\n",
      "          total_loss: -0.06013838997751009\n",
      "          vf_explained_var: 0.2489524632692337\n",
      "          vf_loss: 0.006351675375342546\n",
      "    num_agent_steps_sampled: 1573600\n",
      "    num_agent_steps_trained: 1573600\n",
      "    num_steps_sampled: 786800\n",
      "    num_steps_trained: 786800\n",
      "  iterations_since_restore: 281\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.524\n",
      "    gpu_util_percent0: 0.0616\n",
      "    ram_util_percent: 59.38799999999999\n",
      "    vram_util_percent0: 0.28030078125\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: 0.12400000110268593\n",
      "    agent_1: 0.43800000809133055\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05165576924243966\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8973168927918054\n",
      "    mean_inference_ms: 1.7762935084392037\n",
      "    mean_raw_obs_processing_ms: 0.16325811286926592\n",
      "  time_since_restore: 5233.006506443024\n",
      "  time_this_iter_s: 19.354960441589355\n",
      "  time_total_s: 5233.006506443024\n",
      "  timers:\n",
      "    learn_throughput: 637.956\n",
      "    learn_time_ms: 4389.017\n",
      "    load_throughput: 95001.3\n",
      "    load_time_ms: 29.473\n",
      "    sample_throughput: 191.525\n",
      "    sample_time_ms: 14619.495\n",
      "    update_time_ms: 3.233\n",
      "  timestamp: 1658500159\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 786800\n",
      "  training_iteration: 281\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   281</td><td style=\"text-align: right;\">         5233.01</td><td style=\"text-align: right;\">786800</td><td style=\"text-align: right;\">   0.562</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            199.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1579200\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-29-39\n",
      "  done: false\n",
      "  episode_len_mean: 195.23\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.48500000923871994\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 3405\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.0228545867971013\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010869797568281828\n",
      "          policy_loss: -0.06994142443346348\n",
      "          total_loss: -0.0562235663611708\n",
      "          vf_explained_var: 0.17641811072826385\n",
      "          vf_loss: 0.0066531875321137095\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.8911654903065591\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010875664750528516\n",
      "          policy_loss: -0.06522439247713746\n",
      "          total_loss: -0.052785028123512996\n",
      "          vf_explained_var: 0.19724848866462708\n",
      "          vf_loss: 0.0067091146993334405\n",
      "    num_agent_steps_sampled: 1579200\n",
      "    num_agent_steps_trained: 1579200\n",
      "    num_steps_sampled: 789600\n",
      "    num_steps_trained: 789600\n",
      "  iterations_since_restore: 282\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.324000000000001\n",
      "    gpu_util_percent0: 0.0548\n",
      "    ram_util_percent: 59.452000000000005\n",
      "    vram_util_percent0: 0.26937890625\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: 0.08300000116229057\n",
      "    agent_1: 0.4020000080764294\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051656944437692774\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8974843109832786\n",
      "    mean_inference_ms: 1.7764315377993547\n",
      "    mean_raw_obs_processing_ms: 0.16333659770846812\n",
      "  time_since_restore: 5252.369953393936\n",
      "  time_this_iter_s: 19.363446950912476\n",
      "  time_total_s: 5252.369953393936\n",
      "  timers:\n",
      "    learn_throughput: 639.776\n",
      "    learn_time_ms: 4376.535\n",
      "    load_throughput: 95810.911\n",
      "    load_time_ms: 29.224\n",
      "    sample_throughput: 191.754\n",
      "    sample_time_ms: 14602.026\n",
      "    update_time_ms: 3.223\n",
      "  timestamp: 1658500179\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 789600\n",
      "  training_iteration: 282\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   282</td><td style=\"text-align: right;\">         5252.37</td><td style=\"text-align: right;\">789600</td><td style=\"text-align: right;\">   0.485</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            195.23</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1584800\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-29-58\n",
      "  done: false\n",
      "  episode_len_mean: 193.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.43100000895559787\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 3418\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.046570678551992\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.013600997074892748\n",
      "          policy_loss: -0.07544709151282059\n",
      "          total_loss: -0.059431900391202705\n",
      "          vf_explained_var: 0.10027606040239334\n",
      "          vf_loss: 0.004468210384878884\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.9407469662172454\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01323686368303868\n",
      "          policy_loss: -0.07356993983254749\n",
      "          total_loss: -0.05967545714087291\n",
      "          vf_explained_var: 0.3282468914985657\n",
      "          vf_loss: 0.004181431148113222\n",
      "    num_agent_steps_sampled: 1584800\n",
      "    num_agent_steps_trained: 1584800\n",
      "    num_steps_sampled: 792400\n",
      "    num_steps_trained: 792400\n",
      "  iterations_since_restore: 283\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.453846153846154\n",
      "    gpu_util_percent0: 0.07\n",
      "    ram_util_percent: 59.42307692307692\n",
      "    vram_util_percent0: 0.2744290865384615\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: 0.053000001162290575\n",
      "    agent_1: 0.3780000077933073\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05165864952404731\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8976634442983435\n",
      "    mean_inference_ms: 1.7765608325480189\n",
      "    mean_raw_obs_processing_ms: 0.1633992579624281\n",
      "  time_since_restore: 5271.558945417404\n",
      "  time_this_iter_s: 19.188992023468018\n",
      "  time_total_s: 5271.558945417404\n",
      "  timers:\n",
      "    learn_throughput: 641.036\n",
      "    learn_time_ms: 4367.927\n",
      "    load_throughput: 95667.615\n",
      "    load_time_ms: 29.268\n",
      "    sample_throughput: 192.075\n",
      "    sample_time_ms: 14577.664\n",
      "    update_time_ms: 3.253\n",
      "  timestamp: 1658500198\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 792400\n",
      "  training_iteration: 283\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   283</td><td style=\"text-align: right;\">         5271.56</td><td style=\"text-align: right;\">792400</td><td style=\"text-align: right;\">   0.431</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            193.04</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1590400\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-30-17\n",
      "  done: false\n",
      "  episode_len_mean: 202.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.4100000087171793\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 3431\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.027829988016969\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.009473739305873146\n",
      "          policy_loss: -0.00511844104974963\n",
      "          total_loss: 0.006999155788521637\n",
      "          vf_explained_var: -0.09121601283550262\n",
      "          vf_loss: 0.006574322811675735\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.9355421147885776\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010534594815403272\n",
      "          policy_loss: -0.03830829815082585\n",
      "          total_loss: -0.0268384261067868\n",
      "          vf_explained_var: 0.17253269255161285\n",
      "          vf_loss: 0.004967871871832315\n",
      "    num_agent_steps_sampled: 1590400\n",
      "    num_agent_steps_trained: 1590400\n",
      "    num_steps_sampled: 795200\n",
      "    num_steps_trained: 795200\n",
      "  iterations_since_restore: 284\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.864\n",
      "    gpu_util_percent0: 0.0608\n",
      "    ram_util_percent: 59.488\n",
      "    vram_util_percent0: 0.27333203125\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: 0.03300000116229057\n",
      "    agent_1: 0.3770000075548887\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051661147226745044\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.897869496432362\n",
      "    mean_inference_ms: 1.7767212395677132\n",
      "    mean_raw_obs_processing_ms: 0.16344996991482638\n",
      "  time_since_restore: 5290.704231739044\n",
      "  time_this_iter_s: 19.145286321640015\n",
      "  time_total_s: 5290.704231739044\n",
      "  timers:\n",
      "    learn_throughput: 639.876\n",
      "    learn_time_ms: 4375.85\n",
      "    load_throughput: 95651.798\n",
      "    load_time_ms: 29.273\n",
      "    sample_throughput: 191.86\n",
      "    sample_time_ms: 14593.997\n",
      "    update_time_ms: 3.245\n",
      "  timestamp: 1658500217\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 795200\n",
      "  training_iteration: 284\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   284</td><td style=\"text-align: right;\">          5290.7</td><td style=\"text-align: right;\">795200</td><td style=\"text-align: right;\">    0.41</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            202.37</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1596000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-30-36\n",
      "  done: false\n",
      "  episode_len_mean: 217.13\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.36600000865757465\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 3441\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.090355376402537\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.013355958193284088\n",
      "          policy_loss: -0.08760652951117923\n",
      "          total_loss: -0.0730514391521172\n",
      "          vf_explained_var: -0.1710057407617569\n",
      "          vf_loss: 0.001139042476530622\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.002297434423651\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01667412190390825\n",
      "          policy_loss: -0.10768270489920508\n",
      "          total_loss: -0.09145675839750939\n",
      "          vf_explained_var: 0.011016136035323143\n",
      "          vf_loss: 0.0010978395362577604\n",
      "    num_agent_steps_sampled: 1596000\n",
      "    num_agent_steps_trained: 1596000\n",
      "    num_steps_sampled: 798000\n",
      "    num_steps_trained: 798000\n",
      "  iterations_since_restore: 285\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.751999999999999\n",
      "    gpu_util_percent0: 0.0644\n",
      "    ram_util_percent: 59.372\n",
      "    vram_util_percent0: 0.27462109375\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: 0.0030000011622905733\n",
      "    agent_1: 0.3630000074952841\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05166343568552466\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8980433688985436\n",
      "    mean_inference_ms: 1.7768602825453255\n",
      "    mean_raw_obs_processing_ms: 0.16348296546528282\n",
      "  time_since_restore: 5309.794897317886\n",
      "  time_this_iter_s: 19.090665578842163\n",
      "  time_total_s: 5309.794897317886\n",
      "  timers:\n",
      "    learn_throughput: 639.431\n",
      "    learn_time_ms: 4378.891\n",
      "    load_throughput: 95429.118\n",
      "    load_time_ms: 29.341\n",
      "    sample_throughput: 192.025\n",
      "    sample_time_ms: 14581.413\n",
      "    update_time_ms: 3.221\n",
      "  timestamp: 1658500236\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 798000\n",
      "  training_iteration: 285\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   285</td><td style=\"text-align: right;\">         5309.79</td><td style=\"text-align: right;\">798000</td><td style=\"text-align: right;\">   0.366</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            217.13</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1601600\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-30-56\n",
      "  done: false\n",
      "  episode_len_mean: 219.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.3330000080913305\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 3452\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.081846809458165\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011395016619237416\n",
      "          policy_loss: -0.09467565699898321\n",
      "          total_loss: -0.08215432745471064\n",
      "          vf_explained_var: -0.02546689286828041\n",
      "          vf_loss: 0.0016207896577768786\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.0178305400269374\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011086102938510376\n",
      "          policy_loss: -0.07754131954806369\n",
      "          total_loss: -0.06615898248163007\n",
      "          vf_explained_var: -0.07999961078166962\n",
      "          vf_loss: 0.003241319658584308\n",
      "    num_agent_steps_sampled: 1601600\n",
      "    num_agent_steps_trained: 1601600\n",
      "    num_steps_sampled: 800800\n",
      "    num_steps_trained: 800800\n",
      "  iterations_since_restore: 286\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.896153846153846\n",
      "    gpu_util_percent0: 0.07615384615384616\n",
      "    ram_util_percent: 59.49615384615385\n",
      "    vram_util_percent0: 0.29319786658653846\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.013999999314546586\n",
      "    agent_1: 0.3470000074058771\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05166652778494913\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.898260746996276\n",
      "    mean_inference_ms: 1.777035834211209\n",
      "    mean_raw_obs_processing_ms: 0.16351569986198097\n",
      "  time_since_restore: 5329.442942619324\n",
      "  time_this_iter_s: 19.648045301437378\n",
      "  time_total_s: 5329.442942619324\n",
      "  timers:\n",
      "    learn_throughput: 633.327\n",
      "    learn_time_ms: 4421.096\n",
      "    load_throughput: 96035.693\n",
      "    load_time_ms: 29.156\n",
      "    sample_throughput: 191.373\n",
      "    sample_time_ms: 14631.146\n",
      "    update_time_ms: 3.268\n",
      "  timestamp: 1658500256\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 800800\n",
      "  training_iteration: 286\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   286</td><td style=\"text-align: right;\">         5329.44</td><td style=\"text-align: right;\">800800</td><td style=\"text-align: right;\">   0.333</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            219.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1607200\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-31-16\n",
      "  done: false\n",
      "  episode_len_mean: 218.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000037252903\n",
      "  episode_reward_mean: 0.23200000703334808\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 3464\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.176631269355615\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010954478309290347\n",
      "          policy_loss: -0.0512436089550084\n",
      "          total_loss: -0.039418230735875456\n",
      "          vf_explained_var: -0.16562192142009735\n",
      "          vf_loss: 0.0011614077478346492\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.017404565144153\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.012806824474609726\n",
      "          policy_loss: -0.075259022700179\n",
      "          total_loss: -0.06260297889370815\n",
      "          vf_explained_var: -0.025624772533774376\n",
      "          vf_loss: 0.0019647203815631137\n",
      "    num_agent_steps_sampled: 1607200\n",
      "    num_agent_steps_trained: 1607200\n",
      "    num_steps_sampled: 803600\n",
      "    num_steps_trained: 803600\n",
      "  iterations_since_restore: 287\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 10.884615384615383\n",
      "    gpu_util_percent0: 0.10576923076923077\n",
      "    ram_util_percent: 61.08076923076923\n",
      "    vram_util_percent0: 0.30787635216346154\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.000000037252903\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.07199999928474426\n",
      "    agent_1: 0.3040000063180923\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051671698131689155\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.898557576006159\n",
      "    mean_inference_ms: 1.7773201119445128\n",
      "    mean_raw_obs_processing_ms: 0.1635556915421504\n",
      "  time_since_restore: 5349.704308271408\n",
      "  time_this_iter_s: 20.26136565208435\n",
      "  time_total_s: 5349.704308271408\n",
      "  timers:\n",
      "    learn_throughput: 631.319\n",
      "    learn_time_ms: 4435.16\n",
      "    load_throughput: 93382.923\n",
      "    load_time_ms: 29.984\n",
      "    sample_throughput: 189.786\n",
      "    sample_time_ms: 14753.485\n",
      "    update_time_ms: 3.284\n",
      "  timestamp: 1658500276\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 803600\n",
      "  training_iteration: 287\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   287</td><td style=\"text-align: right;\">          5349.7</td><td style=\"text-align: right;\">803600</td><td style=\"text-align: right;\">   0.232</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            218.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1612800\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-31-36\n",
      "  done: false\n",
      "  episode_len_mean: 219.58\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000037252903\n",
      "  episode_reward_mean: 0.25700000710785387\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 3478\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.0942714601045562\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.008891332564983248\n",
      "          policy_loss: -0.05666054910328794\n",
      "          total_loss: -0.04646115745535575\n",
      "          vf_explained_var: -0.36885276436805725\n",
      "          vf_loss: 0.003047771654781259\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.9834181233530952\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010879379546668252\n",
      "          policy_loss: -0.06378100826051585\n",
      "          total_loss: -0.052648976224147534\n",
      "          vf_explained_var: -0.043625425547361374\n",
      "          vf_loss: 0.003076043706338775\n",
      "    num_agent_steps_sampled: 1612800\n",
      "    num_agent_steps_trained: 1612800\n",
      "    num_steps_sampled: 806400\n",
      "    num_steps_trained: 806400\n",
      "  iterations_since_restore: 288\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.776923076923078\n",
      "    gpu_util_percent0: 0.07115384615384615\n",
      "    ram_util_percent: 62.369230769230775\n",
      "    vram_util_percent0: 0.280517578125\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.000000037252903\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.06199999928474426\n",
      "    agent_1: 0.3190000063925982\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05167870804058941\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8989450564982566\n",
      "    mean_inference_ms: 1.7776872028872683\n",
      "    mean_raw_obs_processing_ms: 0.16360974973729\n",
      "  time_since_restore: 5369.305108308792\n",
      "  time_this_iter_s: 19.600800037384033\n",
      "  time_total_s: 5369.305108308792\n",
      "  timers:\n",
      "    learn_throughput: 630.872\n",
      "    learn_time_ms: 4438.299\n",
      "    load_throughput: 93196.105\n",
      "    load_time_ms: 30.044\n",
      "    sample_throughput: 188.946\n",
      "    sample_time_ms: 14819.082\n",
      "    update_time_ms: 3.263\n",
      "  timestamp: 1658500296\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 806400\n",
      "  training_iteration: 288\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   288</td><td style=\"text-align: right;\">         5369.31</td><td style=\"text-align: right;\">806400</td><td style=\"text-align: right;\">   0.257</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            219.58</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1618400\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-31-55\n",
      "  done: false\n",
      "  episode_len_mean: 214.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000037252903\n",
      "  episode_reward_mean: 0.3290000069886446\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 3494\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.0857670505841575\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.007986156756202109\n",
      "          policy_loss: -0.04963410806366093\n",
      "          total_loss: -0.03754813773160831\n",
      "          vf_explained_var: -0.33512434363365173\n",
      "          vf_loss: 0.011330194541750686\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.9293592806373323\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011154373630232691\n",
      "          policy_loss: -0.06896854004090917\n",
      "          total_loss: -0.05705979631845063\n",
      "          vf_explained_var: 0.1948881596326828\n",
      "          vf_loss: 0.00444409235743002\n",
      "    num_agent_steps_sampled: 1618400\n",
      "    num_agent_steps_trained: 1618400\n",
      "    num_steps_sampled: 809200\n",
      "    num_steps_trained: 809200\n",
      "  iterations_since_restore: 289\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 7.940000000000001\n",
      "    gpu_util_percent0: 0.0704\n",
      "    ram_util_percent: 63.644000000000005\n",
      "    vram_util_percent0: 0.31801171875\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.000000037252903\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.024999999701976777\n",
      "    agent_1: 0.35400000669062137\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05168724548818893\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.8994261030603536\n",
      "    mean_inference_ms: 1.7781369534776448\n",
      "    mean_raw_obs_processing_ms: 0.16367394973797755\n",
      "  time_since_restore: 5388.756952524185\n",
      "  time_this_iter_s: 19.451844215393066\n",
      "  time_total_s: 5388.756952524185\n",
      "  timers:\n",
      "    learn_throughput: 631.778\n",
      "    learn_time_ms: 4431.934\n",
      "    load_throughput: 93448.015\n",
      "    load_time_ms: 29.963\n",
      "    sample_throughput: 188.093\n",
      "    sample_time_ms: 14886.251\n",
      "    update_time_ms: 3.26\n",
      "  timestamp: 1658500315\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 809200\n",
      "  training_iteration: 289\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   289</td><td style=\"text-align: right;\">         5388.76</td><td style=\"text-align: right;\">809200</td><td style=\"text-align: right;\">   0.329</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            214.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1624000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-32-15\n",
      "  done: false\n",
      "  episode_len_mean: 219.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000037252903\n",
      "  episode_reward_mean: 0.37300000682473183\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 3507\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.051802914057459\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011557488507306\n",
      "          policy_loss: -0.07299069035988873\n",
      "          total_loss: -0.05947401846493622\n",
      "          vf_explained_var: -0.3754434585571289\n",
      "          vf_loss: 0.0039047224078482636\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.9803174542529243\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011068076842384015\n",
      "          policy_loss: -0.06839042430848556\n",
      "          total_loss: -0.05686445406865373\n",
      "          vf_explained_var: -0.09733255952596664\n",
      "          vf_loss: 0.0036582575059063466\n",
      "    num_agent_steps_sampled: 1624000\n",
      "    num_agent_steps_trained: 1624000\n",
      "    num_steps_sampled: 812000\n",
      "    num_steps_trained: 812000\n",
      "  iterations_since_restore: 290\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.184615384615386\n",
      "    gpu_util_percent0: 0.07576923076923078\n",
      "    ram_util_percent: 63.56538461538461\n",
      "    vram_util_percent0: 0.31490384615384615\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.000000037252903\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.0029999998211860655\n",
      "    agent_1: 0.3760000066459179\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051694332354921976\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.899848575036674\n",
      "    mean_inference_ms: 1.7785211363624958\n",
      "    mean_raw_obs_processing_ms: 0.16372170504951783\n",
      "  time_since_restore: 5408.535592317581\n",
      "  time_this_iter_s: 19.778639793395996\n",
      "  time_total_s: 5408.535592317581\n",
      "  timers:\n",
      "    learn_throughput: 629.07\n",
      "    learn_time_ms: 4451.015\n",
      "    load_throughput: 93209.271\n",
      "    load_time_ms: 30.04\n",
      "    sample_throughput: 187.238\n",
      "    sample_time_ms: 14954.235\n",
      "    update_time_ms: 3.26\n",
      "  timestamp: 1658500335\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 812000\n",
      "  training_iteration: 290\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   290</td><td style=\"text-align: right;\">         5408.54</td><td style=\"text-align: right;\">812000</td><td style=\"text-align: right;\">   0.373</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            219.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1629600\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-32-35\n",
      "  done: false\n",
      "  episode_len_mean: 226.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000037252903\n",
      "  episode_reward_mean: 0.3480000074207783\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 3516\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.1597076274809384\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.008707238018163052\n",
      "          policy_loss: -0.05701122347575923\n",
      "          total_loss: -0.04633254924279061\n",
      "          vf_explained_var: -0.49109095335006714\n",
      "          vf_loss: 0.005084870216489348\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.0242816880345345\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011026927745521962\n",
      "          policy_loss: -0.04803705208801797\n",
      "          total_loss: -0.03560235679643007\n",
      "          vf_explained_var: -0.2916320860385895\n",
      "          vf_loss: 0.006422026103488558\n",
      "    num_agent_steps_sampled: 1629600\n",
      "    num_agent_steps_trained: 1629600\n",
      "    num_steps_sampled: 814800\n",
      "    num_steps_trained: 814800\n",
      "  iterations_since_restore: 291\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.734615384615385\n",
      "    gpu_util_percent0: 0.0996153846153846\n",
      "    ram_util_percent: 64.22692307692309\n",
      "    vram_util_percent0: 0.3072490985576923\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.000000037252903\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.015999999344348908\n",
      "    agent_1: 0.36400000676512717\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05169917371829024\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.9001353731293853\n",
      "    mean_inference_ms: 1.7787919471570734\n",
      "    mean_raw_obs_processing_ms: 0.16374783469271262\n",
      "  time_since_restore: 5427.951642990112\n",
      "  time_this_iter_s: 19.416050672531128\n",
      "  time_total_s: 5427.951642990112\n",
      "  timers:\n",
      "    learn_throughput: 627.079\n",
      "    learn_time_ms: 4465.148\n",
      "    load_throughput: 93004.581\n",
      "    load_time_ms: 30.106\n",
      "    sample_throughput: 187.342\n",
      "    sample_time_ms: 14945.961\n",
      "    update_time_ms: 3.295\n",
      "  timestamp: 1658500355\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 814800\n",
      "  training_iteration: 291\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   291</td><td style=\"text-align: right;\">         5427.95</td><td style=\"text-align: right;\">814800</td><td style=\"text-align: right;\">   0.348</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            226.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1635200\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-32-54\n",
      "  done: false\n",
      "  episode_len_mean: 237.09\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000037252903\n",
      "  episode_reward_mean: 0.3470000074058771\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 3525\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.087698466011456\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.018497294550173442\n",
      "          policy_loss: -0.10491651857453343\n",
      "          total_loss: -0.08464601404003111\n",
      "          vf_explained_var: -0.17814357578754425\n",
      "          vf_loss: 0.0009400950957136783\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.0887184494308064\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01615771303738876\n",
      "          policy_loss: -0.1026585115427484\n",
      "          total_loss: -0.08682047597124308\n",
      "          vf_explained_var: 0.01470758207142353\n",
      "          vf_loss: 0.0015672576375350001\n",
      "    num_agent_steps_sampled: 1635200\n",
      "    num_agent_steps_trained: 1635200\n",
      "    num_steps_sampled: 817600\n",
      "    num_steps_trained: 817600\n",
      "  iterations_since_restore: 292\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.21923076923077\n",
      "    gpu_util_percent0: 0.09153846153846154\n",
      "    ram_util_percent: 64.5923076923077\n",
      "    vram_util_percent0: 0.3500713641826923\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.000000037252903\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.019999999403953552\n",
      "    agent_1: 0.3670000068098307\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05170423142875487\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.900434586315798\n",
      "    mean_inference_ms: 1.7790777099836022\n",
      "    mean_raw_obs_processing_ms: 0.1637679998026154\n",
      "  time_since_restore: 5447.556537628174\n",
      "  time_this_iter_s: 19.604894638061523\n",
      "  time_total_s: 5447.556537628174\n",
      "  timers:\n",
      "    learn_throughput: 625.105\n",
      "    learn_time_ms: 4479.244\n",
      "    load_throughput: 92642.153\n",
      "    load_time_ms: 30.224\n",
      "    sample_throughput: 187.224\n",
      "    sample_time_ms: 14955.32\n",
      "    update_time_ms: 3.512\n",
      "  timestamp: 1658500374\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 817600\n",
      "  training_iteration: 292\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   292</td><td style=\"text-align: right;\">         5447.56</td><td style=\"text-align: right;\">817600</td><td style=\"text-align: right;\">   0.347</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            237.09</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1640800\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-33-13\n",
      "  done: false\n",
      "  episode_len_mean: 227.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000037252903\n",
      "  episode_reward_mean: 0.3380000076442957\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 3540\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.028937889706521\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.006446209487584163\n",
      "          policy_loss: -0.04765952363340191\n",
      "          total_loss: -0.03567684376598447\n",
      "          vf_explained_var: -0.4559370279312134\n",
      "          vf_loss: 0.015913412688730335\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.8715804794004984\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.009696648497121495\n",
      "          policy_loss: -0.058944893009105555\n",
      "          total_loss: -0.04680404704759851\n",
      "          vf_explained_var: 0.1543930172920227\n",
      "          vf_loss: 0.009199392597760639\n",
      "    num_agent_steps_sampled: 1640800\n",
      "    num_agent_steps_trained: 1640800\n",
      "    num_steps_sampled: 820400\n",
      "    num_steps_trained: 820400\n",
      "  iterations_since_restore: 293\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 7.9159999999999995\n",
      "    gpu_util_percent0: 0.0756\n",
      "    ram_util_percent: 64.888\n",
      "    vram_util_percent0: 0.31901171875\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.000000037252903\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.019999999403953552\n",
      "    agent_1: 0.35800000704824925\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05171245800406208\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.900938205723942\n",
      "    mean_inference_ms: 1.7795553915081002\n",
      "    mean_raw_obs_processing_ms: 0.16381472100414862\n",
      "  time_since_restore: 5466.799808502197\n",
      "  time_this_iter_s: 19.243270874023438\n",
      "  time_total_s: 5466.799808502197\n",
      "  timers:\n",
      "    learn_throughput: 623.492\n",
      "    learn_time_ms: 4490.835\n",
      "    load_throughput: 65103.49\n",
      "    load_time_ms: 43.008\n",
      "    sample_throughput: 187.469\n",
      "    sample_time_ms: 14935.811\n",
      "    update_time_ms: 3.499\n",
      "  timestamp: 1658500393\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 820400\n",
      "  training_iteration: 293\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   293</td><td style=\"text-align: right;\">          5466.8</td><td style=\"text-align: right;\">820400</td><td style=\"text-align: right;\">   0.338</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            227.97</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1646400\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-33-33\n",
      "  done: false\n",
      "  episode_len_mean: 221.38\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000037252903\n",
      "  episode_reward_mean: 0.4120000080764294\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 3554\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.061924376658031\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.008140839271565064\n",
      "          policy_loss: -0.07402102322140265\n",
      "          total_loss: -0.05785807733800972\n",
      "          vf_explained_var: -0.27273431420326233\n",
      "          vf_loss: 0.022443541426279506\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.9262691984574\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011683661226405443\n",
      "          policy_loss: -0.09071662446513074\n",
      "          total_loss: -0.07671126345017651\n",
      "          vf_explained_var: 0.019765129312872887\n",
      "          vf_loss: 0.008914640628583064\n",
      "    num_agent_steps_sampled: 1646400\n",
      "    num_agent_steps_trained: 1646400\n",
      "    num_steps_sampled: 823200\n",
      "    num_steps_trained: 823200\n",
      "  iterations_since_restore: 294\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.116\n",
      "    gpu_util_percent0: 0.0692\n",
      "    ram_util_percent: 64.82400000000001\n",
      "    vram_util_percent0: 0.3730624999999999\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.000000037252903\n",
      "  policy_reward_mean:\n",
      "    agent_0: 0.018000000566244127\n",
      "    agent_1: 0.39400000751018527\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051719382270560194\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.901371337696452\n",
      "    mean_inference_ms: 1.779974618793501\n",
      "    mean_raw_obs_processing_ms: 0.16386798051545653\n",
      "  time_since_restore: 5485.840872049332\n",
      "  time_this_iter_s: 19.0410635471344\n",
      "  time_total_s: 5485.840872049332\n",
      "  timers:\n",
      "    learn_throughput: 624.521\n",
      "    learn_time_ms: 4483.436\n",
      "    load_throughput: 64963.437\n",
      "    load_time_ms: 43.101\n",
      "    sample_throughput: 187.507\n",
      "    sample_time_ms: 14932.741\n",
      "    update_time_ms: 3.5\n",
      "  timestamp: 1658500413\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 823200\n",
      "  training_iteration: 294\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   294</td><td style=\"text-align: right;\">         5485.84</td><td style=\"text-align: right;\">823200</td><td style=\"text-align: right;\">   0.412</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            221.38</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1652000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-33-52\n",
      "  done: false\n",
      "  episode_len_mean: 213.44\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000037252903\n",
      "  episode_reward_mean: 0.34100000858306884\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 3570\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.044822257899103\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.013666180076855636\n",
      "          policy_loss: -0.08162807699443961\n",
      "          total_loss: -0.06379097242270723\n",
      "          vf_explained_var: 0.07472143322229385\n",
      "          vf_loss: 0.009457757547698705\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.9295518497625987\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011673755811992466\n",
      "          policy_loss: -0.06591889396077022\n",
      "          total_loss: -0.0515035240553386\n",
      "          vf_explained_var: 0.3176533281803131\n",
      "          vf_loss: 0.010117249291180911\n",
      "    num_agent_steps_sampled: 1652000\n",
      "    num_agent_steps_trained: 1652000\n",
      "    num_steps_sampled: 826000\n",
      "    num_steps_trained: 826000\n",
      "  iterations_since_restore: 295\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 9.946153846153846\n",
      "    gpu_util_percent0: 0.1\n",
      "    ram_util_percent: 66.07692307692308\n",
      "    vram_util_percent0: 0.31379206730769227\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.000000037252903\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.014999998956918717\n",
      "    agent_1: 0.35600000753998756\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05172662623163083\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.9018307489442163\n",
      "    mean_inference_ms: 1.7804181341218384\n",
      "    mean_raw_obs_processing_ms: 0.16393803072488072\n",
      "  time_since_restore: 5505.787610530853\n",
      "  time_this_iter_s: 19.946738481521606\n",
      "  time_total_s: 5505.787610530853\n",
      "  timers:\n",
      "    learn_throughput: 626.353\n",
      "    learn_time_ms: 4470.325\n",
      "    load_throughput: 64630.381\n",
      "    load_time_ms: 43.323\n",
      "    sample_throughput: 186.286\n",
      "    sample_time_ms: 15030.619\n",
      "    update_time_ms: 3.519\n",
      "  timestamp: 1658500432\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 826000\n",
      "  training_iteration: 295\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   295</td><td style=\"text-align: right;\">         5505.79</td><td style=\"text-align: right;\">826000</td><td style=\"text-align: right;\">   0.341</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            213.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1657600\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-34-12\n",
      "  done: false\n",
      "  episode_len_mean: 214.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000037252903\n",
      "  episode_reward_mean: 0.36100000858306885\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 3582\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.991261597545374\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.016133164282308682\n",
      "          policy_loss: -0.09836002055938782\n",
      "          total_loss: -0.07951260317280512\n",
      "          vf_explained_var: 0.20780667662620544\n",
      "          vf_loss: 0.004355559528838577\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.8974303894099736\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.015310829159864914\n",
      "          policy_loss: -0.08314113481381875\n",
      "          total_loss: -0.0677196499101618\n",
      "          vf_explained_var: 0.2451157122850418\n",
      "          vf_loss: 0.002568595776946971\n",
      "    num_agent_steps_sampled: 1657600\n",
      "    num_agent_steps_trained: 1657600\n",
      "    num_steps_sampled: 828800\n",
      "    num_steps_trained: 828800\n",
      "  iterations_since_restore: 296\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 9.261538461538462\n",
      "    gpu_util_percent0: 0.065\n",
      "    ram_util_percent: 63.95384615384616\n",
      "    vram_util_percent0: 0.2928072415865385\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.000000037252903\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.0059999989718198775\n",
      "    agent_1: 0.3670000075548887\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05173175415200526\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.9021615705524413\n",
      "    mean_inference_ms: 1.780744569536755\n",
      "    mean_raw_obs_processing_ms: 0.16398392392821218\n",
      "  time_since_restore: 5525.327396154404\n",
      "  time_this_iter_s: 19.539785623550415\n",
      "  time_total_s: 5525.327396154404\n",
      "  timers:\n",
      "    learn_throughput: 629.561\n",
      "    learn_time_ms: 4447.541\n",
      "    load_throughput: 64577.322\n",
      "    load_time_ms: 43.359\n",
      "    sample_throughput: 186.152\n",
      "    sample_time_ms: 15041.492\n",
      "    update_time_ms: 3.47\n",
      "  timestamp: 1658500452\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 828800\n",
      "  training_iteration: 296\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   296</td><td style=\"text-align: right;\">         5525.33</td><td style=\"text-align: right;\">828800</td><td style=\"text-align: right;\">   0.361</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            214.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1663200\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-34-32\n",
      "  done: false\n",
      "  episode_len_mean: 212.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000037252903\n",
      "  episode_reward_mean: 0.26500000827014447\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 3598\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.003232995669047\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011619944704948707\n",
      "          policy_loss: -0.06742889992338383\n",
      "          total_loss: -0.052276820863447766\n",
      "          vf_explained_var: 0.1926078051328659\n",
      "          vf_loss: 0.008315018788423566\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.852266787063508\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011115402383932774\n",
      "          policy_loss: -0.0682989016765662\n",
      "          total_loss: -0.05484727196865909\n",
      "          vf_explained_var: 0.2796841263771057\n",
      "          vf_loss: 0.008868248198095466\n",
      "    num_agent_steps_sampled: 1663200\n",
      "    num_agent_steps_trained: 1663200\n",
      "    num_steps_sampled: 831600\n",
      "    num_steps_trained: 831600\n",
      "  iterations_since_restore: 297\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 10.115384615384615\n",
      "    gpu_util_percent0: 0.09346153846153847\n",
      "    ram_util_percent: 65.36153846153846\n",
      "    vram_util_percent0: 0.3598783052884616\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.000000037252903\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.060999999046325686\n",
      "    agent_1: 0.32600000731647016\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05173897100265187\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.9026188026019066\n",
      "    mean_inference_ms: 1.7811978499098517\n",
      "    mean_raw_obs_processing_ms: 0.1640503746581749\n",
      "  time_since_restore: 5545.3671000003815\n",
      "  time_this_iter_s: 20.039703845977783\n",
      "  time_total_s: 5545.3671000003815\n",
      "  timers:\n",
      "    learn_throughput: 630.348\n",
      "    learn_time_ms: 4441.994\n",
      "    load_throughput: 65388.948\n",
      "    load_time_ms: 42.821\n",
      "    sample_throughput: 186.358\n",
      "    sample_time_ms: 15024.864\n",
      "    update_time_ms: 3.446\n",
      "  timestamp: 1658500472\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 831600\n",
      "  training_iteration: 297\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   297</td><td style=\"text-align: right;\">         5545.37</td><td style=\"text-align: right;\">831600</td><td style=\"text-align: right;\">   0.265</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            212.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1668800\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-34-51\n",
      "  done: false\n",
      "  episode_len_mean: 202.61\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000037252903\n",
      "  episode_reward_mean: 0.2830000077933073\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 3615\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.030396824081739\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.008430675945303466\n",
      "          policy_loss: -0.06746934215525038\n",
      "          total_loss: -0.052879997343595844\n",
      "          vf_explained_var: -0.03777884691953659\n",
      "          vf_loss: 0.016983192814915952\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.858806229773022\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011723834500286367\n",
      "          policy_loss: -0.07657928938057185\n",
      "          total_loss: -0.062217933743210906\n",
      "          vf_explained_var: 0.2784600555896759\n",
      "          vf_loss: 0.00973610181626143\n",
      "    num_agent_steps_sampled: 1668800\n",
      "    num_agent_steps_trained: 1668800\n",
      "    num_steps_sampled: 834400\n",
      "    num_steps_trained: 834400\n",
      "  iterations_since_restore: 298\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 7.688000000000001\n",
      "    gpu_util_percent0: 0.068\n",
      "    ram_util_percent: 62.824000000000005\n",
      "    vram_util_percent0: 0.335359375\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.000000037252903\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.05299999952316284\n",
      "    agent_1: 0.3360000073164702\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05174652872090808\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.9030788437718162\n",
      "    mean_inference_ms: 1.7816612229931754\n",
      "    mean_raw_obs_processing_ms: 0.16414064428479386\n",
      "  time_since_restore: 5564.700445175171\n",
      "  time_this_iter_s: 19.33334517478943\n",
      "  time_total_s: 5564.700445175171\n",
      "  timers:\n",
      "    learn_throughput: 632.869\n",
      "    learn_time_ms: 4424.3\n",
      "    load_throughput: 65362.199\n",
      "    load_time_ms: 42.838\n",
      "    sample_throughput: 186.467\n",
      "    sample_time_ms: 15016.087\n",
      "    update_time_ms: 3.438\n",
      "  timestamp: 1658500491\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 834400\n",
      "  training_iteration: 298\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   298</td><td style=\"text-align: right;\">          5564.7</td><td style=\"text-align: right;\">834400</td><td style=\"text-align: right;\">   0.283</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            202.61</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1674400\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-35-10\n",
      "  done: false\n",
      "  episode_len_mean: 192.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000037252903\n",
      "  episode_reward_mean: 0.26900000751018527\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 3628\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.030655619289194\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.013992248594896143\n",
      "          policy_loss: -0.10902125516683252\n",
      "          total_loss: -0.09337702383016308\n",
      "          vf_explained_var: -0.03162381052970886\n",
      "          vf_loss: 0.0021338467054842374\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.9152152619901157\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.015756393884074477\n",
      "          policy_loss: -0.11451553954040199\n",
      "          total_loss: -0.09869021894562702\n",
      "          vf_explained_var: 0.3737086355686188\n",
      "          vf_loss: 0.002470596444817645\n",
      "    num_agent_steps_sampled: 1674400\n",
      "    num_agent_steps_trained: 1674400\n",
      "    num_steps_sampled: 837200\n",
      "    num_steps_trained: 837200\n",
      "  iterations_since_restore: 299\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.324000000000001\n",
      "    gpu_util_percent0: 0.0724\n",
      "    ram_util_percent: 59.364\n",
      "    vram_util_percent0: 0.28747265624999996\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.000000037252903\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.06299999952316285\n",
      "    agent_1: 0.3320000070333481\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05175113378846021\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.9033748798283763\n",
      "    mean_inference_ms: 1.7819521159979912\n",
      "    mean_raw_obs_processing_ms: 0.16421778352868074\n",
      "  time_since_restore: 5583.554309844971\n",
      "  time_this_iter_s: 18.853864669799805\n",
      "  time_total_s: 5583.554309844971\n",
      "  timers:\n",
      "    learn_throughput: 631.422\n",
      "    learn_time_ms: 4434.438\n",
      "    load_throughput: 65330.239\n",
      "    load_time_ms: 42.859\n",
      "    sample_throughput: 187.341\n",
      "    sample_time_ms: 14946.03\n",
      "    update_time_ms: 3.446\n",
      "  timestamp: 1658500510\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 837200\n",
      "  training_iteration: 299\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   299</td><td style=\"text-align: right;\">         5583.55</td><td style=\"text-align: right;\">837200</td><td style=\"text-align: right;\">   0.269</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            192.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1680000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-35-29\n",
      "  done: false\n",
      "  episode_len_mean: 196.2\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000037252903\n",
      "  episode_reward_mean: 0.28100000724196433\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 3640\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.1177411136173068\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.018320693774452265\n",
      "          policy_loss: -0.06496380824717649\n",
      "          total_loss: -0.04409898557267817\n",
      "          vf_explained_var: -0.11511406302452087\n",
      "          vf_loss: 0.003239490449795128\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.008850631969316\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.013946953876500245\n",
      "          policy_loss: -0.05358400564130404\n",
      "          total_loss: -0.03945840881380848\n",
      "          vf_explained_var: 0.30549177527427673\n",
      "          vf_loss: 0.002894966422640469\n",
      "    num_agent_steps_sampled: 1680000\n",
      "    num_agent_steps_trained: 1680000\n",
      "    num_steps_sampled: 840000\n",
      "    num_steps_trained: 840000\n",
      "  iterations_since_restore: 300\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.923999999999999\n",
      "    gpu_util_percent0: 0.0676\n",
      "    ram_util_percent: 59.376\n",
      "    vram_util_percent0: 0.28523046875\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.000000037252903\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.05799999952316284\n",
      "    agent_1: 0.3390000067651272\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05175501054269033\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.9036289858435236\n",
      "    mean_inference_ms: 1.782206727891461\n",
      "    mean_raw_obs_processing_ms: 0.16428280856860314\n",
      "  time_since_restore: 5602.331683397293\n",
      "  time_this_iter_s: 18.777373552322388\n",
      "  time_total_s: 5602.331683397293\n",
      "  timers:\n",
      "    learn_throughput: 636.203\n",
      "    learn_time_ms: 4401.11\n",
      "    load_throughput: 65437.552\n",
      "    load_time_ms: 42.789\n",
      "    sample_throughput: 188.179\n",
      "    sample_time_ms: 14879.446\n",
      "    update_time_ms: 3.447\n",
      "  timestamp: 1658500529\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 840000\n",
      "  training_iteration: 300\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   300</td><td style=\"text-align: right;\">         5602.33</td><td style=\"text-align: right;\">840000</td><td style=\"text-align: right;\">   0.281</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             196.2</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1685600\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-35-48\n",
      "  done: false\n",
      "  episode_len_mean: 200.52\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000014901161\n",
      "  episode_reward_mean: 0.1990000069886446\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 3652\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.1305643068183038\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010853615687538181\n",
      "          policy_loss: -0.07313496143442103\n",
      "          total_loss: -0.05975671894780784\n",
      "          vf_explained_var: -0.2116481065750122\n",
      "          vf_loss: 0.005863548319861626\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.9711308489952768\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011788724177488266\n",
      "          policy_loss: -0.06905373055001075\n",
      "          total_loss: -0.056097301237584704\n",
      "          vf_explained_var: 0.20823165774345398\n",
      "          vf_loss: 0.0056736308716132215\n",
      "    num_agent_steps_sampled: 1685600\n",
      "    num_agent_steps_trained: 1685600\n",
      "    num_steps_sampled: 842800\n",
      "    num_steps_trained: 842800\n",
      "  iterations_since_restore: 301\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.208333333333333\n",
      "    gpu_util_percent0: 0.05375\n",
      "    ram_util_percent: 59.36666666666667\n",
      "    vram_util_percent0: 0.2818277994791667\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.000000014901161\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.10699999950826168\n",
      "    agent_1: 0.3060000064969063\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05175848228745473\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.9038505201710905\n",
      "    mean_inference_ms: 1.782432142541823\n",
      "    mean_raw_obs_processing_ms: 0.1643409055594251\n",
      "  time_since_restore: 5620.893712282181\n",
      "  time_this_iter_s: 18.562028884887695\n",
      "  time_total_s: 5620.893712282181\n",
      "  timers:\n",
      "    learn_throughput: 642.21\n",
      "    learn_time_ms: 4359.948\n",
      "    load_throughput: 65400.273\n",
      "    load_time_ms: 42.813\n",
      "    sample_throughput: 188.738\n",
      "    sample_time_ms: 14835.342\n",
      "    update_time_ms: 3.421\n",
      "  timestamp: 1658500548\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 842800\n",
      "  training_iteration: 301\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   301</td><td style=\"text-align: right;\">         5620.89</td><td style=\"text-align: right;\">842800</td><td style=\"text-align: right;\">   0.199</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            200.52</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1691200\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-36-07\n",
      "  done: false\n",
      "  episode_len_mean: 200.23\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000014901161\n",
      "  episode_reward_mean: 0.23200000651180744\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 3666\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.0136594424645105\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011381950477812076\n",
      "          policy_loss: -0.087937401374802\n",
      "          total_loss: -0.07484944190599933\n",
      "          vf_explained_var: -0.02159508876502514\n",
      "          vf_loss: 0.0031993337453015904\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.9032623633032753\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.013120942785482117\n",
      "          policy_loss: -0.07574097698047158\n",
      "          total_loss: -0.06173878550171572\n",
      "          vf_explained_var: 0.25914645195007324\n",
      "          vf_loss: 0.004775309231176618\n",
      "    num_agent_steps_sampled: 1691200\n",
      "    num_agent_steps_trained: 1691200\n",
      "    num_steps_sampled: 845600\n",
      "    num_steps_trained: 845600\n",
      "  iterations_since_restore: 302\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.14\n",
      "    gpu_util_percent0: 0.0592\n",
      "    ram_util_percent: 59.332\n",
      "    vram_util_percent0: 0.27924609375\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.000000014901161\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.09099999994039536\n",
      "    agent_1: 0.3230000064522028\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05176081082340891\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.9040671088267205\n",
      "    mean_inference_ms: 1.7826034572494875\n",
      "    mean_raw_obs_processing_ms: 0.16439786177513965\n",
      "  time_since_restore: 5639.636444568634\n",
      "  time_this_iter_s: 18.742732286453247\n",
      "  time_total_s: 5639.636444568634\n",
      "  timers:\n",
      "    learn_throughput: 646.599\n",
      "    learn_time_ms: 4330.351\n",
      "    load_throughput: 65592.515\n",
      "    load_time_ms: 42.688\n",
      "    sample_throughput: 189.452\n",
      "    sample_time_ms: 14779.473\n",
      "    update_time_ms: 3.21\n",
      "  timestamp: 1658500567\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 845600\n",
      "  training_iteration: 302\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   302</td><td style=\"text-align: right;\">         5639.64</td><td style=\"text-align: right;\">845600</td><td style=\"text-align: right;\">   0.232</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            200.23</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1696800\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-36-25\n",
      "  done: false\n",
      "  episode_len_mean: 202.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000014901161\n",
      "  episode_reward_mean: 0.2520000065863133\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 3678\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.0975736443485533\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010178867063323478\n",
      "          policy_loss: -0.07583972881600078\n",
      "          total_loss: -0.06442054127486092\n",
      "          vf_explained_var: 0.0772010087966919\n",
      "          vf_loss: 0.00239887569984733\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.001347303390503\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.013161729353654984\n",
      "          policy_loss: -0.0753835724341722\n",
      "          total_loss: -0.06166295863695701\n",
      "          vf_explained_var: 0.03213658556342125\n",
      "          vf_loss: 0.003971522150355408\n",
      "    num_agent_steps_sampled: 1696800\n",
      "    num_agent_steps_trained: 1696800\n",
      "    num_steps_sampled: 848400\n",
      "    num_steps_trained: 848400\n",
      "  iterations_since_restore: 303\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.020833333333333\n",
      "    gpu_util_percent0: 0.050416666666666665\n",
      "    ram_util_percent: 59.29999999999999\n",
      "    vram_util_percent0: 0.27551676432291666\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.000000014901161\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.07799999982118606\n",
      "    agent_1: 0.3300000064074993\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05176211661560152\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.904212233379729\n",
      "    mean_inference_ms: 1.7827132211120897\n",
      "    mean_raw_obs_processing_ms: 0.16444162719981023\n",
      "  time_since_restore: 5658.261748552322\n",
      "  time_this_iter_s: 18.625303983688354\n",
      "  time_total_s: 5658.261748552322\n",
      "  timers:\n",
      "    learn_throughput: 649.597\n",
      "    learn_time_ms: 4310.363\n",
      "    load_throughput: 93727.763\n",
      "    load_time_ms: 29.874\n",
      "    sample_throughput: 189.816\n",
      "    sample_time_ms: 14751.166\n",
      "    update_time_ms: 3.189\n",
      "  timestamp: 1658500585\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 848400\n",
      "  training_iteration: 303\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   303</td><td style=\"text-align: right;\">         5658.26</td><td style=\"text-align: right;\">848400</td><td style=\"text-align: right;\">   0.252</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            202.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1702400\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-36-44\n",
      "  done: false\n",
      "  episode_len_mean: 205.08\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000014901161\n",
      "  episode_reward_mean: 0.24000000670552255\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 3694\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.0568591420139586\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.007401040151209956\n",
      "          policy_loss: -0.12272646271767804\n",
      "          total_loss: -0.11419687599187628\n",
      "          vf_explained_var: 0.18206088244915009\n",
      "          vf_loss: 0.003022682369624991\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.9111109493034226\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.008832507573052175\n",
      "          policy_loss: -0.11778539249124671\n",
      "          total_loss: -0.10838274480747281\n",
      "          vf_explained_var: 0.261130690574646\n",
      "          vf_loss: 0.003896514949485122\n",
      "    num_agent_steps_sampled: 1702400\n",
      "    num_agent_steps_trained: 1702400\n",
      "    num_steps_sampled: 851200\n",
      "    num_steps_trained: 851200\n",
      "  iterations_since_restore: 304\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.96\n",
      "    gpu_util_percent0: 0.0608\n",
      "    ram_util_percent: 59.29999999999999\n",
      "    vram_util_percent0: 0.27483984375000003\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.000000014901161\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.08799999982118606\n",
      "    agent_1: 0.3280000065267086\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05176224420770182\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.90433026669602\n",
      "    mean_inference_ms: 1.7827782329971\n",
      "    mean_raw_obs_processing_ms: 0.1644963012122267\n",
      "  time_since_restore: 5677.007270812988\n",
      "  time_this_iter_s: 18.745522260665894\n",
      "  time_total_s: 5677.007270812988\n",
      "  timers:\n",
      "    learn_throughput: 650.603\n",
      "    learn_time_ms: 4303.702\n",
      "    load_throughput: 93812.366\n",
      "    load_time_ms: 29.847\n",
      "    sample_throughput: 190.111\n",
      "    sample_time_ms: 14728.272\n",
      "    update_time_ms: 3.193\n",
      "  timestamp: 1658500604\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 851200\n",
      "  training_iteration: 304\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   304</td><td style=\"text-align: right;\">         5677.01</td><td style=\"text-align: right;\">851200</td><td style=\"text-align: right;\">    0.24</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            205.08</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1708000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-37-03\n",
      "  done: false\n",
      "  episode_len_mean: 208.55\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.400000013411045\n",
      "  episode_reward_mean: 0.2620000065863132\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 3706\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.1047250628471375\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.014843792711564986\n",
      "          policy_loss: -0.12600394581386354\n",
      "          total_loss: -0.10991183256089157\n",
      "          vf_explained_var: -0.14223137497901917\n",
      "          vf_loss: 0.0007656066245978028\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.9240585022739\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01310014531452387\n",
      "          policy_loss: -0.0930938813688131\n",
      "          total_loss: -0.07999334015004847\n",
      "          vf_explained_var: -0.0654238685965538\n",
      "          vf_loss: 0.0022854624935245652\n",
      "    num_agent_steps_sampled: 1708000\n",
      "    num_agent_steps_trained: 1708000\n",
      "    num_steps_sampled: 854000\n",
      "    num_steps_trained: 854000\n",
      "  iterations_since_restore: 305\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.951999999999999\n",
      "    gpu_util_percent0: 0.06\n",
      "    ram_util_percent: 59.348\n",
      "    vram_util_percent0: 0.27420703125\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 0.5\n",
      "    agent_1: 1.0000000149011612\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.07799999982118606\n",
      "    agent_1: 0.34000000640749933\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051761879207975314\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.9043883083450175\n",
      "    mean_inference_ms: 1.782797491207399\n",
      "    mean_raw_obs_processing_ms: 0.16452397218218656\n",
      "  time_since_restore: 5695.80432343483\n",
      "  time_this_iter_s: 18.79705262184143\n",
      "  time_total_s: 5695.80432343483\n",
      "  timers:\n",
      "    learn_throughput: 652.572\n",
      "    learn_time_ms: 4290.714\n",
      "    load_throughput: 94889.002\n",
      "    load_time_ms: 29.508\n",
      "    sample_throughput: 191.425\n",
      "    sample_time_ms: 14627.119\n",
      "    update_time_ms: 3.182\n",
      "  timestamp: 1658500623\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 854000\n",
      "  training_iteration: 305\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   305</td><td style=\"text-align: right;\">          5695.8</td><td style=\"text-align: right;\">854000</td><td style=\"text-align: right;\">   0.262</td><td style=\"text-align: right;\">                 1.4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            208.55</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1713600\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-37-22\n",
      "  done: false\n",
      "  episode_len_mean: 205.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000037252903\n",
      "  episode_reward_mean: 0.2560000068694353\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 3721\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.1232594028115273\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.009909128997051589\n",
      "          policy_loss: -0.04977310910769683\n",
      "          total_loss: -0.03538216202014155\n",
      "          vf_explained_var: -0.4412374496459961\n",
      "          vf_loss: 0.011779024258777056\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.8762383879650206\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011840786818977801\n",
      "          policy_loss: -0.05906322756986594\n",
      "          total_loss: -0.0453629581882827\n",
      "          vf_explained_var: 0.19265887141227722\n",
      "          vf_loss: 0.007535754899869235\n",
      "    num_agent_steps_sampled: 1713600\n",
      "    num_agent_steps_trained: 1713600\n",
      "    num_steps_sampled: 856800\n",
      "    num_steps_trained: 856800\n",
      "  iterations_since_restore: 306\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.041666666666667\n",
      "    gpu_util_percent0: 0.04958333333333333\n",
      "    ram_util_percent: 59.74583333333334\n",
      "    vram_util_percent0: 0.2520060221354167\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.000000037252903\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.07599999979138375\n",
      "    agent_1: 0.3320000066608191\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05176140905638421\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.9044546715663984\n",
      "    mean_inference_ms: 1.7828246859820036\n",
      "    mean_raw_obs_processing_ms: 0.16456013534193345\n",
      "  time_since_restore: 5714.662096500397\n",
      "  time_this_iter_s: 18.857773065567017\n",
      "  time_total_s: 5714.662096500397\n",
      "  timers:\n",
      "    learn_throughput: 656.978\n",
      "    learn_time_ms: 4261.94\n",
      "    load_throughput: 94667.108\n",
      "    load_time_ms: 29.577\n",
      "    sample_throughput: 191.927\n",
      "    sample_time_ms: 14588.853\n",
      "    update_time_ms: 3.202\n",
      "  timestamp: 1658500642\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 856800\n",
      "  training_iteration: 306\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   306</td><td style=\"text-align: right;\">         5714.66</td><td style=\"text-align: right;\">856800</td><td style=\"text-align: right;\">   0.256</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            205.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1719200\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-37-41\n",
      "  done: false\n",
      "  episode_len_mean: 198.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000037252903\n",
      "  episode_reward_mean: 0.22100000694394112\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 3737\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.0250249895311536\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011665355322498688\n",
      "          policy_loss: -0.07593519547316689\n",
      "          total_loss: -0.06088660110184546\n",
      "          vf_explained_var: 0.0045847040601074696\n",
      "          vf_loss: 0.007899633875197088\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.823611823930627\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010554820074416157\n",
      "          policy_loss: -0.06332102517453245\n",
      "          total_loss: -0.050804382617310954\n",
      "          vf_explained_var: 0.43722042441368103\n",
      "          vf_loss: 0.007765429312366038\n",
      "    num_agent_steps_sampled: 1719200\n",
      "    num_agent_steps_trained: 1719200\n",
      "    num_steps_sampled: 859600\n",
      "    num_steps_trained: 859600\n",
      "  iterations_since_restore: 307\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.038461538461538\n",
      "    gpu_util_percent0: 0.06461538461538462\n",
      "    ram_util_percent: 59.79999999999999\n",
      "    vram_util_percent0: 0.2500300480769231\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.000000037252903\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.09499999977648259\n",
      "    agent_1: 0.3160000067204237\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051761647073654375\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.9045543764890756\n",
      "    mean_inference_ms: 1.7828797968292016\n",
      "    mean_raw_obs_processing_ms: 0.16461053710256063\n",
      "  time_since_restore: 5733.831091165543\n",
      "  time_this_iter_s: 19.168994665145874\n",
      "  time_total_s: 5733.831091165543\n",
      "  timers:\n",
      "    learn_throughput: 660.977\n",
      "    learn_time_ms: 4236.155\n",
      "    load_throughput: 95927.44\n",
      "    load_time_ms: 29.189\n",
      "    sample_throughput: 192.735\n",
      "    sample_time_ms: 14527.725\n",
      "    update_time_ms: 3.432\n",
      "  timestamp: 1658500661\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 859600\n",
      "  training_iteration: 307\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   307</td><td style=\"text-align: right;\">         5733.83</td><td style=\"text-align: right;\">859600</td><td style=\"text-align: right;\">   0.221</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            198.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1724800\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-38-00\n",
      "  done: false\n",
      "  episode_len_mean: 198.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000037252903\n",
      "  episode_reward_mean: 0.26000000692903996\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 3752\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.0409563332796097\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.014633449499810094\n",
      "          policy_loss: -0.08376206364092857\n",
      "          total_loss: -0.06731027802604894\n",
      "          vf_explained_var: 0.09399066865444183\n",
      "          vf_loss: 0.0023921596035680046\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.881334468367554\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.016109336457532018\n",
      "          policy_loss: -0.08740746118872353\n",
      "          total_loss: -0.07149416665578472\n",
      "          vf_explained_var: 0.28907036781311035\n",
      "          vf_loss: 0.001673968671162757\n",
      "    num_agent_steps_sampled: 1724800\n",
      "    num_agent_steps_trained: 1724800\n",
      "    num_steps_sampled: 862400\n",
      "    num_steps_trained: 862400\n",
      "  iterations_since_restore: 308\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.958333333333333\n",
      "    gpu_util_percent0: 0.04958333333333333\n",
      "    ram_util_percent: 59.70000000000001\n",
      "    vram_util_percent0: 0.25006103515625\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.000000037252903\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.07599999979138375\n",
      "    agent_1: 0.3360000067204237\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0517617988008727\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.9046509571546153\n",
      "    mean_inference_ms: 1.7829311636575562\n",
      "    mean_raw_obs_processing_ms: 0.16466630302980892\n",
      "  time_since_restore: 5752.600853681564\n",
      "  time_this_iter_s: 18.76976251602173\n",
      "  time_total_s: 5752.600853681564\n",
      "  timers:\n",
      "    learn_throughput: 660.895\n",
      "    learn_time_ms: 4236.682\n",
      "    load_throughput: 95956.362\n",
      "    load_time_ms: 29.18\n",
      "    sample_throughput: 193.491\n",
      "    sample_time_ms: 14470.934\n",
      "    update_time_ms: 3.432\n",
      "  timestamp: 1658500680\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 862400\n",
      "  training_iteration: 308\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   308</td><td style=\"text-align: right;\">          5752.6</td><td style=\"text-align: right;\">862400</td><td style=\"text-align: right;\">    0.26</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             198.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1730400\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-38-19\n",
      "  done: false\n",
      "  episode_len_mean: 201.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000037252903\n",
      "  episode_reward_mean: 0.2730000070482492\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 3764\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.12832228378171\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010582265574393922\n",
      "          policy_loss: -0.07112598463234983\n",
      "          total_loss: -0.05881388231167269\n",
      "          vf_explained_var: -0.10018899291753769\n",
      "          vf_loss: 0.003688850804662382\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.9758348074697314\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011359558425106211\n",
      "          policy_loss: -0.05535076165688224\n",
      "          total_loss: -0.04361770338506349\n",
      "          vf_explained_var: 0.011304652318358421\n",
      "          vf_loss: 0.003412019449526616\n",
      "    num_agent_steps_sampled: 1730400\n",
      "    num_agent_steps_trained: 1730400\n",
      "    num_steps_sampled: 865200\n",
      "    num_steps_trained: 865200\n",
      "  iterations_since_restore: 309\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.0760000000000005\n",
      "    gpu_util_percent0: 0.0504\n",
      "    ram_util_percent: 59.79999999999999\n",
      "    vram_util_percent0: 0.25005078125\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.000000037252903\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.07199999980628491\n",
      "    agent_1: 0.34500000685453414\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05176259094982049\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.904737248060826\n",
      "    mean_inference_ms: 1.7829966148570895\n",
      "    mean_raw_obs_processing_ms: 0.1647067249027722\n",
      "  time_since_restore: 5771.6022210121155\n",
      "  time_this_iter_s: 19.001367330551147\n",
      "  time_total_s: 5771.6022210121155\n",
      "  timers:\n",
      "    learn_throughput: 661.614\n",
      "    learn_time_ms: 4232.073\n",
      "    load_throughput: 95389.742\n",
      "    load_time_ms: 29.353\n",
      "    sample_throughput: 193.237\n",
      "    sample_time_ms: 14489.946\n",
      "    update_time_ms: 3.446\n",
      "  timestamp: 1658500699\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 865200\n",
      "  training_iteration: 309\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   309</td><td style=\"text-align: right;\">          5771.6</td><td style=\"text-align: right;\">865200</td><td style=\"text-align: right;\">   0.273</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            201.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1736000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-38-37\n",
      "  done: false\n",
      "  episode_len_mean: 197.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000037252903\n",
      "  episode_reward_mean: 0.23900000721216202\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 3778\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.046305507776283\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.00878596744244495\n",
      "          policy_loss: -0.058294763888018984\n",
      "          total_loss: -0.04745538340440752\n",
      "          vf_explained_var: -0.28330695629119873\n",
      "          vf_loss: 0.005156159031771711\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.8743383657364618\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.00909270463542798\n",
      "          policy_loss: -0.05649816482625846\n",
      "          total_loss: -0.046243133139796555\n",
      "          vf_explained_var: 0.04221024736762047\n",
      "          vf_loss: 0.005543359467299611\n",
      "    num_agent_steps_sampled: 1736000\n",
      "    num_agent_steps_trained: 1736000\n",
      "    num_steps_sampled: 868000\n",
      "    num_steps_trained: 868000\n",
      "  iterations_since_restore: 310\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.176\n",
      "    gpu_util_percent0: 0.06199999999999999\n",
      "    ram_util_percent: 59.924\n",
      "    vram_util_percent0: 0.25003125\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.000000037252903\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.09099999971687794\n",
      "    agent_1: 0.33000000692903997\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05176342946873749\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.904863047998431\n",
      "    mean_inference_ms: 1.7830739849306851\n",
      "    mean_raw_obs_processing_ms: 0.1647594121496844\n",
      "  time_since_restore: 5790.335112571716\n",
      "  time_this_iter_s: 18.73289155960083\n",
      "  time_total_s: 5790.335112571716\n",
      "  timers:\n",
      "    learn_throughput: 661.891\n",
      "    learn_time_ms: 4230.301\n",
      "    load_throughput: 94995.229\n",
      "    load_time_ms: 29.475\n",
      "    sample_throughput: 193.278\n",
      "    sample_time_ms: 14486.916\n",
      "    update_time_ms: 3.478\n",
      "  timestamp: 1658500717\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 868000\n",
      "  training_iteration: 310\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   310</td><td style=\"text-align: right;\">         5790.34</td><td style=\"text-align: right;\">868000</td><td style=\"text-align: right;\">   0.239</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            197.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1741600\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-38-56\n",
      "  done: false\n",
      "  episode_len_mean: 195.54\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000037252903\n",
      "  episode_reward_mean: 0.31200000740587713\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 3794\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.0660886487790515\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.007319825539356305\n",
      "          policy_loss: -0.05567245786577197\n",
      "          total_loss: -0.04313890317243498\n",
      "          vf_explained_var: -0.4822688400745392\n",
      "          vf_loss: 0.014724457035342874\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.8756504502324831\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010464220511082048\n",
      "          policy_loss: -0.07049132359956968\n",
      "          total_loss: -0.05846731860981977\n",
      "          vf_explained_var: 0.3280482888221741\n",
      "          vf_loss: 0.006679527566856926\n",
      "    num_agent_steps_sampled: 1741600\n",
      "    num_agent_steps_trained: 1741600\n",
      "    num_steps_sampled: 870800\n",
      "    num_steps_trained: 870800\n",
      "  iterations_since_restore: 311\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.032\n",
      "    gpu_util_percent0: 0.0564\n",
      "    ram_util_percent: 60.044\n",
      "    vram_util_percent0: 0.25005468750000004\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.000000037252903\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.04299999944865704\n",
      "    agent_1: 0.35500000685453414\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05176454841574486\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.9050147867885414\n",
      "    mean_inference_ms: 1.7831650100797676\n",
      "    mean_raw_obs_processing_ms: 0.1648198851746255\n",
      "  time_since_restore: 5809.240947961807\n",
      "  time_this_iter_s: 18.905835390090942\n",
      "  time_total_s: 5809.240947961807\n",
      "  timers:\n",
      "    learn_throughput: 662.306\n",
      "    learn_time_ms: 4227.656\n",
      "    load_throughput: 95021.9\n",
      "    load_time_ms: 29.467\n",
      "    sample_throughput: 192.826\n",
      "    sample_time_ms: 14520.859\n",
      "    update_time_ms: 3.468\n",
      "  timestamp: 1658500736\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 870800\n",
      "  training_iteration: 311\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   311</td><td style=\"text-align: right;\">         5809.24</td><td style=\"text-align: right;\">870800</td><td style=\"text-align: right;\">   0.312</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            195.54</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1747200\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-39-15\n",
      "  done: false\n",
      "  episode_len_mean: 196.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000037252903\n",
      "  episode_reward_mean: 0.2950000074505806\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 3806\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.052786074578762\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.014522723487103716\n",
      "          policy_loss: -0.08676102492470472\n",
      "          total_loss: -0.07025490859675865\n",
      "          vf_explained_var: 0.11602085828781128\n",
      "          vf_loss: 0.0029168989941743867\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.9169064962438174\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01449030097679392\n",
      "          policy_loss: -0.06411460955617561\n",
      "          total_loss: -0.049102061962046376\n",
      "          vf_explained_var: 0.02547200210392475\n",
      "          vf_loss: 0.003766681561198008\n",
      "    num_agent_steps_sampled: 1747200\n",
      "    num_agent_steps_trained: 1747200\n",
      "    num_steps_sampled: 873600\n",
      "    num_steps_trained: 873600\n",
      "  iterations_since_restore: 312\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.904166666666666\n",
      "    gpu_util_percent0: 0.05958333333333333\n",
      "    ram_util_percent: 59.8375\n",
      "    vram_util_percent0: 0.2500284830729167\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.000000037252903\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.052999999448657034\n",
      "    agent_1: 0.34800000689923766\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05176480563077476\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.905109912289541\n",
      "    mean_inference_ms: 1.7832066494707968\n",
      "    mean_raw_obs_processing_ms: 0.16486331115665884\n",
      "  time_since_restore: 5827.613193511963\n",
      "  time_this_iter_s: 18.37224555015564\n",
      "  time_total_s: 5827.613193511963\n",
      "  timers:\n",
      "    learn_throughput: 662.023\n",
      "    learn_time_ms: 4229.458\n",
      "    load_throughput: 95271.192\n",
      "    load_time_ms: 29.39\n",
      "    sample_throughput: 193.342\n",
      "    sample_time_ms: 14482.093\n",
      "    update_time_ms: 3.477\n",
      "  timestamp: 1658500755\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 873600\n",
      "  training_iteration: 312\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   312</td><td style=\"text-align: right;\">         5827.61</td><td style=\"text-align: right;\">873600</td><td style=\"text-align: right;\">   0.295</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            196.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1752800\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-39-33\n",
      "  done: false\n",
      "  episode_len_mean: 189.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.3170000079274178\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 3824\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.05169115960598\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.009700538418483576\n",
      "          policy_loss: -0.062188472109916064\n",
      "          total_loss: -0.0477168809624605\n",
      "          vf_explained_var: -0.4016377031803131\n",
      "          vf_loss: 0.01259415636342878\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.7980245880427814\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010056615039365174\n",
      "          policy_loss: -0.06765656083768504\n",
      "          total_loss: -0.05583214509083302\n",
      "          vf_explained_var: 0.0693754032254219\n",
      "          vf_loss: 0.007181184186352073\n",
      "    num_agent_steps_sampled: 1752800\n",
      "    num_agent_steps_trained: 1752800\n",
      "    num_steps_sampled: 876400\n",
      "    num_steps_trained: 876400\n",
      "  iterations_since_restore: 313\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.966666666666666\n",
      "    gpu_util_percent0: 0.049999999999999996\n",
      "    ram_util_percent: 59.78333333333333\n",
      "    vram_util_percent0: 0.2500284830729167\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.03899999924004078\n",
      "    agent_1: 0.35600000716745855\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05176422493446207\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.905205034362384\n",
      "    mean_inference_ms: 1.7832270262840755\n",
      "    mean_raw_obs_processing_ms: 0.1649350658966856\n",
      "  time_since_restore: 5846.242714881897\n",
      "  time_this_iter_s: 18.629521369934082\n",
      "  time_total_s: 5846.242714881897\n",
      "  timers:\n",
      "    learn_throughput: 661.084\n",
      "    learn_time_ms: 4235.469\n",
      "    load_throughput: 95527.0\n",
      "    load_time_ms: 29.311\n",
      "    sample_throughput: 193.417\n",
      "    sample_time_ms: 14476.519\n",
      "    update_time_ms: 3.506\n",
      "  timestamp: 1658500773\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 876400\n",
      "  training_iteration: 313\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   313</td><td style=\"text-align: right;\">         5846.24</td><td style=\"text-align: right;\">876400</td><td style=\"text-align: right;\">   0.317</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            189.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1758400\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-39-52\n",
      "  done: false\n",
      "  episode_len_mean: 194.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.4040000079572201\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 3838\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.1191535535312833\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.009339475283748587\n",
      "          policy_loss: -0.0499675605730902\n",
      "          total_loss: -0.0361871550656447\n",
      "          vf_explained_var: -0.48494619131088257\n",
      "          vf_loss: 0.011860691496535548\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.8942719101905823\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.013806580543987926\n",
      "          policy_loss: -0.06085424611107252\n",
      "          total_loss: -0.04653693504925676\n",
      "          vf_explained_var: 0.17200009524822235\n",
      "          vf_loss: 0.003706933604050954\n",
      "    num_agent_steps_sampled: 1758400\n",
      "    num_agent_steps_trained: 1758400\n",
      "    num_steps_sampled: 879200\n",
      "    num_steps_trained: 879200\n",
      "  iterations_since_restore: 314\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.920833333333333\n",
      "    gpu_util_percent0: 0.05125\n",
      "    ram_util_percent: 59.79999999999999\n",
      "    vram_util_percent0: 0.25003662109374997\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: 0.006000000759959221\n",
      "    agent_1: 0.39800000719726086\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05176293806402905\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.905233205094154\n",
      "    mean_inference_ms: 1.7832075227303061\n",
      "    mean_raw_obs_processing_ms: 0.1649842116466353\n",
      "  time_since_restore: 5864.6658680438995\n",
      "  time_this_iter_s: 18.423153162002563\n",
      "  time_total_s: 5864.6658680438995\n",
      "  timers:\n",
      "    learn_throughput: 662.483\n",
      "    learn_time_ms: 4226.524\n",
      "    load_throughput: 95883.582\n",
      "    load_time_ms: 29.202\n",
      "    sample_throughput: 193.727\n",
      "    sample_time_ms: 14453.327\n",
      "    update_time_ms: 3.495\n",
      "  timestamp: 1658500792\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 879200\n",
      "  training_iteration: 314\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   314</td><td style=\"text-align: right;\">         5864.67</td><td style=\"text-align: right;\">879200</td><td style=\"text-align: right;\">   0.404</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            194.39</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1764000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-40-10\n",
      "  done: false\n",
      "  episode_len_mean: 186.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.42700000859797\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 3855\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.9626808514197667\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.009458845197287468\n",
      "          policy_loss: -0.061776096042411244\n",
      "          total_loss: -0.04462735897924999\n",
      "          vf_explained_var: -0.24651439487934113\n",
      "          vf_loss: 0.020907028574410583\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.7952025848485174\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010654445709430563\n",
      "          policy_loss: -0.06814950335750664\n",
      "          total_loss: -0.05501788189992242\n",
      "          vf_explained_var: 0.15757052600383759\n",
      "          vf_loss: 0.00920286039105295\n",
      "    num_agent_steps_sampled: 1764000\n",
      "    num_agent_steps_trained: 1764000\n",
      "    num_steps_sampled: 882000\n",
      "    num_steps_trained: 882000\n",
      "  iterations_since_restore: 315\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.151999999999999\n",
      "    gpu_util_percent0: 0.062400000000000004\n",
      "    ram_util_percent: 59.80399999999999\n",
      "    vram_util_percent0: 0.25028515625000003\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: 0.03300000123679638\n",
      "    agent_1: 0.39400000736117363\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051761070785026606\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.9052441172314825\n",
      "    mean_inference_ms: 1.7831561239513827\n",
      "    mean_raw_obs_processing_ms: 0.16504985508038242\n",
      "  time_since_restore: 5883.165948152542\n",
      "  time_this_iter_s: 18.500080108642578\n",
      "  time_total_s: 5883.165948152542\n",
      "  timers:\n",
      "    learn_throughput: 663.451\n",
      "    learn_time_ms: 4220.356\n",
      "    load_throughput: 95727.738\n",
      "    load_time_ms: 29.25\n",
      "    sample_throughput: 194.043\n",
      "    sample_time_ms: 14429.799\n",
      "    update_time_ms: 3.492\n",
      "  timestamp: 1658500810\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 882000\n",
      "  training_iteration: 315\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   315</td><td style=\"text-align: right;\">         5883.17</td><td style=\"text-align: right;\">882000</td><td style=\"text-align: right;\">   0.427</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            186.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1769600\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-40-29\n",
      "  done: false\n",
      "  episode_len_mean: 177.65\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.5200000086426735\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 3871\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.03258027810426\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011229963025899202\n",
      "          policy_loss: -0.06698573669252385\n",
      "          total_loss: -0.051283674981234414\n",
      "          vf_explained_var: -0.2616875171661377\n",
      "          vf_loss: 0.011172295353996256\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.8222259591732706\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.013805725577193803\n",
      "          policy_loss: -0.09962504621390031\n",
      "          total_loss: -0.08515837732591622\n",
      "          vf_explained_var: 0.11690209060907364\n",
      "          vf_loss: 0.0040502050691256525\n",
      "    num_agent_steps_sampled: 1769600\n",
      "    num_agent_steps_trained: 1769600\n",
      "    num_steps_sampled: 884800\n",
      "    num_steps_trained: 884800\n",
      "  iterations_since_restore: 316\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.224\n",
      "    gpu_util_percent0: 0.0576\n",
      "    ram_util_percent: 59.78799999999999\n",
      "    vram_util_percent0: 0.25002734375\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: 0.08700000129640102\n",
      "    agent_1: 0.4330000073462725\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05175889158671019\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.905245724051218\n",
      "    mean_inference_ms: 1.783108095707048\n",
      "    mean_raw_obs_processing_ms: 0.16512082252242602\n",
      "  time_since_restore: 5902.087689161301\n",
      "  time_this_iter_s: 18.921741008758545\n",
      "  time_total_s: 5902.087689161301\n",
      "  timers:\n",
      "    learn_throughput: 663.788\n",
      "    learn_time_ms: 4218.216\n",
      "    load_throughput: 95788.014\n",
      "    load_time_ms: 29.231\n",
      "    sample_throughput: 193.969\n",
      "    sample_time_ms: 14435.299\n",
      "    update_time_ms: 3.497\n",
      "  timestamp: 1658500829\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 884800\n",
      "  training_iteration: 316\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   316</td><td style=\"text-align: right;\">         5902.09</td><td style=\"text-align: right;\">884800</td><td style=\"text-align: right;\">    0.52</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            177.65</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1775200\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-40-48\n",
      "  done: false\n",
      "  episode_len_mean: 181.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.5460000084340573\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 3884\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.084590231733663\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.015310480332462226\n",
      "          policy_loss: -0.07891217729619322\n",
      "          total_loss: -0.057367432265891694\n",
      "          vf_explained_var: -0.1422705352306366\n",
      "          vf_loss: 0.01480837272763546\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.909101474852789\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01293863276626363\n",
      "          policy_loss: -0.07381091617433067\n",
      "          total_loss: -0.059463887250915705\n",
      "          vf_explained_var: 0.17512404918670654\n",
      "          vf_loss: 0.006287068033348727\n",
      "    num_agent_steps_sampled: 1775200\n",
      "    num_agent_steps_trained: 1775200\n",
      "    num_steps_sampled: 887600\n",
      "    num_steps_trained: 887600\n",
      "  iterations_since_restore: 317\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.9375\n",
      "    gpu_util_percent0: 0.050833333333333335\n",
      "    ram_util_percent: 59.76250000000001\n",
      "    vram_util_percent0: 0.2500284830729167\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: 0.10100000105798244\n",
      "    agent_1: 0.4450000073760748\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05175725807834262\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.9052374162076946\n",
      "    mean_inference_ms: 1.7830719826681394\n",
      "    mean_raw_obs_processing_ms: 0.16517321766167573\n",
      "  time_since_restore: 5920.719631910324\n",
      "  time_this_iter_s: 18.631942749023438\n",
      "  time_total_s: 5920.719631910324\n",
      "  timers:\n",
      "    learn_throughput: 666.059\n",
      "    learn_time_ms: 4203.829\n",
      "    load_throughput: 95542.466\n",
      "    load_time_ms: 29.306\n",
      "    sample_throughput: 194.49\n",
      "    sample_time_ms: 14396.663\n",
      "    update_time_ms: 3.269\n",
      "  timestamp: 1658500848\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 887600\n",
      "  training_iteration: 317\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   317</td><td style=\"text-align: right;\">         5920.72</td><td style=\"text-align: right;\">887600</td><td style=\"text-align: right;\">   0.546</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            181.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1780800\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-41-07\n",
      "  done: false\n",
      "  episode_len_mean: 178.54\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.5590000082552433\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 3901\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.0526322594710757\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.012145743743705563\n",
      "          policy_loss: -0.0656429658542731\n",
      "          total_loss: -0.051401219812327155\n",
      "          vf_explained_var: 0.07889781892299652\n",
      "          vf_loss: 0.004086364310361594\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.8216821852894056\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.012441211536085947\n",
      "          policy_loss: -0.08312769541780221\n",
      "          total_loss: -0.07015436013656047\n",
      "          vf_explained_var: 0.3861297369003296\n",
      "          vf_loss: 0.003681826925025061\n",
      "    num_agent_steps_sampled: 1780800\n",
      "    num_agent_steps_trained: 1780800\n",
      "    num_steps_sampled: 890400\n",
      "    num_steps_trained: 890400\n",
      "  iterations_since_restore: 318\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.075999999999999\n",
      "    gpu_util_percent0: 0.0564\n",
      "    ram_util_percent: 59.79999999999999\n",
      "    vram_util_percent0: 0.25005468750000004\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: 0.10200000084936618\n",
      "    agent_1: 0.4570000074058771\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05175578889559743\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.905238687303458\n",
      "    mean_inference_ms: 1.7830514259454464\n",
      "    mean_raw_obs_processing_ms: 0.16525302952290286\n",
      "  time_since_restore: 5939.632751464844\n",
      "  time_this_iter_s: 18.913119554519653\n",
      "  time_total_s: 5939.632751464844\n",
      "  timers:\n",
      "    learn_throughput: 667.058\n",
      "    learn_time_ms: 4197.535\n",
      "    load_throughput: 95768.174\n",
      "    load_time_ms: 29.237\n",
      "    sample_throughput: 194.21\n",
      "    sample_time_ms: 14417.361\n",
      "    update_time_ms: 3.282\n",
      "  timestamp: 1658500867\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 890400\n",
      "  training_iteration: 318\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   318</td><td style=\"text-align: right;\">         5939.63</td><td style=\"text-align: right;\">890400</td><td style=\"text-align: right;\">   0.559</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            178.54</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1786400\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-41-26\n",
      "  done: false\n",
      "  episode_len_mean: 187.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.5490000081062317\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 3912\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.044777495165666\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.012187627704766843\n",
      "          policy_loss: -0.08321949894733345\n",
      "          total_loss: -0.06888938153722501\n",
      "          vf_explained_var: -0.13701732456684113\n",
      "          vf_loss: 0.004194799756944468\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.932630331743331\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01183945601460232\n",
      "          policy_loss: -0.061272792533383746\n",
      "          total_loss: -0.048735053515494134\n",
      "          vf_explained_var: -0.18131986260414124\n",
      "          vf_loss: 0.004287875338507417\n",
      "    num_agent_steps_sampled: 1786400\n",
      "    num_agent_steps_trained: 1786400\n",
      "    num_steps_sampled: 893200\n",
      "    num_steps_trained: 893200\n",
      "  iterations_since_restore: 319\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.151999999999999\n",
      "    gpu_util_percent0: 0.0572\n",
      "    ram_util_percent: 59.84400000000001\n",
      "    vram_util_percent0: 0.254953125\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: 0.10200000084936618\n",
      "    agent_1: 0.4470000072568655\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05175549264261352\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.905274862538008\n",
      "    mean_inference_ms: 1.7830711594587547\n",
      "    mean_raw_obs_processing_ms: 0.16529621844192793\n",
      "  time_since_restore: 5958.717411518097\n",
      "  time_this_iter_s: 19.084660053253174\n",
      "  time_total_s: 5958.717411518097\n",
      "  timers:\n",
      "    learn_throughput: 666.313\n",
      "    learn_time_ms: 4202.227\n",
      "    load_throughput: 96255.298\n",
      "    load_time_ms: 29.089\n",
      "    sample_throughput: 194.156\n",
      "    sample_time_ms: 14421.418\n",
      "    update_time_ms: 3.255\n",
      "  timestamp: 1658500886\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 893200\n",
      "  training_iteration: 319\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   319</td><td style=\"text-align: right;\">         5958.72</td><td style=\"text-align: right;\">893200</td><td style=\"text-align: right;\">   0.549</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            187.47</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1792000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-41-45\n",
      "  done: false\n",
      "  episode_len_mean: 194.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000037252903\n",
      "  episode_reward_mean: 0.5130000075697899\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 3922\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.0102480493840718\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.013728594100663991\n",
      "          policy_loss: -0.11458451329408922\n",
      "          total_loss: -0.09927324385561016\n",
      "          vf_explained_var: 0.06842031329870224\n",
      "          vf_loss: 0.002005854339499603\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.9563892107634318\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.013950193641740242\n",
      "          policy_loss: -0.08519500667638827\n",
      "          total_loss: -0.07142088738085779\n",
      "          vf_explained_var: 0.08462289720773697\n",
      "          vf_loss: 0.0018200869383127568\n",
      "    num_agent_steps_sampled: 1792000\n",
      "    num_agent_steps_trained: 1792000\n",
      "    num_steps_sampled: 896000\n",
      "    num_steps_trained: 896000\n",
      "  iterations_since_restore: 320\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.047999999999999\n",
      "    gpu_util_percent0: 0.0608\n",
      "    ram_util_percent: 59.684\n",
      "    vram_util_percent0: 0.25272656250000003\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.000000037252903\n",
      "  policy_reward_mean:\n",
      "    agent_0: 0.08500000096857548\n",
      "    agent_1: 0.4280000066012144\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051755443267495836\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.9053213635198585\n",
      "    mean_inference_ms: 1.7830994598808012\n",
      "    mean_raw_obs_processing_ms: 0.16532674509572934\n",
      "  time_since_restore: 5977.469278335571\n",
      "  time_this_iter_s: 18.751866817474365\n",
      "  time_total_s: 5977.469278335571\n",
      "  timers:\n",
      "    learn_throughput: 667.485\n",
      "    learn_time_ms: 4194.854\n",
      "    load_throughput: 96426.64\n",
      "    load_time_ms: 29.038\n",
      "    sample_throughput: 194.029\n",
      "    sample_time_ms: 14430.854\n",
      "    update_time_ms: 3.245\n",
      "  timestamp: 1658500905\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 896000\n",
      "  training_iteration: 320\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   320</td><td style=\"text-align: right;\">         5977.47</td><td style=\"text-align: right;\">896000</td><td style=\"text-align: right;\">   0.513</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            194.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1797600\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-42-04\n",
      "  done: false\n",
      "  episode_len_mean: 189.28\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000037252903\n",
      "  episode_reward_mean: 0.4920000071823597\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 3940\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.0210681656996408\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.007578413426504396\n",
      "          policy_loss: -0.05121539267167377\n",
      "          total_loss: -0.03912117582409077\n",
      "          vf_explained_var: -0.22132587432861328\n",
      "          vf_loss: 0.012586386825984976\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.7957791058080537\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.00991314725137858\n",
      "          policy_loss: -0.061433487300140165\n",
      "          total_loss: -0.05035565343548244\n",
      "          vf_explained_var: 0.41594594717025757\n",
      "          vf_loss: 0.005456827111932701\n",
      "    num_agent_steps_sampled: 1797600\n",
      "    num_agent_steps_trained: 1797600\n",
      "    num_steps_sampled: 898800\n",
      "    num_steps_trained: 898800\n",
      "  iterations_since_restore: 321\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.288\n",
      "    gpu_util_percent0: 0.0652\n",
      "    ram_util_percent: 59.78\n",
      "    vram_util_percent0: 0.2578359375\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.000000037252903\n",
      "  policy_reward_mean:\n",
      "    agent_0: 0.0710000005364418\n",
      "    agent_1: 0.4210000066459179\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05175622200920822\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.905450528927455\n",
      "    mean_inference_ms: 1.7831862909794705\n",
      "    mean_raw_obs_processing_ms: 0.16539715917003647\n",
      "  time_since_restore: 5996.52987408638\n",
      "  time_this_iter_s: 19.060595750808716\n",
      "  time_total_s: 5996.52987408638\n",
      "  timers:\n",
      "    learn_throughput: 666.666\n",
      "    learn_time_ms: 4200.003\n",
      "    load_throughput: 96806.338\n",
      "    load_time_ms: 28.924\n",
      "    sample_throughput: 193.847\n",
      "    sample_time_ms: 14444.379\n",
      "    update_time_ms: 3.262\n",
      "  timestamp: 1658500924\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 898800\n",
      "  training_iteration: 321\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   321</td><td style=\"text-align: right;\">         5996.53</td><td style=\"text-align: right;\">898800</td><td style=\"text-align: right;\">   0.492</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            189.28</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1803200\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-42-23\n",
      "  done: false\n",
      "  episode_len_mean: 197.65\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000014901161\n",
      "  episode_reward_mean: 0.4540000069886446\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 3955\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.0852936410478184\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.013120683289466204\n",
      "          policy_loss: -0.07831805573931931\n",
      "          total_loss: -0.06332756455005784\n",
      "          vf_explained_var: -0.14887449145317078\n",
      "          vf_loss: 0.0031315350734075764\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.922647395304271\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.012169886279359226\n",
      "          policy_loss: -0.057370468752369994\n",
      "          total_loss: -0.044985029713383744\n",
      "          vf_explained_var: 0.11448496580123901\n",
      "          vf_loss: 0.0028979826738577685\n",
      "    num_agent_steps_sampled: 1803200\n",
      "    num_agent_steps_trained: 1803200\n",
      "    num_steps_sampled: 901600\n",
      "    num_steps_trained: 901600\n",
      "  iterations_since_restore: 322\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.28\n",
      "    gpu_util_percent0: 0.06680000000000001\n",
      "    ram_util_percent: 59.784000000000006\n",
      "    vram_util_percent0: 0.2591796875\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.000000014901161\n",
      "  policy_reward_mean:\n",
      "    agent_0: 0.0410000005364418\n",
      "    agent_1: 0.4130000064522028\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051757681077701304\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.905583368579847\n",
      "    mean_inference_ms: 1.7832971024069542\n",
      "    mean_raw_obs_processing_ms: 0.1654532710218148\n",
      "  time_since_restore: 6015.503463745117\n",
      "  time_this_iter_s: 18.973589658737183\n",
      "  time_total_s: 6015.503463745117\n",
      "  timers:\n",
      "    learn_throughput: 667.241\n",
      "    learn_time_ms: 4196.385\n",
      "    load_throughput: 96592.078\n",
      "    load_time_ms: 28.988\n",
      "    sample_throughput: 192.996\n",
      "    sample_time_ms: 14508.049\n",
      "    update_time_ms: 3.296\n",
      "  timestamp: 1658500943\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 901600\n",
      "  training_iteration: 322\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   322</td><td style=\"text-align: right;\">          6015.5</td><td style=\"text-align: right;\">901600</td><td style=\"text-align: right;\">   0.454</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            197.65</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1808800\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-42-41\n",
      "  done: false\n",
      "  episode_len_mean: 202.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000014901161\n",
      "  episode_reward_mean: 0.4070000070333481\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 3965\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.9935964651051021\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.012061805248592435\n",
      "          policy_loss: -0.0668171228750171\n",
      "          total_loss: -0.05321341026657527\n",
      "          vf_explained_var: -0.11381405591964722\n",
      "          vf_loss: 0.0024644738951749225\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.8629853061976887\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011973487500940464\n",
      "          policy_loss: -0.06835137088923407\n",
      "          total_loss: -0.05619523239008794\n",
      "          vf_explained_var: 0.05034879967570305\n",
      "          vf_loss: 0.002733224936361824\n",
      "    num_agent_steps_sampled: 1808800\n",
      "    num_agent_steps_trained: 1808800\n",
      "    num_steps_sampled: 904400\n",
      "    num_steps_trained: 904400\n",
      "  iterations_since_restore: 323\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.9333333333333345\n",
      "    gpu_util_percent0: 0.06\n",
      "    ram_util_percent: 59.72083333333333\n",
      "    vram_util_percent0: 0.25205078125\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.000000014901161\n",
      "  policy_reward_mean:\n",
      "    agent_0: 0.011000000536441804\n",
      "    agent_1: 0.39600000649690625\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051758450937155434\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.905644071724845\n",
      "    mean_inference_ms: 1.7833548356127427\n",
      "    mean_raw_obs_processing_ms: 0.16547782950415307\n",
      "  time_since_restore: 6033.905391454697\n",
      "  time_this_iter_s: 18.401927709579468\n",
      "  time_total_s: 6033.905391454697\n",
      "  timers:\n",
      "    learn_throughput: 667.641\n",
      "    learn_time_ms: 4193.868\n",
      "    load_throughput: 96267.055\n",
      "    load_time_ms: 29.086\n",
      "    sample_throughput: 193.265\n",
      "    sample_time_ms: 14487.881\n",
      "    update_time_ms: 3.274\n",
      "  timestamp: 1658500961\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 904400\n",
      "  training_iteration: 323\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   323</td><td style=\"text-align: right;\">         6033.91</td><td style=\"text-align: right;\">904400</td><td style=\"text-align: right;\">   0.407</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            202.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1814400\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-43-00\n",
      "  done: false\n",
      "  episode_len_mean: 206.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.4400000074505806\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 3978\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.045470863225914\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.007430302713843257\n",
      "          policy_loss: -0.056084389703452565\n",
      "          total_loss: -0.04232735299285109\n",
      "          vf_explained_var: -0.5223551988601685\n",
      "          vf_loss: 0.01783781829206938\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.8564260601997375\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010279401160725465\n",
      "          policy_loss: -0.06573333866842967\n",
      "          total_loss: -0.053564220486372334\n",
      "          vf_explained_var: 0.12495005875825882\n",
      "          vf_loss: 0.007598540554248584\n",
      "    num_agent_steps_sampled: 1814400\n",
      "    num_agent_steps_trained: 1814400\n",
      "    num_steps_sampled: 907200\n",
      "    num_steps_trained: 907200\n",
      "  iterations_since_restore: 324\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.2625\n",
      "    gpu_util_percent0: 0.05791666666666667\n",
      "    ram_util_percent: 59.824999999999996\n",
      "    vram_util_percent0: 0.2548421223958333\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: 0.03200000047683716\n",
      "    agent_1: 0.40800000697374345\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051759342086621495\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.905731987437232\n",
      "    mean_inference_ms: 1.7834224859573913\n",
      "    mean_raw_obs_processing_ms: 0.165510711092535\n",
      "  time_since_restore: 6052.615019083023\n",
      "  time_this_iter_s: 18.709627628326416\n",
      "  time_total_s: 6052.615019083023\n",
      "  timers:\n",
      "    learn_throughput: 667.251\n",
      "    learn_time_ms: 4196.324\n",
      "    load_throughput: 96142.222\n",
      "    load_time_ms: 29.124\n",
      "    sample_throughput: 192.916\n",
      "    sample_time_ms: 14514.057\n",
      "    update_time_ms: 3.292\n",
      "  timestamp: 1658500980\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 907200\n",
      "  training_iteration: 324\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   324</td><td style=\"text-align: right;\">         6052.62</td><td style=\"text-align: right;\">907200</td><td style=\"text-align: right;\">    0.44</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            206.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1820000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-43-19\n",
      "  done: false\n",
      "  episode_len_mean: 211.52\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.4320000074803829\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 3990\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.056211906529608\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.014519626216765882\n",
      "          policy_loss: -0.08200909711740678\n",
      "          total_loss: -0.06559763397637566\n",
      "          vf_explained_var: -0.4261251986026764\n",
      "          vf_loss: 0.002660711361906059\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.9407624683919407\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01665533589815594\n",
      "          policy_loss: -0.11298345527687897\n",
      "          total_loss: -0.09653918390254798\n",
      "          vf_explained_var: -0.04817742481827736\n",
      "          vf_loss: 0.0017016516128586815\n",
      "    num_agent_steps_sampled: 1820000\n",
      "    num_agent_steps_trained: 1820000\n",
      "    num_steps_sampled: 910000\n",
      "    num_steps_trained: 910000\n",
      "  iterations_since_restore: 325\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.8919999999999995\n",
      "    gpu_util_percent0: 0.054000000000000006\n",
      "    ram_util_percent: 59.736\n",
      "    vram_util_percent0: 0.24794921875\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: 0.02200000047683716\n",
      "    agent_1: 0.4100000070035458\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051760209919435314\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.9058203119033363\n",
      "    mean_inference_ms: 1.7834812009710155\n",
      "    mean_raw_obs_processing_ms: 0.16553338070128526\n",
      "  time_since_restore: 6071.425649881363\n",
      "  time_this_iter_s: 18.810630798339844\n",
      "  time_total_s: 6071.425649881363\n",
      "  timers:\n",
      "    learn_throughput: 666.455\n",
      "    learn_time_ms: 4201.333\n",
      "    load_throughput: 96059.337\n",
      "    load_time_ms: 29.149\n",
      "    sample_throughput: 192.571\n",
      "    sample_time_ms: 14540.069\n",
      "    update_time_ms: 3.302\n",
      "  timestamp: 1658500999\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 910000\n",
      "  training_iteration: 325\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   325</td><td style=\"text-align: right;\">         6071.43</td><td style=\"text-align: right;\">910000</td><td style=\"text-align: right;\">   0.432</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            211.52</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1825600\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-43-38\n",
      "  done: false\n",
      "  episode_len_mean: 222.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.3840000074356794\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 4002\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.0912742643129256\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010970375879851677\n",
      "          policy_loss: -0.11506514662704874\n",
      "          total_loss: -0.10280887863683304\n",
      "          vf_explained_var: -0.215839222073555\n",
      "          vf_loss: 0.0022390546222422494\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.9320480720627875\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010411019494294169\n",
      "          policy_loss: -0.11377596996802215\n",
      "          total_loss: -0.10323546839936171\n",
      "          vf_explained_var: 0.15356048941612244\n",
      "          vf_loss: 0.002663444432289994\n",
      "    num_agent_steps_sampled: 1825600\n",
      "    num_agent_steps_trained: 1825600\n",
      "    num_steps_sampled: 912800\n",
      "    num_steps_trained: 912800\n",
      "  iterations_since_restore: 326\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.104000000000001\n",
      "    gpu_util_percent0: 0.0616\n",
      "    ram_util_percent: 59.86\n",
      "    vram_util_percent0: 0.24817968750000002\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.006999999582767487\n",
      "    agent_1: 0.39100000701844695\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05176102856733466\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.905897456570804\n",
      "    mean_inference_ms: 1.7835430448534333\n",
      "    mean_raw_obs_processing_ms: 0.16554988462630635\n",
      "  time_since_restore: 6090.166877508163\n",
      "  time_this_iter_s: 18.741227626800537\n",
      "  time_total_s: 6090.166877508163\n",
      "  timers:\n",
      "    learn_throughput: 664.745\n",
      "    learn_time_ms: 4212.143\n",
      "    load_throughput: 96187.736\n",
      "    load_time_ms: 29.11\n",
      "    sample_throughput: 192.914\n",
      "    sample_time_ms: 14514.217\n",
      "    update_time_ms: 3.323\n",
      "  timestamp: 1658501018\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 912800\n",
      "  training_iteration: 326\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   326</td><td style=\"text-align: right;\">         6090.17</td><td style=\"text-align: right;\">912800</td><td style=\"text-align: right;\">   0.384</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            222.25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1831200\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-43-57\n",
      "  done: false\n",
      "  episode_len_mean: 214.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.42600000761449336\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 4015\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.1336471239725747\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.008666553729868444\n",
      "          policy_loss: -0.06203502085554646\n",
      "          total_loss: -0.050406534043196144\n",
      "          vf_explained_var: -0.04781840369105339\n",
      "          vf_loss: 0.007895973683237875\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.985896953514644\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010333943518957313\n",
      "          policy_loss: -0.07058108049291366\n",
      "          total_loss: -0.059579335065878\n",
      "          vf_explained_var: -0.15487541258335114\n",
      "          vf_loss: 0.004264102035882561\n",
      "    num_agent_steps_sampled: 1831200\n",
      "    num_agent_steps_trained: 1831200\n",
      "    num_steps_sampled: 915600\n",
      "    num_steps_trained: 915600\n",
      "  iterations_since_restore: 327\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.4\n",
      "    gpu_util_percent0: 0.068\n",
      "    ram_util_percent: 60.048\n",
      "    vram_util_percent0: 0.25317968750000003\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: 0.010000000298023224\n",
      "    agent_1: 0.41600000731647013\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05176219158913426\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.905972611819714\n",
      "    mean_inference_ms: 1.7836203321791348\n",
      "    mean_raw_obs_processing_ms: 0.16557485839035457\n",
      "  time_since_restore: 6109.524813890457\n",
      "  time_this_iter_s: 19.3579363822937\n",
      "  time_total_s: 6109.524813890457\n",
      "  timers:\n",
      "    learn_throughput: 658.262\n",
      "    learn_time_ms: 4253.625\n",
      "    load_throughput: 96217.051\n",
      "    load_time_ms: 29.101\n",
      "    sample_throughput: 192.506\n",
      "    sample_time_ms: 14545.037\n",
      "    update_time_ms: 3.331\n",
      "  timestamp: 1658501037\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 915600\n",
      "  training_iteration: 327\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   327</td><td style=\"text-align: right;\">         6109.52</td><td style=\"text-align: right;\">915600</td><td style=\"text-align: right;\">   0.426</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            214.47</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1836800\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-44-17\n",
      "  done: false\n",
      "  episode_len_mean: 215.21\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.4330000077188015\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 4029\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.103493789831797\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011103184535941755\n",
      "          policy_loss: -0.07633603975171441\n",
      "          total_loss: -0.06340553370509519\n",
      "          vf_explained_var: -0.07025698572397232\n",
      "          vf_loss: 0.003751779865394513\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.759666618491922\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01235776349137123\n",
      "          policy_loss: -0.060290866338792035\n",
      "          total_loss: -0.04704261814064618\n",
      "          vf_explained_var: 0.28525128960609436\n",
      "          vf_loss: 0.004631194807925134\n",
      "    num_agent_steps_sampled: 1836800\n",
      "    num_agent_steps_trained: 1836800\n",
      "    num_steps_sampled: 918400\n",
      "    num_steps_trained: 918400\n",
      "  iterations_since_restore: 328\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 7.507999999999999\n",
      "    gpu_util_percent0: 0.0812\n",
      "    ram_util_percent: 60.443999999999996\n",
      "    vram_util_percent0: 0.26816796875\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: 0.006000000238418579\n",
      "    agent_1: 0.4270000074803829\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05176375305286615\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.906064230726231\n",
      "    mean_inference_ms: 1.7837190057108125\n",
      "    mean_raw_obs_processing_ms: 0.16560393382282726\n",
      "  time_since_restore: 6128.891359567642\n",
      "  time_this_iter_s: 19.36654567718506\n",
      "  time_total_s: 6128.891359567642\n",
      "  timers:\n",
      "    learn_throughput: 655.183\n",
      "    learn_time_ms: 4273.615\n",
      "    load_throughput: 95645.02\n",
      "    load_time_ms: 29.275\n",
      "    sample_throughput: 192.171\n",
      "    sample_time_ms: 14570.332\n",
      "    update_time_ms: 3.311\n",
      "  timestamp: 1658501057\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 918400\n",
      "  training_iteration: 328\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   328</td><td style=\"text-align: right;\">         6128.89</td><td style=\"text-align: right;\">918400</td><td style=\"text-align: right;\">   0.433</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            215.21</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1842400\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-44-36\n",
      "  done: false\n",
      "  episode_len_mean: 221.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.4150000074505806\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 4042\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.1006308311507818\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01436222220019147\n",
      "          policy_loss: -0.09150555061045279\n",
      "          total_loss: -0.07556596602461511\n",
      "          vf_explained_var: -0.11556098610162735\n",
      "          vf_loss: 0.0018718818865593114\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.9184717301811491\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01565714563677349\n",
      "          policy_loss: -0.10745737261493646\n",
      "          total_loss: -0.09212197158748972\n",
      "          vf_explained_var: 0.25201886892318726\n",
      "          vf_loss: 0.0013592246465296263\n",
      "    num_agent_steps_sampled: 1842400\n",
      "    num_agent_steps_trained: 1842400\n",
      "    num_steps_sampled: 921200\n",
      "    num_steps_trained: 921200\n",
      "  iterations_since_restore: 329\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.865384615384617\n",
      "    gpu_util_percent0: 0.07576923076923077\n",
      "    ram_util_percent: 61.07692307692308\n",
      "    vram_util_percent0: 0.2704965444711538\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.01\n",
      "    agent_1: 0.4250000074505806\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051765778914467084\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.9061809605046007\n",
      "    mean_inference_ms: 1.783838213206174\n",
      "    mean_raw_obs_processing_ms: 0.1656258875749457\n",
      "  time_since_restore: 6148.201090812683\n",
      "  time_this_iter_s: 19.309731245040894\n",
      "  time_total_s: 6148.201090812683\n",
      "  timers:\n",
      "    learn_throughput: 655.299\n",
      "    learn_time_ms: 4272.86\n",
      "    load_throughput: 95172.984\n",
      "    load_time_ms: 29.42\n",
      "    sample_throughput: 191.869\n",
      "    sample_time_ms: 14593.288\n",
      "    update_time_ms: 3.307\n",
      "  timestamp: 1658501076\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 921200\n",
      "  training_iteration: 329\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   329</td><td style=\"text-align: right;\">          6148.2</td><td style=\"text-align: right;\">921200</td><td style=\"text-align: right;\">   0.415</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            221.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1848000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-44-55\n",
      "  done: false\n",
      "  episode_len_mean: 226.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.3850000077486038\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 4054\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.069044847573553\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01149071677671207\n",
      "          policy_loss: -0.03744142703397388\n",
      "          total_loss: -0.023735167193080997\n",
      "          vf_explained_var: -0.30093997716903687\n",
      "          vf_loss: 0.004680841275482332\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.8775764714394296\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010464637156028907\n",
      "          policy_loss: -0.060386870174538455\n",
      "          total_loss: -0.04913870451223158\n",
      "          vf_explained_var: 0.10886885970830917\n",
      "          vf_loss: 0.004465855509795888\n",
      "    num_agent_steps_sampled: 1848000\n",
      "    num_agent_steps_trained: 1848000\n",
      "    num_steps_sampled: 924000\n",
      "    num_steps_trained: 924000\n",
      "  iterations_since_restore: 330\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.343999999999999\n",
      "    gpu_util_percent0: 0.062\n",
      "    ram_util_percent: 61.11600000000001\n",
      "    vram_util_percent0: 0.27631640625\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.01899999976158142\n",
      "    agent_1: 0.4040000075101852\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05176782317962535\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.906284463215189\n",
      "    mean_inference_ms: 1.7839549829717316\n",
      "    mean_raw_obs_processing_ms: 0.16564027198162537\n",
      "  time_since_restore: 6167.2336349487305\n",
      "  time_this_iter_s: 19.032544136047363\n",
      "  time_total_s: 6167.2336349487305\n",
      "  timers:\n",
      "    learn_throughput: 653.0\n",
      "    learn_time_ms: 4287.903\n",
      "    load_throughput: 95268.332\n",
      "    load_time_ms: 29.391\n",
      "    sample_throughput: 191.7\n",
      "    sample_time_ms: 14606.138\n",
      "    update_time_ms: 3.326\n",
      "  timestamp: 1658501095\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 924000\n",
      "  training_iteration: 330\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   330</td><td style=\"text-align: right;\">         6167.23</td><td style=\"text-align: right;\">924000</td><td style=\"text-align: right;\">   0.385</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            226.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1853600\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-45-14\n",
      "  done: false\n",
      "  episode_len_mean: 215.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.46000000812113284\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 4069\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.9952703455374354\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.007049958433380753\n",
      "          policy_loss: 0.020136529000453828\n",
      "          total_loss: 0.032019574642098245\n",
      "          vf_explained_var: -0.5326237678527832\n",
      "          vf_loss: 0.013650079024782394\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.9044694020634605\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.008944869162212996\n",
      "          policy_loss: 0.0031751472193891892\n",
      "          total_loss: 0.013648310790033844\n",
      "          vf_explained_var: -0.047080740332603455\n",
      "          vf_loss: 0.006623842828828076\n",
      "    num_agent_steps_sampled: 1853600\n",
      "    num_agent_steps_trained: 1853600\n",
      "    num_steps_sampled: 926800\n",
      "    num_steps_trained: 926800\n",
      "  iterations_since_restore: 331\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 7.16\n",
      "    gpu_util_percent0: 0.062400000000000004\n",
      "    ram_util_percent: 61.20400000000001\n",
      "    vram_util_percent0: 0.28275390625\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: 0.03100000023841858\n",
      "    agent_1: 0.42900000788271425\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05177152957872067\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.9064611212352\n",
      "    mean_inference_ms: 1.7841573839846716\n",
      "    mean_raw_obs_processing_ms: 0.16567580563240714\n",
      "  time_since_restore: 6186.6520228385925\n",
      "  time_this_iter_s: 19.41838788986206\n",
      "  time_total_s: 6186.6520228385925\n",
      "  timers:\n",
      "    learn_throughput: 651.162\n",
      "    learn_time_ms: 4300.007\n",
      "    load_throughput: 94394.174\n",
      "    load_time_ms: 29.663\n",
      "    sample_throughput: 191.392\n",
      "    sample_time_ms: 14629.67\n",
      "    update_time_ms: 3.306\n",
      "  timestamp: 1658501114\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 926800\n",
      "  training_iteration: 331\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   331</td><td style=\"text-align: right;\">         6186.65</td><td style=\"text-align: right;\">926800</td><td style=\"text-align: right;\">    0.46</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             215.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1859200\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-45-34\n",
      "  done: false\n",
      "  episode_len_mean: 219.92\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.420000007674098\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 4080\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.132627984952359\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.014138323556320028\n",
      "          policy_loss: -0.08454608071631464\n",
      "          total_loss: -0.06882971952983921\n",
      "          vf_explained_var: -0.016468729823827744\n",
      "          vf_loss: 0.001991698785873485\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.9340682725111644\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.015825276567178786\n",
      "          policy_loss: -0.07571336076355129\n",
      "          total_loss: -0.06011377167665376\n",
      "          vf_explained_var: 0.2211473286151886\n",
      "          vf_loss: 0.0016519525058919541\n",
      "    num_agent_steps_sampled: 1859200\n",
      "    num_agent_steps_trained: 1859200\n",
      "    num_steps_sampled: 929600\n",
      "    num_steps_trained: 929600\n",
      "  iterations_since_restore: 332\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 7.16\n",
      "    gpu_util_percent0: 0.07519999999999999\n",
      "    ram_util_percent: 61.2\n",
      "    vram_util_percent0: 0.28429296875\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: 0.0010000002384185792\n",
      "    agent_1: 0.41900000743567944\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051774799123648574\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.90661573710463\n",
      "    mean_inference_ms: 1.7843341857121084\n",
      "    mean_raw_obs_processing_ms: 0.16570093018621165\n",
      "  time_since_restore: 6205.915904283524\n",
      "  time_this_iter_s: 19.26388144493103\n",
      "  time_total_s: 6205.915904283524\n",
      "  timers:\n",
      "    learn_throughput: 648.862\n",
      "    learn_time_ms: 4315.247\n",
      "    load_throughput: 93891.042\n",
      "    load_time_ms: 29.822\n",
      "    sample_throughput: 191.212\n",
      "    sample_time_ms: 14643.432\n",
      "    update_time_ms: 3.256\n",
      "  timestamp: 1658501134\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 929600\n",
      "  training_iteration: 332\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   332</td><td style=\"text-align: right;\">         6205.92</td><td style=\"text-align: right;\">929600</td><td style=\"text-align: right;\">    0.42</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            219.92</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1864800\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-45-53\n",
      "  done: false\n",
      "  episode_len_mean: 216.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.41600000761449335\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 4093\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.006630158140546\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011750635749058583\n",
      "          policy_loss: -0.07789262933129933\n",
      "          total_loss: -0.06468488923668961\n",
      "          vf_explained_var: -0.08628026396036148\n",
      "          vf_loss: 0.0023488911155298063\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.8562137849983715\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.012258883529362705\n",
      "          policy_loss: -0.0663964972996577\n",
      "          total_loss: -0.053591649007319664\n",
      "          vf_explained_var: 0.22068364918231964\n",
      "          vf_loss: 0.0037623302272376826\n",
      "    num_agent_steps_sampled: 1864800\n",
      "    num_agent_steps_trained: 1864800\n",
      "    num_steps_sampled: 932400\n",
      "    num_steps_trained: 932400\n",
      "  iterations_since_restore: 333\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.604000000000001\n",
      "    gpu_util_percent0: 0.0656\n",
      "    ram_util_percent: 61.232\n",
      "    vram_util_percent0: 0.283828125\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: 0.0010000002384185792\n",
      "    agent_1: 0.4150000073760748\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051778569095742355\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.906796320310381\n",
      "    mean_inference_ms: 1.7845395231108272\n",
      "    mean_raw_obs_processing_ms: 0.16573361376870682\n",
      "  time_since_restore: 6224.88715171814\n",
      "  time_this_iter_s: 18.97124743461609\n",
      "  time_total_s: 6224.88715171814\n",
      "  timers:\n",
      "    learn_throughput: 647.718\n",
      "    learn_time_ms: 4322.868\n",
      "    load_throughput: 93334.609\n",
      "    load_time_ms: 30.0\n",
      "    sample_throughput: 190.592\n",
      "    sample_time_ms: 14691.106\n",
      "    update_time_ms: 3.287\n",
      "  timestamp: 1658501153\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 932400\n",
      "  training_iteration: 333\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   333</td><td style=\"text-align: right;\">         6224.89</td><td style=\"text-align: right;\">932400</td><td style=\"text-align: right;\">   0.416</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            216.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1870400\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-46-13\n",
      "  done: false\n",
      "  episode_len_mean: 215.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.3660000074654818\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 4107\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.984524833659331\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.009541937610716538\n",
      "          policy_loss: -0.047070143995543254\n",
      "          total_loss: -0.03506903255748468\n",
      "          vf_explained_var: -0.24543550610542297\n",
      "          vf_loss: 0.005971357435988639\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.8220412873086476\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010722624366312222\n",
      "          policy_loss: -0.04160141914811296\n",
      "          total_loss: -0.028984133989373124\n",
      "          vf_explained_var: 0.03320875018835068\n",
      "          vf_loss: 0.007571837497726804\n",
      "    num_agent_steps_sampled: 1870400\n",
      "    num_agent_steps_trained: 1870400\n",
      "    num_steps_sampled: 935200\n",
      "    num_steps_trained: 935200\n",
      "  iterations_since_restore: 334\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.392307692307693\n",
      "    gpu_util_percent0: 0.10346153846153847\n",
      "    ram_util_percent: 61.9076923076923\n",
      "    vram_util_percent0: 0.29863656850961545\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.01899999976158142\n",
      "    agent_1: 0.3850000072270632\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05178301056274408\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.9070201236993016\n",
      "    mean_inference_ms: 1.7847818138496578\n",
      "    mean_raw_obs_processing_ms: 0.1657745677318967\n",
      "  time_since_restore: 6244.717292070389\n",
      "  time_this_iter_s: 19.830140352249146\n",
      "  time_total_s: 6244.717292070389\n",
      "  timers:\n",
      "    learn_throughput: 638.655\n",
      "    learn_time_ms: 4384.211\n",
      "    load_throughput: 92897.98\n",
      "    load_time_ms: 30.141\n",
      "    sample_throughput: 189.939\n",
      "    sample_time_ms: 14741.571\n",
      "    update_time_ms: 3.267\n",
      "  timestamp: 1658501173\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 935200\n",
      "  training_iteration: 334\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   334</td><td style=\"text-align: right;\">         6244.72</td><td style=\"text-align: right;\">935200</td><td style=\"text-align: right;\">   0.366</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            215.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1876000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-46-32\n",
      "  done: false\n",
      "  episode_len_mean: 195.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.366000007763505\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4127\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.9923306551007998\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.007287766421365262\n",
      "          policy_loss: -0.06219902491163035\n",
      "          total_loss: -0.050146333036190344\n",
      "          vf_explained_var: -0.39123785495758057\n",
      "          vf_loss: 0.013367143782012309\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.759015091473148\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.008545189756034223\n",
      "          policy_loss: -0.06413067196366942\n",
      "          total_loss: -0.053226969844607605\n",
      "          vf_explained_var: 0.26683488488197327\n",
      "          vf_loss: 0.008821153572041242\n",
      "    num_agent_steps_sampled: 1876000\n",
      "    num_agent_steps_trained: 1876000\n",
      "    num_steps_sampled: 938000\n",
      "    num_steps_trained: 938000\n",
      "  iterations_since_restore: 335\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 7.488461538461538\n",
      "    gpu_util_percent0: 0.10461538461538462\n",
      "    ram_util_percent: 61.79230769230769\n",
      "    vram_util_percent0: 0.3063063401442308\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.012999999523162841\n",
      "    agent_1: 0.37900000728666783\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05178907288242836\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.907345176929273\n",
      "    mean_inference_ms: 1.785138804465723\n",
      "    mean_raw_obs_processing_ms: 0.1658576661840495\n",
      "  time_since_restore: 6264.411328077316\n",
      "  time_this_iter_s: 19.69403600692749\n",
      "  time_total_s: 6264.411328077316\n",
      "  timers:\n",
      "    learn_throughput: 634.458\n",
      "    learn_time_ms: 4413.216\n",
      "    load_throughput: 92924.589\n",
      "    load_time_ms: 30.132\n",
      "    sample_throughput: 189.177\n",
      "    sample_time_ms: 14800.971\n",
      "    update_time_ms: 3.241\n",
      "  timestamp: 1658501192\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 938000\n",
      "  training_iteration: 335\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   335</td><td style=\"text-align: right;\">         6264.41</td><td style=\"text-align: right;\">938000</td><td style=\"text-align: right;\">   0.366</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            195.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1881600\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-46-51\n",
      "  done: false\n",
      "  episode_len_mean: 189.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.35300000868737696\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 4144\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.0495192000553724\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01059877971263022\n",
      "          policy_loss: -0.06695341550567675\n",
      "          total_loss: -0.04822661334661201\n",
      "          vf_explained_var: -0.19698742032051086\n",
      "          vf_loss: 0.02185408532906357\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.8487601397292954\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011562626607969034\n",
      "          policy_loss: -0.058398263660603245\n",
      "          total_loss: -0.04313292450387962\n",
      "          vf_explained_var: 0.2556264102458954\n",
      "          vf_loss: 0.012764938510025829\n",
      "    num_agent_steps_sampled: 1881600\n",
      "    num_agent_steps_trained: 1881600\n",
      "    num_steps_sampled: 940800\n",
      "    num_steps_trained: 940800\n",
      "  iterations_since_restore: 336\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.276\n",
      "    gpu_util_percent0: 0.0588\n",
      "    ram_util_percent: 61.356\n",
      "    vram_util_percent0: 0.30801953125000003\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.006999999284744263\n",
      "    agent_1: 0.3600000079721212\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051792836280772823\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.9075708464860486\n",
      "    mean_inference_ms: 1.7853811407397746\n",
      "    mean_raw_obs_processing_ms: 0.16593621928160884\n",
      "  time_since_restore: 6283.291296720505\n",
      "  time_this_iter_s: 18.879968643188477\n",
      "  time_total_s: 6283.291296720505\n",
      "  timers:\n",
      "    learn_throughput: 634.695\n",
      "    learn_time_ms: 4411.569\n",
      "    load_throughput: 92810.47\n",
      "    load_time_ms: 30.169\n",
      "    sample_throughput: 188.976\n",
      "    sample_time_ms: 14816.715\n",
      "    update_time_ms: 3.211\n",
      "  timestamp: 1658501211\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 940800\n",
      "  training_iteration: 336\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   336</td><td style=\"text-align: right;\">         6283.29</td><td style=\"text-align: right;\">940800</td><td style=\"text-align: right;\">   0.353</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            189.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1887200\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-47-10\n",
      "  done: false\n",
      "  episode_len_mean: 188.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.455000009611249\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 4159\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.0298064844239327\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011520398804426537\n",
      "          policy_loss: -0.09295276335927026\n",
      "          total_loss: -0.0752192519834504\n",
      "          vf_explained_var: -0.14203503727912903\n",
      "          vf_loss: 0.01603539774701598\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.817455808321635\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.015167917257378174\n",
      "          policy_loss: -0.10415617333325956\n",
      "          total_loss: -0.08728256874157869\n",
      "          vf_explained_var: 0.37015607953071594\n",
      "          vf_loss: 0.0070269347648025445\n",
      "    num_agent_steps_sampled: 1887200\n",
      "    num_agent_steps_trained: 1887200\n",
      "    num_steps_sampled: 943600\n",
      "    num_steps_trained: 943600\n",
      "  iterations_since_restore: 337\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.995833333333333\n",
      "    gpu_util_percent0: 0.05458333333333334\n",
      "    ram_util_percent: 61.17916666666667\n",
      "    vram_util_percent0: 0.30486653645833334\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: 0.049000000953674315\n",
      "    agent_1: 0.40600000865757463\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051794931854169904\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.9077434175213126\n",
      "    mean_inference_ms: 1.7855506416849858\n",
      "    mean_raw_obs_processing_ms: 0.1660070771504284\n",
      "  time_since_restore: 6301.907992601395\n",
      "  time_this_iter_s: 18.616695880889893\n",
      "  time_total_s: 6301.907992601395\n",
      "  timers:\n",
      "    learn_throughput: 640.037\n",
      "    learn_time_ms: 4374.744\n",
      "    load_throughput: 93001.12\n",
      "    load_time_ms: 30.107\n",
      "    sample_throughput: 189.448\n",
      "    sample_time_ms: 14779.749\n",
      "    update_time_ms: 3.218\n",
      "  timestamp: 1658501230\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 943600\n",
      "  training_iteration: 337\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   337</td><td style=\"text-align: right;\">         6301.91</td><td style=\"text-align: right;\">943600</td><td style=\"text-align: right;\">   0.455</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            188.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1892800\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-47-29\n",
      "  done: false\n",
      "  episode_len_mean: 189.95\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.34300000935792924\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 4172\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.077341221627735\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.014491935821173368\n",
      "          policy_loss: -0.08780031876328091\n",
      "          total_loss: -0.06833126935470361\n",
      "          vf_explained_var: 0.25485730171203613\n",
      "          vf_loss: 0.01150311375126226\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.9278177051317125\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.013453601149048922\n",
      "          policy_loss: -0.06625722709040496\n",
      "          total_loss: -0.05103935911347435\n",
      "          vf_explained_var: 0.22852477431297302\n",
      "          vf_loss: 0.007325184584173951\n",
      "    num_agent_steps_sampled: 1892800\n",
      "    num_agent_steps_trained: 1892800\n",
      "    num_steps_sampled: 946400\n",
      "    num_steps_trained: 946400\n",
      "  iterations_since_restore: 338\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.112000000000002\n",
      "    gpu_util_percent0: 0.06\n",
      "    ram_util_percent: 61.136\n",
      "    vram_util_percent0: 0.30518749999999994\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.01399999886751175\n",
      "    agent_1: 0.357000008225441\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05179586615969231\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.9078644266374023\n",
      "    mean_inference_ms: 1.785656771206078\n",
      "    mean_raw_obs_processing_ms: 0.16606332739888913\n",
      "  time_since_restore: 6320.703038692474\n",
      "  time_this_iter_s: 18.795046091079712\n",
      "  time_total_s: 6320.703038692474\n",
      "  timers:\n",
      "    learn_throughput: 640.701\n",
      "    learn_time_ms: 4370.211\n",
      "    load_throughput: 93432.105\n",
      "    load_time_ms: 29.968\n",
      "    sample_throughput: 190.127\n",
      "    sample_time_ms: 14726.975\n",
      "    update_time_ms: 3.25\n",
      "  timestamp: 1658501249\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 946400\n",
      "  training_iteration: 338\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   338</td><td style=\"text-align: right;\">          6320.7</td><td style=\"text-align: right;\">946400</td><td style=\"text-align: right;\">   0.343</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            189.95</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1898400\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-47-48\n",
      "  done: false\n",
      "  episode_len_mean: 188.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.34000000916421413\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 4182\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.043472787454015\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011732798372866823\n",
      "          policy_loss: -0.06939497974789924\n",
      "          total_loss: -0.05202751812647052\n",
      "          vf_explained_var: -0.11577526479959488\n",
      "          vf_loss: 0.014324545631617574\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.9614708001414936\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01392389323026897\n",
      "          policy_loss: -0.06662448688452319\n",
      "          total_loss: -0.050983984912731796\n",
      "          vf_explained_var: 0.15714772045612335\n",
      "          vf_loss: 0.007229088120982938\n",
      "    num_agent_steps_sampled: 1898400\n",
      "    num_agent_steps_trained: 1898400\n",
      "    num_steps_sampled: 949200\n",
      "    num_steps_trained: 949200\n",
      "  iterations_since_restore: 339\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.272\n",
      "    gpu_util_percent0: 0.061200000000000004\n",
      "    ram_util_percent: 61.028\n",
      "    vram_util_percent0: 0.2968828125\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.003999998867511749\n",
      "    agent_1: 0.34400000803172587\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05179624796517523\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.907941257205094\n",
      "    mean_inference_ms: 1.785722810665525\n",
      "    mean_raw_obs_processing_ms: 0.16610211777951445\n",
      "  time_since_restore: 6339.474891662598\n",
      "  time_this_iter_s: 18.77185297012329\n",
      "  time_total_s: 6339.474891662598\n",
      "  timers:\n",
      "    learn_throughput: 641.306\n",
      "    learn_time_ms: 4366.088\n",
      "    load_throughput: 93765.554\n",
      "    load_time_ms: 29.862\n",
      "    sample_throughput: 190.766\n",
      "    sample_time_ms: 14677.63\n",
      "    update_time_ms: 3.291\n",
      "  timestamp: 1658501268\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 949200\n",
      "  training_iteration: 339\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   339</td><td style=\"text-align: right;\">         6339.47</td><td style=\"text-align: right;\">949200</td><td style=\"text-align: right;\">    0.34</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            188.64</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1904000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-48-08\n",
      "  done: false\n",
      "  episode_len_mean: 192.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.3480000089854002\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 4195\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.0074002299280393\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.015128279872775091\n",
      "          policy_loss: -0.09338928279854979\n",
      "          total_loss: -0.07532074780448013\n",
      "          vf_explained_var: 0.18589670956134796\n",
      "          vf_loss: 0.005378457081492559\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.9393084042129063\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.013429074593347115\n",
      "          policy_loss: -0.07754142437313151\n",
      "          total_loss: -0.06273931181931403\n",
      "          vf_explained_var: -0.07620762288570404\n",
      "          vf_loss: 0.006222004783921875\n",
      "    num_agent_steps_sampled: 1904000\n",
      "    num_agent_steps_trained: 1904000\n",
      "    num_steps_sampled: 952000\n",
      "    num_steps_trained: 952000\n",
      "  iterations_since_restore: 340\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.657692307692308\n",
      "    gpu_util_percent0: 0.0726923076923077\n",
      "    ram_util_percent: 63.79615384615384\n",
      "    vram_util_percent0: 0.2838604266826923\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: 0.006000001132488251\n",
      "    agent_1: 0.34200000785291196\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05179869441871197\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.9081106774982803\n",
      "    mean_inference_ms: 1.7858985590050507\n",
      "    mean_raw_obs_processing_ms: 0.1661610489301381\n",
      "  time_since_restore: 6359.712076187134\n",
      "  time_this_iter_s: 20.237184524536133\n",
      "  time_total_s: 6359.712076187134\n",
      "  timers:\n",
      "    learn_throughput: 641.882\n",
      "    learn_time_ms: 4362.173\n",
      "    load_throughput: 85940.99\n",
      "    load_time_ms: 32.58\n",
      "    sample_throughput: 189.198\n",
      "    sample_time_ms: 14799.35\n",
      "    update_time_ms: 3.258\n",
      "  timestamp: 1658501288\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 952000\n",
      "  training_iteration: 340\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   340</td><td style=\"text-align: right;\">         6359.71</td><td style=\"text-align: right;\">952000</td><td style=\"text-align: right;\">   0.348</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            192.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1909600\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-48-27\n",
      "  done: false\n",
      "  episode_len_mean: 190.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.34000000931322577\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 4210\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.0201906969859484\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011267952129459984\n",
      "          policy_loss: -0.11233789993760486\n",
      "          total_loss: -0.0979316049211082\n",
      "          vf_explained_var: 0.2464640885591507\n",
      "          vf_loss: 0.007336601877752747\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.8125421234539576\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010058174773800454\n",
      "          policy_loss: -0.09796690352197315\n",
      "          total_loss: -0.08636842729278474\n",
      "          vf_explained_var: 0.2913033962249756\n",
      "          vf_loss: 0.006548985525157713\n",
      "    num_agent_steps_sampled: 1909600\n",
      "    num_agent_steps_trained: 1909600\n",
      "    num_steps_sampled: 954800\n",
      "    num_steps_trained: 954800\n",
      "  iterations_since_restore: 341\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.94\n",
      "    gpu_util_percent0: 0.0496\n",
      "    ram_util_percent: 64.108\n",
      "    vram_util_percent0: 0.2858125\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: 0.00200000137090683\n",
      "    agent_1: 0.33800000794231894\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05180059695175772\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.908285936312934\n",
      "    mean_inference_ms: 1.786046066651012\n",
      "    mean_raw_obs_processing_ms: 0.16622548064177411\n",
      "  time_since_restore: 6378.695095777512\n",
      "  time_this_iter_s: 18.983019590377808\n",
      "  time_total_s: 6378.695095777512\n",
      "  timers:\n",
      "    learn_throughput: 641.823\n",
      "    learn_time_ms: 4362.575\n",
      "    load_throughput: 86777.442\n",
      "    load_time_ms: 32.266\n",
      "    sample_throughput: 189.758\n",
      "    sample_time_ms: 14755.658\n",
      "    update_time_ms: 3.266\n",
      "  timestamp: 1658501307\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 954800\n",
      "  training_iteration: 341\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   341</td><td style=\"text-align: right;\">          6378.7</td><td style=\"text-align: right;\">954800</td><td style=\"text-align: right;\">    0.34</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            190.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1915200\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-48-46\n",
      "  done: false\n",
      "  episode_len_mean: 199.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.30600000962615015\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 4226\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.9091727840048927\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.0086837233116478\n",
      "          policy_loss: -0.054103304961851485\n",
      "          total_loss: -0.03930604645046558\n",
      "          vf_explained_var: 0.16078971326351166\n",
      "          vf_loss: 0.016620128255453892\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.6885730805141586\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010111120602589578\n",
      "          policy_loss: -0.0698755798075581\n",
      "          total_loss: -0.057126878444684\n",
      "          vf_explained_var: 0.354442834854126\n",
      "          vf_loss: 0.009534169984233034\n",
      "    num_agent_steps_sampled: 1915200\n",
      "    num_agent_steps_trained: 1915200\n",
      "    num_steps_sampled: 957600\n",
      "    num_steps_trained: 957600\n",
      "  iterations_since_restore: 342\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.667999999999999\n",
      "    gpu_util_percent0: 0.06480000000000001\n",
      "    ram_util_percent: 62.948\n",
      "    vram_util_percent0: 0.27298046875\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.016999998390674592\n",
      "    agent_1: 0.3230000080168247\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05180203226629577\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.9084419990724455\n",
      "    mean_inference_ms: 1.7861662001551302\n",
      "    mean_raw_obs_processing_ms: 0.16628429758342056\n",
      "  time_since_restore: 6397.635099649429\n",
      "  time_this_iter_s: 18.940003871917725\n",
      "  time_total_s: 6397.635099649429\n",
      "  timers:\n",
      "    learn_throughput: 642.207\n",
      "    learn_time_ms: 4359.962\n",
      "    load_throughput: 87155.404\n",
      "    load_time_ms: 32.127\n",
      "    sample_throughput: 190.144\n",
      "    sample_time_ms: 14725.712\n",
      "    update_time_ms: 3.297\n",
      "  timestamp: 1658501326\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 957600\n",
      "  training_iteration: 342\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   342</td><td style=\"text-align: right;\">         6397.64</td><td style=\"text-align: right;\">957600</td><td style=\"text-align: right;\">   0.306</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            199.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1920800\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-49-05\n",
      "  done: false\n",
      "  episode_len_mean: 199.18\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000059604645\n",
      "  episode_reward_mean: 0.29700000867247583\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 4241\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.0724159993586087\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.013028233260162432\n",
      "          policy_loss: -0.10600582006814616\n",
      "          total_loss: -0.0904545651170511\n",
      "          vf_explained_var: 0.28514665365219116\n",
      "          vf_loss: 0.0050139445078717215\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.9159853898343586\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.014196911568380158\n",
      "          policy_loss: -0.09335365561214747\n",
      "          total_loss: -0.07839567192699871\n",
      "          vf_explained_var: 0.2425592988729477\n",
      "          vf_loss: 0.004447351505968115\n",
      "    num_agent_steps_sampled: 1920800\n",
      "    num_agent_steps_trained: 1920800\n",
      "    num_steps_sampled: 960400\n",
      "    num_steps_trained: 960400\n",
      "  iterations_since_restore: 343\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 7.396\n",
      "    gpu_util_percent0: 0.09480000000000001\n",
      "    ram_util_percent: 62.724000000000004\n",
      "    vram_util_percent0: 0.28119921875\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.0000000596046448\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.03299999862909317\n",
      "    agent_1: 0.330000007301569\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05180372950772631\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.9086067112489618\n",
      "    mean_inference_ms: 1.7862987892401747\n",
      "    mean_raw_obs_processing_ms: 0.16633584508888954\n",
      "  time_since_restore: 6416.851216077805\n",
      "  time_this_iter_s: 19.216116428375244\n",
      "  time_total_s: 6416.851216077805\n",
      "  timers:\n",
      "    learn_throughput: 640.135\n",
      "    learn_time_ms: 4374.078\n",
      "    load_throughput: 87578.254\n",
      "    load_time_ms: 31.971\n",
      "    sample_throughput: 189.992\n",
      "    sample_time_ms: 14737.484\n",
      "    update_time_ms: 3.293\n",
      "  timestamp: 1658501345\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 960400\n",
      "  training_iteration: 343\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   343</td><td style=\"text-align: right;\">         6416.85</td><td style=\"text-align: right;\">960400</td><td style=\"text-align: right;\">   0.297</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            199.18</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1926400\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-49-24\n",
      "  done: false\n",
      "  episode_len_mean: 196.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000037252903\n",
      "  episode_reward_mean: 0.2260000080615282\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 4259\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.0166762279612676\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011598027635943066\n",
      "          policy_loss: -0.06775504866337065\n",
      "          total_loss: -0.05360535295176305\n",
      "          vf_explained_var: 0.27167025208473206\n",
      "          vf_loss: 0.005539900242833288\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.77254813217691\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010988648408748627\n",
      "          policy_loss: -0.07789379352536907\n",
      "          total_loss: -0.06626809330815117\n",
      "          vf_explained_var: 0.2387894243001938\n",
      "          vf_loss: 0.003923024128328377\n",
      "    num_agent_steps_sampled: 1926400\n",
      "    num_agent_steps_trained: 1926400\n",
      "    num_steps_sampled: 963200\n",
      "    num_steps_trained: 963200\n",
      "  iterations_since_restore: 344\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.723999999999998\n",
      "    gpu_util_percent0: 0.0592\n",
      "    ram_util_percent: 61.711999999999996\n",
      "    vram_util_percent0: 0.2699453125\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.000000037252903\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.07599999859929085\n",
      "    agent_1: 0.30200000666081905\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05180596957159578\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.9088135854995483\n",
      "    mean_inference_ms: 1.7864653705357108\n",
      "    mean_raw_obs_processing_ms: 0.16640960960773854\n",
      "  time_since_restore: 6435.949028491974\n",
      "  time_this_iter_s: 19.09781241416931\n",
      "  time_total_s: 6435.949028491974\n",
      "  timers:\n",
      "    learn_throughput: 644.925\n",
      "    learn_time_ms: 4341.588\n",
      "    load_throughput: 86936.045\n",
      "    load_time_ms: 32.208\n",
      "    sample_throughput: 190.524\n",
      "    sample_time_ms: 14696.306\n",
      "    update_time_ms: 3.316\n",
      "  timestamp: 1658501364\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 963200\n",
      "  training_iteration: 344\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   344</td><td style=\"text-align: right;\">         6435.95</td><td style=\"text-align: right;\">963200</td><td style=\"text-align: right;\">   0.226</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            196.31</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1932000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-49-43\n",
      "  done: false\n",
      "  episode_len_mean: 188.02\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000037252903\n",
      "  episode_reward_mean: 0.25600000835955145\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 4274\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.9344572989003999\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011921712808120705\n",
      "          policy_loss: -0.08245671575265251\n",
      "          total_loss: -0.068204102819263\n",
      "          vf_explained_var: 0.20856600999832153\n",
      "          vf_loss: 0.004696550751326694\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.6720389821344899\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.012815760246279189\n",
      "          policy_loss: -0.0885994647389799\n",
      "          total_loss: -0.07525258148477103\n",
      "          vf_explained_var: 0.35089707374572754\n",
      "          vf_loss: 0.003501309346618308\n",
      "    num_agent_steps_sampled: 1932000\n",
      "    num_agent_steps_trained: 1932000\n",
      "    num_steps_sampled: 966000\n",
      "    num_steps_trained: 966000\n",
      "  iterations_since_restore: 345\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.316\n",
      "    gpu_util_percent0: 0.0704\n",
      "    ram_util_percent: 61.66799999999999\n",
      "    vram_util_percent0: 0.2689765625\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.000000037252903\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.06599999830126763\n",
      "    agent_1: 0.32200000666081907\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051807803692761285\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.9089766251889313\n",
      "    mean_inference_ms: 1.7866049307567835\n",
      "    mean_raw_obs_processing_ms: 0.166475683551739\n",
      "  time_since_restore: 6454.623186349869\n",
      "  time_this_iter_s: 18.674157857894897\n",
      "  time_total_s: 6454.623186349869\n",
      "  timers:\n",
      "    learn_throughput: 649.32\n",
      "    learn_time_ms: 4312.202\n",
      "    load_throughput: 87131.802\n",
      "    load_time_ms: 32.135\n",
      "    sample_throughput: 191.469\n",
      "    sample_time_ms: 14623.792\n",
      "    update_time_ms: 3.325\n",
      "  timestamp: 1658501383\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 966000\n",
      "  training_iteration: 345\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   345</td><td style=\"text-align: right;\">         6454.62</td><td style=\"text-align: right;\">966000</td><td style=\"text-align: right;\">   0.256</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            188.02</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1937600\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-50-01\n",
      "  done: false\n",
      "  episode_len_mean: 184.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000037252903\n",
      "  episode_reward_mean: 0.2980000085383654\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 4287\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.9893505420713198\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.008575873557279824\n",
      "          policy_loss: -0.04874325090245942\n",
      "          total_loss: -0.0364478839148346\n",
      "          vf_explained_var: -0.014197953045368195\n",
      "          vf_loss: 0.009919609599732905\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.88152971579915\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.009584197097470075\n",
      "          policy_loss: -0.060482479571960755\n",
      "          total_loss: -0.04946373742183953\n",
      "          vf_explained_var: 0.12667226791381836\n",
      "          vf_loss: 0.006328987090195629\n",
      "    num_agent_steps_sampled: 1937600\n",
      "    num_agent_steps_trained: 1937600\n",
      "    num_steps_sampled: 968800\n",
      "    num_steps_trained: 968800\n",
      "  iterations_since_restore: 346\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.020833333333333\n",
      "    gpu_util_percent0: 0.05666666666666667\n",
      "    ram_util_percent: 61.612500000000004\n",
      "    vram_util_percent0: 0.2661214192708334\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.000000037252903\n",
      "  policy_reward_mean:\n",
      "    agent_0: -0.04599999830126762\n",
      "    agent_1: 0.344000006839633\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051808061151350254\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.909072144762872\n",
      "    mean_inference_ms: 1.7866600286709151\n",
      "    mean_raw_obs_processing_ms: 0.16653346046707226\n",
      "  time_since_restore: 6473.153195381165\n",
      "  time_this_iter_s: 18.530009031295776\n",
      "  time_total_s: 6473.153195381165\n",
      "  timers:\n",
      "    learn_throughput: 649.202\n",
      "    learn_time_ms: 4312.985\n",
      "    load_throughput: 81677.065\n",
      "    load_time_ms: 34.281\n",
      "    sample_throughput: 191.969\n",
      "    sample_time_ms: 14585.711\n",
      "    update_time_ms: 3.323\n",
      "  timestamp: 1658501401\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 968800\n",
      "  training_iteration: 346\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   346</td><td style=\"text-align: right;\">         6473.15</td><td style=\"text-align: right;\">968800</td><td style=\"text-align: right;\">   0.298</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             184.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1943200\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-50-21\n",
      "  done: false\n",
      "  episode_len_mean: 181.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000037252903\n",
      "  episode_reward_mean: 0.3880000089854002\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 4305\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.9638248603968393\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.007505150100357445\n",
      "          policy_loss: -0.03163324564645466\n",
      "          total_loss: -0.020317467312062427\n",
      "          vf_explained_var: -0.11887654662132263\n",
      "          vf_loss: 0.010531537549554958\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.8068400376609393\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010646948800101847\n",
      "          policy_loss: -0.06756124075175085\n",
      "          total_loss: -0.056268032518259826\n",
      "          vf_explained_var: 0.37455153465270996\n",
      "          vf_loss: 0.003990024854785935\n",
      "    num_agent_steps_sampled: 1943200\n",
      "    num_agent_steps_trained: 1943200\n",
      "    num_steps_sampled: 971600\n",
      "    num_steps_trained: 971600\n",
      "  iterations_since_restore: 347\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 7.188\n",
      "    gpu_util_percent0: 0.06240000000000001\n",
      "    ram_util_percent: 61.832\n",
      "    vram_util_percent0: 0.27048828125\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.000000037252903\n",
      "  policy_reward_mean:\n",
      "    agent_0: 1.9371509552001954e-09\n",
      "    agent_1: 0.3880000070482492\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05180680492388621\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.9091662806880305\n",
      "    mean_inference_ms: 1.786677100286617\n",
      "    mean_raw_obs_processing_ms: 0.16661970649815444\n",
      "  time_since_restore: 6492.311766386032\n",
      "  time_this_iter_s: 19.158571004867554\n",
      "  time_total_s: 6492.311766386032\n",
      "  timers:\n",
      "    learn_throughput: 648.923\n",
      "    learn_time_ms: 4314.843\n",
      "    load_throughput: 81558.007\n",
      "    load_time_ms: 34.331\n",
      "    sample_throughput: 191.283\n",
      "    sample_time_ms: 14638.032\n",
      "    update_time_ms: 3.33\n",
      "  timestamp: 1658501421\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 971600\n",
      "  training_iteration: 347\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   347</td><td style=\"text-align: right;\">         6492.31</td><td style=\"text-align: right;\">971600</td><td style=\"text-align: right;\">   0.388</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            181.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1948800\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-50-39\n",
      "  done: false\n",
      "  episode_len_mean: 182.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000037252903\n",
      "  episode_reward_mean: 0.4440000094473362\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 4317\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.9884509362635159\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01180753611023286\n",
      "          policy_loss: -0.05601350558888606\n",
      "          total_loss: -0.039230181421901635\n",
      "          vf_explained_var: -0.30044084787368774\n",
      "          vf_loss: 0.012351680088960123\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.8398046937017214\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011989032735265118\n",
      "          policy_loss: -0.05984520348918176\n",
      "          total_loss: -0.04697694059004356\n",
      "          vf_explained_var: 0.03217628598213196\n",
      "          vf_loss: 0.004694213530023089\n",
      "    num_agent_steps_sampled: 1948800\n",
      "    num_agent_steps_trained: 1948800\n",
      "    num_steps_sampled: 974400\n",
      "    num_steps_trained: 974400\n",
      "  iterations_since_restore: 348\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.235999999999999\n",
      "    gpu_util_percent0: 0.0636\n",
      "    ram_util_percent: 61.52799999999999\n",
      "    vram_util_percent0: 0.26733593750000006\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.000000037252903\n",
      "  policy_reward_mean:\n",
      "    agent_0: 0.03700000241398811\n",
      "    agent_1: 0.4070000070333481\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051805854884276484\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.909224206306219\n",
      "    mean_inference_ms: 1.7866790572862283\n",
      "    mean_raw_obs_processing_ms: 0.16666801864813102\n",
      "  time_since_restore: 6510.917411327362\n",
      "  time_this_iter_s: 18.605644941329956\n",
      "  time_total_s: 6510.917411327362\n",
      "  timers:\n",
      "    learn_throughput: 649.101\n",
      "    learn_time_ms: 4313.655\n",
      "    load_throughput: 81546.001\n",
      "    load_time_ms: 34.336\n",
      "    sample_throughput: 191.513\n",
      "    sample_time_ms: 14620.441\n",
      "    update_time_ms: 3.314\n",
      "  timestamp: 1658501439\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 974400\n",
      "  training_iteration: 348\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   348</td><td style=\"text-align: right;\">         6510.92</td><td style=\"text-align: right;\">974400</td><td style=\"text-align: right;\">   0.444</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             182.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1954400\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-50-58\n",
      "  done: false\n",
      "  episode_len_mean: 188.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000014901161\n",
      "  episode_reward_mean: 0.4680000087618828\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 4330\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.0939042032474564\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.0146046517546175\n",
      "          policy_loss: -0.09964882459497035\n",
      "          total_loss: -0.08325039673460803\n",
      "          vf_explained_var: 0.10897552222013474\n",
      "          vf_loss: 0.0023951858456712216\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.8962846902154742\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.015160346955420931\n",
      "          policy_loss: -0.08058360196550243\n",
      "          total_loss: -0.06576667861788467\n",
      "          vf_explained_var: 0.32713380455970764\n",
      "          vf_loss: 0.0012709970989297809\n",
      "    num_agent_steps_sampled: 1954400\n",
      "    num_agent_steps_trained: 1954400\n",
      "    num_steps_sampled: 977200\n",
      "    num_steps_trained: 977200\n",
      "  iterations_since_restore: 349\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.316666666666666\n",
      "    gpu_util_percent0: 0.06\n",
      "    ram_util_percent: 61.604166666666664\n",
      "    vram_util_percent0: 0.2712931315104167\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.000000014901161\n",
      "  policy_reward_mean:\n",
      "    agent_0: 0.040000001937150954\n",
      "    agent_1: 0.4280000068247318\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051804845694000116\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.909286744021249\n",
      "    mean_inference_ms: 1.7866841018215243\n",
      "    mean_raw_obs_processing_ms: 0.16671580375965234\n",
      "  time_since_restore: 6529.772554636002\n",
      "  time_this_iter_s: 18.855143308639526\n",
      "  time_total_s: 6529.772554636002\n",
      "  timers:\n",
      "    learn_throughput: 648.483\n",
      "    learn_time_ms: 4317.767\n",
      "    load_throughput: 81706.501\n",
      "    load_time_ms: 34.269\n",
      "    sample_throughput: 191.458\n",
      "    sample_time_ms: 14624.611\n",
      "    update_time_ms: 3.314\n",
      "  timestamp: 1658501458\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 977200\n",
      "  training_iteration: 349\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   349</td><td style=\"text-align: right;\">         6529.77</td><td style=\"text-align: right;\">977200</td><td style=\"text-align: right;\">   0.468</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            188.03</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1960000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-51-17\n",
      "  done: false\n",
      "  episode_len_mean: 187.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000014901161\n",
      "  episode_reward_mean: 0.4320000088214874\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 4344\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.9498866651029814\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.012806768419835635\n",
      "          policy_loss: -0.053222893677316495\n",
      "          total_loss: -0.03840278676350516\n",
      "          vf_explained_var: -0.05777366831898689\n",
      "          vf_loss: 0.0034925277900800297\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.8080829875100226\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010168973147411043\n",
      "          policy_loss: -0.04787448076084339\n",
      "          total_loss: -0.03695759177311889\n",
      "          vf_explained_var: 0.07122930884361267\n",
      "          vf_loss: 0.0042816862424366435\n",
      "    num_agent_steps_sampled: 1960000\n",
      "    num_agent_steps_trained: 1960000\n",
      "    num_steps_sampled: 980000\n",
      "    num_steps_trained: 980000\n",
      "  iterations_since_restore: 350\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.4879999999999995\n",
      "    gpu_util_percent0: 0.0616\n",
      "    ram_util_percent: 61.587999999999994\n",
      "    vram_util_percent0: 0.27164453125\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.000000014901161\n",
      "  policy_reward_mean:\n",
      "    agent_0: 0.021000001952052115\n",
      "    agent_1: 0.41100000686943533\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05180351830888599\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.9093402847554906\n",
      "    mean_inference_ms: 1.786669535800305\n",
      "    mean_raw_obs_processing_ms: 0.16676174558768217\n",
      "  time_since_restore: 6548.754517316818\n",
      "  time_this_iter_s: 18.98196268081665\n",
      "  time_total_s: 6548.754517316818\n",
      "  timers:\n",
      "    learn_throughput: 646.278\n",
      "    learn_time_ms: 4332.499\n",
      "    load_throughput: 82713.44\n",
      "    load_time_ms: 33.852\n",
      "    sample_throughput: 193.306\n",
      "    sample_time_ms: 14484.799\n",
      "    update_time_ms: 3.321\n",
      "  timestamp: 1658501477\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 980000\n",
      "  training_iteration: 350\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   350</td><td style=\"text-align: right;\">         6548.75</td><td style=\"text-align: right;\">980000</td><td style=\"text-align: right;\">   0.432</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            187.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1965600\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-51-36\n",
      "  done: false\n",
      "  episode_len_mean: 196.82\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000037252903\n",
      "  episode_reward_mean: 0.48600000888109207\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 4358\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.0298788455270587\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.008521333737966597\n",
      "          policy_loss: -0.07083407489985152\n",
      "          total_loss: -0.0551558718715179\n",
      "          vf_explained_var: -0.3737837076187134\n",
      "          vf_loss: 0.019799746149406668\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.8948061508791787\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011374106165536801\n",
      "          policy_loss: -0.0769319304844365\n",
      "          total_loss: -0.06366835257640127\n",
      "          vf_explained_var: 0.24814771115779877\n",
      "          vf_loss: 0.007643422482336367\n",
      "    num_agent_steps_sampled: 1965600\n",
      "    num_agent_steps_trained: 1965600\n",
      "    num_steps_sampled: 982800\n",
      "    num_steps_trained: 982800\n",
      "  iterations_since_restore: 351\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.251999999999999\n",
      "    gpu_util_percent0: 0.0576\n",
      "    ram_util_percent: 61.608000000000004\n",
      "    vram_util_percent0: 0.26978515625\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.000000037252903\n",
      "  policy_reward_mean:\n",
      "    agent_0: 0.055000001713633534\n",
      "    agent_1: 0.4310000071674585\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051802559683805496\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.9094093796125287\n",
      "    mean_inference_ms: 1.78666878003799\n",
      "    mean_raw_obs_processing_ms: 0.16680144549583717\n",
      "  time_since_restore: 6567.797136545181\n",
      "  time_this_iter_s: 19.042619228363037\n",
      "  time_total_s: 6567.797136545181\n",
      "  timers:\n",
      "    learn_throughput: 647.551\n",
      "    learn_time_ms: 4323.985\n",
      "    load_throughput: 82511.845\n",
      "    load_time_ms: 33.935\n",
      "    sample_throughput: 193.114\n",
      "    sample_time_ms: 14499.223\n",
      "    update_time_ms: 3.314\n",
      "  timestamp: 1658501496\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 982800\n",
      "  training_iteration: 351\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   351</td><td style=\"text-align: right;\">          6567.8</td><td style=\"text-align: right;\">982800</td><td style=\"text-align: right;\">   0.486</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            196.82</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1971200\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-51-55\n",
      "  done: false\n",
      "  episode_len_mean: 200.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000037252903\n",
      "  episode_reward_mean: 0.5300000086426735\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 4372\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.04329706231753\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010732888452677402\n",
      "          policy_loss: -0.048045066837966305\n",
      "          total_loss: -0.03263139787755097\n",
      "          vf_explained_var: -0.2395053207874298\n",
      "          vf_loss: 0.011958115208212152\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.8986297835196768\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.013796382132090048\n",
      "          policy_loss: -0.07189171769104655\n",
      "          total_loss: -0.05738018936608569\n",
      "          vf_explained_var: 0.06282438337802887\n",
      "          vf_loss: 0.004295645621780187\n",
      "    num_agent_steps_sampled: 1971200\n",
      "    num_agent_steps_trained: 1971200\n",
      "    num_steps_sampled: 985600\n",
      "    num_steps_trained: 985600\n",
      "  iterations_since_restore: 352\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.052\n",
      "    gpu_util_percent0: 0.056799999999999996\n",
      "    ram_util_percent: 61.57599999999999\n",
      "    vram_util_percent0: 0.26810546874999996\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.000000037252903\n",
      "  policy_reward_mean:\n",
      "    agent_0: 0.07800000123679637\n",
      "    agent_1: 0.4520000074058771\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05180213291834482\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.9094835502561964\n",
      "    mean_inference_ms: 1.7866864510751947\n",
      "    mean_raw_obs_processing_ms: 0.16684050581895163\n",
      "  time_since_restore: 6586.737155914307\n",
      "  time_this_iter_s: 18.940019369125366\n",
      "  time_total_s: 6586.737155914307\n",
      "  timers:\n",
      "    learn_throughput: 648.923\n",
      "    learn_time_ms: 4314.839\n",
      "    load_throughput: 82603.019\n",
      "    load_time_ms: 33.897\n",
      "    sample_throughput: 192.991\n",
      "    sample_time_ms: 14508.425\n",
      "    update_time_ms: 3.313\n",
      "  timestamp: 1658501515\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 985600\n",
      "  training_iteration: 352\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   352</td><td style=\"text-align: right;\">         6586.74</td><td style=\"text-align: right;\">985600</td><td style=\"text-align: right;\">    0.53</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             200.7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1976800\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-52-15\n",
      "  done: false\n",
      "  episode_len_mean: 206.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000037252903\n",
      "  episode_reward_mean: 0.573000008687377\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 4382\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.006395644375256\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.014182752517681831\n",
      "          policy_loss: -0.10045556893982437\n",
      "          total_loss: -0.0841947729800484\n",
      "          vf_explained_var: 0.018768485635519028\n",
      "          vf_loss: 0.003253330341457123\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.8849645437938827\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.017415321471471974\n",
      "          policy_loss: -0.09042723682337271\n",
      "          total_loss: -0.07316824129000972\n",
      "          vf_explained_var: 0.3322288393974304\n",
      "          vf_loss: 0.0017916590313689085\n",
      "    num_agent_steps_sampled: 1976800\n",
      "    num_agent_steps_trained: 1976800\n",
      "    num_steps_sampled: 988400\n",
      "    num_steps_trained: 988400\n",
      "  iterations_since_restore: 353\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 7.080769230769231\n",
      "    gpu_util_percent0: 0.07846153846153846\n",
      "    ram_util_percent: 61.79230769230771\n",
      "    vram_util_percent0: 0.27100360576923077\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.000000037252903\n",
      "  policy_reward_mean:\n",
      "    agent_0: 0.09800000123679638\n",
      "    agent_1: 0.4750000074505806\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05180269258280621\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.9095692649651506\n",
      "    mean_inference_ms: 1.7867512576001225\n",
      "    mean_raw_obs_processing_ms: 0.16686563950180003\n",
      "  time_since_restore: 6606.172896385193\n",
      "  time_this_iter_s: 19.43574047088623\n",
      "  time_total_s: 6606.172896385193\n",
      "  timers:\n",
      "    learn_throughput: 651.187\n",
      "    learn_time_ms: 4299.843\n",
      "    load_throughput: 82889.453\n",
      "    load_time_ms: 33.78\n",
      "    sample_throughput: 192.498\n",
      "    sample_time_ms: 14545.6\n",
      "    update_time_ms: 3.298\n",
      "  timestamp: 1658501535\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 988400\n",
      "  training_iteration: 353\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   353</td><td style=\"text-align: right;\">         6606.17</td><td style=\"text-align: right;\">988400</td><td style=\"text-align: right;\">   0.573</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            206.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1982400\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-52-35\n",
      "  done: false\n",
      "  episode_len_mean: 208.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000037252903\n",
      "  episode_reward_mean: 0.5470000082254409\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 4397\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.0500591400833357\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.008345828846072004\n",
      "          policy_loss: -0.0782043655757055\n",
      "          total_loss: -0.06289510519118699\n",
      "          vf_explained_var: -0.08859357237815857\n",
      "          vf_loss: 0.019334125617336082\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.8183816586221968\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010607762733342265\n",
      "          policy_loss: -0.07132980770585987\n",
      "          total_loss: -0.05923787152936538\n",
      "          vf_explained_var: 0.04383920505642891\n",
      "          vf_loss: 0.006395684015799253\n",
      "    num_agent_steps_sampled: 1982400\n",
      "    num_agent_steps_trained: 1982400\n",
      "    num_steps_sampled: 991200\n",
      "    num_steps_trained: 991200\n",
      "  iterations_since_restore: 354\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.142307692307693\n",
      "    gpu_util_percent0: 0.09923076923076923\n",
      "    ram_util_percent: 62.3923076923077\n",
      "    vram_util_percent0: 0.2880634014423077\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.000000037252903\n",
      "  policy_reward_mean:\n",
      "    agent_0: 0.08300000078976154\n",
      "    agent_1: 0.46400000743567943\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05180446429208848\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.9097199624111965\n",
      "    mean_inference_ms: 1.7869005165588556\n",
      "    mean_raw_obs_processing_ms: 0.16690215359116878\n",
      "  time_since_restore: 6626.0904104709625\n",
      "  time_this_iter_s: 19.917514085769653\n",
      "  time_total_s: 6626.0904104709625\n",
      "  timers:\n",
      "    learn_throughput: 647.908\n",
      "    learn_time_ms: 4321.604\n",
      "    load_throughput: 82832.042\n",
      "    load_time_ms: 33.803\n",
      "    sample_throughput: 191.707\n",
      "    sample_time_ms: 14605.61\n",
      "    update_time_ms: 3.279\n",
      "  timestamp: 1658501555\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 991200\n",
      "  training_iteration: 354\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   354</td><td style=\"text-align: right;\">         6626.09</td><td style=\"text-align: right;\">991200</td><td style=\"text-align: right;\">   0.547</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            208.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1988000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-52-54\n",
      "  done: false\n",
      "  episode_len_mean: 209.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000037252903\n",
      "  episode_reward_mean: 0.49500000827014445\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 4412\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.0081332453659604\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011774702565334158\n",
      "          policy_loss: -0.07087444279507811\n",
      "          total_loss: -0.05602674352366237\n",
      "          vf_explained_var: 0.05194200202822685\n",
      "          vf_loss: 0.006954918627646596\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.8337200794901167\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.011617941005489994\n",
      "          policy_loss: -0.07311109102496023\n",
      "          total_loss: -0.06001931102357395\n",
      "          vf_explained_var: 0.15086159110069275\n",
      "          vf_loss: 0.006384394360100573\n",
      "    num_agent_steps_sampled: 1988000\n",
      "    num_agent_steps_trained: 1988000\n",
      "    num_steps_sampled: 994000\n",
      "    num_steps_trained: 994000\n",
      "  iterations_since_restore: 355\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.8500000000000005\n",
      "    gpu_util_percent0: 0.07076923076923076\n",
      "    ram_util_percent: 62.642307692307696\n",
      "    vram_util_percent0: 0.2830679086538462\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.000000037252903\n",
      "  policy_reward_mean:\n",
      "    agent_0: 0.04800000078976154\n",
      "    agent_1: 0.44700000748038293\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051806977001100006\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.9098948256351242\n",
      "    mean_inference_ms: 1.787084177395221\n",
      "    mean_raw_obs_processing_ms: 0.16694341191265244\n",
      "  time_since_restore: 6645.509764432907\n",
      "  time_this_iter_s: 19.41935396194458\n",
      "  time_total_s: 6645.509764432907\n",
      "  timers:\n",
      "    learn_throughput: 645.121\n",
      "    learn_time_ms: 4340.273\n",
      "    load_throughput: 82829.647\n",
      "    load_time_ms: 33.804\n",
      "    sample_throughput: 190.984\n",
      "    sample_time_ms: 14660.882\n",
      "    update_time_ms: 3.468\n",
      "  timestamp: 1658501574\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 994000\n",
      "  training_iteration: 355\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   355</td><td style=\"text-align: right;\">         6645.51</td><td style=\"text-align: right;\">994000</td><td style=\"text-align: right;\">   0.495</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            209.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1993600\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-53-13\n",
      "  done: false\n",
      "  episode_len_mean: 203.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000037252903\n",
      "  episode_reward_mean: 0.4780000077933073\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 4427\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.9032798044028736\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.00811752270165833\n",
      "          policy_loss: 0.02373165971532996\n",
      "          total_loss: 0.033758224182141326\n",
      "          vf_explained_var: -0.11035522073507309\n",
      "          vf_loss: 0.004812738381674751\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.7252949462050484\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010878380861256889\n",
      "          policy_loss: -0.02264305746038666\n",
      "          total_loss: -0.010907850449993498\n",
      "          vf_explained_var: 0.1381678581237793\n",
      "          vf_loss: 0.004494309258786545\n",
      "    num_agent_steps_sampled: 1993600\n",
      "    num_agent_steps_trained: 1993600\n",
      "    num_steps_sampled: 996800\n",
      "    num_steps_trained: 996800\n",
      "  iterations_since_restore: 356\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 7.115999999999999\n",
      "    gpu_util_percent0: 0.07119999999999999\n",
      "    ram_util_percent: 62.523999999999994\n",
      "    vram_util_percent0: 0.28187109375\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.000000037252903\n",
      "  policy_reward_mean:\n",
      "    agent_0: 0.04100000031292438\n",
      "    agent_1: 0.4370000074803829\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051810127783488674\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.910092627464617\n",
      "    mean_inference_ms: 1.7873000849538443\n",
      "    mean_raw_obs_processing_ms: 0.16699389123631034\n",
      "  time_since_restore: 6664.945959568024\n",
      "  time_this_iter_s: 19.436195135116577\n",
      "  time_total_s: 6664.945959568024\n",
      "  timers:\n",
      "    learn_throughput: 643.304\n",
      "    learn_time_ms: 4352.532\n",
      "    load_throughput: 88609.404\n",
      "    load_time_ms: 31.599\n",
      "    sample_throughput: 189.943\n",
      "    sample_time_ms: 14741.245\n",
      "    update_time_ms: 3.477\n",
      "  timestamp: 1658501593\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 996800\n",
      "  training_iteration: 356\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   356</td><td style=\"text-align: right;\">         6664.95</td><td style=\"text-align: right;\">996800</td><td style=\"text-align: right;\">   0.478</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             203.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 1999200\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-53-33\n",
      "  done: false\n",
      "  episode_len_mean: 203.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000037252903\n",
      "  episode_reward_mean: 0.4290000076591969\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 4440\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.9973588429746174\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.01230684425115394\n",
      "          policy_loss: -0.06864804294448168\n",
      "          total_loss: -0.053577131555703955\n",
      "          vf_explained_var: -0.14116355776786804\n",
      "          vf_loss: 0.00587035846166102\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.8181413486599922\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.012367204105890922\n",
      "          policy_loss: -0.06492101124379024\n",
      "          total_loss: -0.05052769388116542\n",
      "          vf_explained_var: 0.07155999541282654\n",
      "          vf_loss: 0.00794246699158629\n",
      "    num_agent_steps_sampled: 1999200\n",
      "    num_agent_steps_trained: 1999200\n",
      "    num_steps_sampled: 999600\n",
      "    num_steps_trained: 999600\n",
      "  iterations_since_restore: 357\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 7.5280000000000005\n",
      "    gpu_util_percent0: 0.0636\n",
      "    ram_util_percent: 62.5\n",
      "    vram_util_percent0: 0.2869375\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.000000037252903\n",
      "  policy_reward_mean:\n",
      "    agent_0: 0.020000000298023225\n",
      "    agent_1: 0.40900000736117365\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.051813086200431506\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.910270378975673\n",
      "    mean_inference_ms: 1.7875087437855866\n",
      "    mean_raw_obs_processing_ms: 0.16703702928345987\n",
      "  time_since_restore: 6684.122497797012\n",
      "  time_this_iter_s: 19.176538228988647\n",
      "  time_total_s: 6684.122497797012\n",
      "  timers:\n",
      "    learn_throughput: 640.268\n",
      "    learn_time_ms: 4373.167\n",
      "    load_throughput: 87909.51\n",
      "    load_time_ms: 31.851\n",
      "    sample_throughput: 190.192\n",
      "    sample_time_ms: 14722.0\n",
      "    update_time_ms: 3.488\n",
      "  timestamp: 1658501613\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 999600\n",
      "  training_iteration: 357\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   357</td><td style=\"text-align: right;\">         6684.12</td><td style=\"text-align: right;\">999600</td><td style=\"text-align: right;\">   0.429</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            203.97</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_3_vs_3_auto_GK_70b76_00000:\n",
      "  agent_timesteps_total: 2004800\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-22_14-53-52\n",
      "  done: true\n",
      "  episode_len_mean: 203.92\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.000000037252903\n",
      "  episode_reward_mean: 0.4500000073760748\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 4454\n",
      "  experiment_id: b81591126e024851b4a1ca07dd4ed56d\n",
      "  hostname: 437ad2db36ab\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.125\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 2.0710917015870414\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.008338562692459958\n",
      "          policy_loss: -0.060039390739499744\n",
      "          total_loss: -0.04640938131480999\n",
      "          vf_explained_var: -0.07333405315876007\n",
      "          vf_loss: 0.014588742675273568\n",
      "      agent_1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0\n",
      "          cur_lr: 0.00022602718266055705\n",
      "          entropy: 1.8482758385084925\n",
      "          entropy_coeff: 0.0004158966184268587\n",
      "          kl: 0.010331463773169366\n",
      "          policy_loss: -0.06761530351303961\n",
      "          total_loss: -0.056025487133454246\n",
      "          vf_explained_var: 0.06394151598215103\n",
      "          vf_loss: 0.0057865392597401906\n",
      "    num_agent_steps_sampled: 2004800\n",
      "    num_agent_steps_trained: 2004800\n",
      "    num_steps_sampled: 1002400\n",
      "    num_steps_trained: 1002400\n",
      "  iterations_since_restore: 358\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 7.761538461538462\n",
      "    gpu_util_percent0: 0.07807692307692307\n",
      "    ram_util_percent: 62.78461538461538\n",
      "    vram_util_percent0: 0.2892841045673077\n",
      "  pid: 321\n",
      "  policy_reward_max:\n",
      "    agent_0: 2.0\n",
      "    agent_1: 2.000000037252903\n",
      "  policy_reward_mean:\n",
      "    agent_0: 0.03300000011920929\n",
      "    agent_1: 0.4170000072568655\n",
      "  policy_reward_min:\n",
      "    agent_0: -1.0\n",
      "    agent_1: -1.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05181713175185248\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.910498383081454\n",
      "    mean_inference_ms: 1.7877833027573262\n",
      "    mean_raw_obs_processing_ms: 0.167086956237794\n",
      "  time_since_restore: 6703.852488994598\n",
      "  time_this_iter_s: 19.72999119758606\n",
      "  time_total_s: 6703.852488994598\n",
      "  timers:\n",
      "    learn_throughput: 640.082\n",
      "    learn_time_ms: 4374.442\n",
      "    load_throughput: 87451.802\n",
      "    load_time_ms: 32.018\n",
      "    sample_throughput: 188.768\n",
      "    sample_time_ms: 14832.988\n",
      "    update_time_ms: 3.482\n",
      "  timestamp: 1658501632\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1002400\n",
      "  training_iteration: 358\n",
      "  trial_id: 70b76_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/20 CPUs, 1.0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>RUNNING </td><td>172.17.0.2:321</td><td style=\"text-align: right;\">   358</td><td style=\"text-align: right;\">         6703.85</td><td style=\"text-align: right;\">1002400</td><td style=\"text-align: right;\">    0.45</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            203.92</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/20 CPUs, 0/1 GPUs, 0.0/7.69 GiB heap, 0.0/3.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /mnt/logs/PPO<br>Number of trials: 1/1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_3_vs_3_auto_GK_70b76_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">   358</td><td style=\"text-align: right;\">         6703.85</td><td style=\"text-align: right;\">1002400</td><td style=\"text-align: right;\">    0.45</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            203.92</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-22 14:53:53,294\tINFO tune.py:561 -- Total run time: 6727.54 seconds (6727.19 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best checkpoint found: /mnt/logs/PPO/PPO_3_vs_3_auto_GK_70b76_00000_0_2022-07-22_13-01-45/checkpoint_000358/checkpoint-358\n"
     ]
    }
   ],
   "source": [
    "def gen_policy(idx):\n",
    "    return (None, obs_space[f'player_{idx}'], act_space[f'player_{idx}'], {})\n",
    "\n",
    "\n",
    "n_policies = len(obs_space)\n",
    "policies = {\n",
    "    'agent_{}'.format(idx): gen_policy(idx) for idx in range(n_policies)\n",
    "}\n",
    "policy_ids = list(policies.keys())\n",
    "policy_mapping_fn = lambda agent_id, episode, **kwargs: \\\n",
    "    policy_ids[0 if len(policy_ids) == 1 else int(agent_id.split('_')[1])]\n",
    "default_multiagent = {\n",
    "    'policies': policies,\n",
    "    'policy_mapping_fn': policy_mapping_fn,\n",
    "}\n",
    "config = {\n",
    "    'env': env_name,\n",
    "    'framework': 'torch',\n",
    "    'lr': 0.00022602718266055705,\n",
    "    'gamma': 0.9936809332376452,\n",
    "    'lambda': 0.9517171675473532,\n",
    "    'kl_target': 0.010117093480119358,\n",
    "    'kl_coeff': 1.0,\n",
    "    'clip_param': 0.20425701146213993,\n",
    "    'vf_loss_coeff': 0.3503035138680095,\n",
    "    'vf_clip_param': 1.4862186106326711,\n",
    "    'entropy_coeff': 0.0004158966184268587,\n",
    "    'num_sgd_iter': 16,\n",
    "    'train_batch_size': 2_800,\n",
    "    'rollout_fragment_length': 100,\n",
    "    'sgd_minibatch_size': 128,\n",
    "    'num_workers': 1,  # one goes to the trainer\n",
    "    'num_envs_per_worker': 1,\n",
    "    'num_gpus': n_gpus,\n",
    "    'batch_mode': 'truncate_episodes',\n",
    "    'observation_filter': 'NoFilter',\n",
    "    'log_level': 'INFO',\n",
    "    'ignore_worker_failures': False,\n",
    "    'horizon': 500,\n",
    "    'model': {\n",
    "        'vf_share_layers': \"true\",\n",
    "        'use_lstm': \"true\",\n",
    "        'max_seq_len': 13,\n",
    "        'fcnet_hiddens': [256, 256],\n",
    "        'fcnet_activation': \"tanh\",\n",
    "        'lstm_cell_size': 256,\n",
    "        'lstm_use_prev_action': \"true\",\n",
    "        'lstm_use_prev_reward': \"true\",\n",
    "    },\n",
    "    'multiagent': default_multiagent,\n",
    "}\n",
    "\n",
    "stop = {\n",
    "    \"timesteps_total\": 1000_000,\n",
    "}\n",
    "\n",
    "policy_type = \"independent\"\n",
    "config_type = \"fixed\"\n",
    "\n",
    "a = tune.run(\n",
    "    'PPO',\n",
    "    name=\"PPO\",\n",
    "    reuse_actors=False,\n",
    "    raise_on_failed_trial=True,\n",
    "    fail_fast=True,\n",
    "    max_failures=0,\n",
    "    stop=stop,\n",
    "    #         checkpoint_freq=100,\n",
    "    checkpoint_at_end=True,\n",
    "    local_dir=\"../../logs\",\n",
    "    config=config,\n",
    "#     verbose=3\n",
    ")\n",
    "checkpoint_path = a.get_best_checkpoint(a.get_best_trial(\n",
    "    \"episode_reward_mean\", \"max\"), \"episode_reward_mean\", \"max\")\n",
    "print('Best checkpoint found:', checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36fd803e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "978907a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../../logs/PPO/PPO_3_vs_3_auto_GK_70b76_00000_0_2022-07-22_13-01-45/progress.csv\")\n",
    "plt.figure(figsize=(16, 9))\n",
    "plt.plot(df.episode_reward_mean, label=\"mean-reward\")\n",
    "plt.plot(df.episode_reward_max, label=\"max-reward\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6127b3f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(16, 9))\n",
    "plt.plot(df[\"info/learner/agent_0/learner_stats/policy_loss\"], label=\"agent_0\")\n",
    "plt.plot(df[\"info/learner/agent_1/learner_stats/policy_loss\"], label=\"agent_1\")\n",
    "plt.ylabel(\"policy_loss\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66aa5f66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
